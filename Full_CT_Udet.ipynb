{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XuOSMsgitQGW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oplab\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "import hashlib\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "from dsetsFullCT import TrainingLuna2dSegmentationDataset, Luna2dSegmentationDataset, PrepcacheLunaDataset, getCt\n",
    "from util import logging, enumerateWithEstimate\n",
    "from UDet_4layer import UDet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb3D88NFuOP2"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e09Iypx9whOJ"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,  #原論文channel數是64，為2^6\n",
    "                 batch_norm=False, up_mode='upconv'):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
    "                                                padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
    "                                            padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i) #channel數會隨著down sampling增加，以2的倍數增加\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path)-1:\n",
    "                blocks.append(x)  #put the result in blocks, and to be a bridge to upsampleing\n",
    "                x = F.avg_pool2d(x, 2)  #做一次的avarage pooling, stride為2, kernel size 為2(大小砍半)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i-1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "class UNetConvBlock(nn.Module): #每一層都會做2次的convolution，kenrel size 都是3\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        # block.append(nn.LeakyReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        # block.append(nn.LeakyReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
    "                                         stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R4uR6voouQao"
   },
   "outputs": [],
   "source": [
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs): #kwarg is a dictionary containing all keyword arguments passed to the constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # we will do batchnormalization first \n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])  #in kwarg, we have in_channels params to give the input channel\n",
    "        self.unet = UNet(**kwargs)\n",
    "        self.final = nn.Sigmoid() #use sigmoid to limit the output to 0,1\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        init_set = {\n",
    "            nn.Conv2d,\n",
    "            nn.Conv3d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.ConvTranspose3d,\n",
    "            nn.Linear,\n",
    "        }\n",
    "        for m in self.modules():\n",
    "            if type(m) in init_set:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = \\\n",
    "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "        # nn.init.constant_(self.unet.last.bias, -4)\n",
    "        # nn.init.constant_(self.unet.last.bias, 4)\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.unet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output\n",
    "    \n",
    "class UDetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs): #kwarg is a dictionary containing all keyword arguments passed to the constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # we will do batchnormalization first \n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])  #in kwarg, we have in_channels params to give the input channel\n",
    "        self.udet = UDet(**kwargs)\n",
    "        self.final = nn.Sigmoid() #use sigmoid to limit the output to 0,1\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        init_set = {\n",
    "            nn.Conv2d,\n",
    "            nn.Conv3d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.ConvTranspose3d,\n",
    "            nn.Linear,\n",
    "        }\n",
    "        for m in self.modules():\n",
    "            if type(m) in init_set:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = \\\n",
    "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "        # nn.init.constant_(self.unet.last.bias, -4)\n",
    "        # nn.init.constant_(self.unet.last.bias, 4)\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.udet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output\n",
    "\n",
    "class SegmentationAugmentation(nn.Module):\n",
    "    def __init__(\n",
    "            self, flip=None, offset=None, scale=None, rotate=None, noise=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flip = flip\n",
    "        self.offset = offset\n",
    "        self.scale = scale\n",
    "        self.rotate = rotate\n",
    "        self.noise = noise\n",
    "\n",
    "    def forward(self, input_g, label_g):\n",
    "        transform_t = self._build2dTransformMatrix()\n",
    "        transform_t = transform_t.expand(input_g.shape[0], -1, -1)  #對Index複製\n",
    "        transform_t = transform_t.to(input_g.device, torch.float32) #transform前兩行有關伸縮旋轉，最後一行有關平移\n",
    "        affine_t = F.affine_grid(transform_t[:,:2],\n",
    "                input_g.size(), align_corners=False)  #使用affine grid的原因是因為用grid可以減少對整張圖的運算量\n",
    "                                     #而且如果用原圖，可能會造成座標落在非整數格上，如此會讓圖型產生矩齒狀\n",
    "\n",
    "        augmented_input_g = F.grid_sample(input_g,\n",
    "                affine_t, padding_mode='border',\n",
    "                align_corners=False)\n",
    "        augmented_label_g = F.grid_sample(label_g.to(torch.float32),#grid sample只吃float，所以這裡轉float，但用同一個affine grid\n",
    "                affine_t, padding_mode='border',\n",
    "                align_corners=False)\n",
    "\n",
    "        if self.noise:\n",
    "            noise_t = torch.randn_like(augmented_input_g)\n",
    "            noise_t *= self.noise\n",
    "\n",
    "            augmented_input_g += noise_t\n",
    "\n",
    "        return augmented_input_g, augmented_label_g > 0.5 #把label改回成0,1\n",
    "\n",
    "    def _build2dTransformMatrix(self):\n",
    "        transform_t = torch.eye(3)  #建立一個3*3單位矩陣\n",
    "\n",
    "        for i in range(2):  #我們只有2D\n",
    "            if self.flip:\n",
    "                if random.random() > 0.5:\n",
    "                    transform_t[i,i] *= -1\n",
    "\n",
    "            if self.offset:\n",
    "                offset_float = self.offset\n",
    "                random_float = (random.random() * 2 - 1)\n",
    "                transform_t[2,i] = offset_float * random_float\n",
    "\n",
    "            if self.scale:\n",
    "                scale_float = self.scale\n",
    "                random_float = (random.random() * 2 - 1)\n",
    "                transform_t[i,i] *= 1.0 + scale_float * random_float\n",
    "\n",
    "        if self.rotate:\n",
    "            angle_rad = random.random() * math.pi * 2 #隨機弧度\n",
    "            s = math.sin(angle_rad) #轉角度\n",
    "            c = math.cos(angle_rad)\n",
    "\n",
    "            rotation_t = torch.tensor([\n",
    "                [c, -s, 0],\n",
    "                [s, c, 0],\n",
    "                [0, 0, 1]])\n",
    "\n",
    "            transform_t @= rotation_t #矩陣乘法\n",
    "\n",
    "        return transform_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsRW7wTH2ISQ"
   },
   "source": [
    "# Prepcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PzMZ9d-Vov1j"
   },
   "outputs": [],
   "source": [
    "class LunaPrepCacheApp:\n",
    "    @classmethod\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=1024,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=1, #8\n",
    "            type=int,\n",
    "        )\n",
    "        # parser.add_argument('--scaled',\n",
    "        #     help=\"Scale the CT chunks to square voxels.\",\n",
    "        #     default=False,\n",
    "        #     action='store_true',\n",
    "        # )\n",
    "\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        self.prep_dl = DataLoader(\n",
    "             PrepcacheLunaDataset(\n",
    "#                 # sortby_str='series_uid',\n",
    "             ),\n",
    "   \n",
    "            batch_size=self.cli_args.batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            self.prep_dl,\n",
    "            \"Stuffing cache\",\n",
    "            start_ndx=self.prep_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymn03JNItty_"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        smooth = 1.  # Smoothing factor to prevent division by zero\n",
    "\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        if self.weight is not None:\n",
    "            # Apply class-wise weights\n",
    "            dice = 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "            weighted_dice = dice * self.weight\n",
    "            return weighted_dice.mean()\n",
    "        else:\n",
    "            return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3wyfcVhgykNH"
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)\n",
    "METRICS_LOSS_NDX = 1\n",
    "METRICS_FNLOSS_NDX = 2\n",
    "METRICS_FPLOSS_NDX = 3\n",
    "METRICS_TP_NDX = 7\n",
    "METRICS_FN_NDX = 8\n",
    "METRICS_FP_NDX = 9\n",
    "\n",
    "METRICS_SIZE = 10\n",
    "class SegmentationTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=16,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--epochs',\n",
    "            help='Number of epochs to train for',\n",
    "            default=1,\n",
    "            type=int,\n",
    "        )\n",
    "\n",
    "        parser.add_argument('--augmented',\n",
    "            help=\"Augment the training data.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "        parser.add_argument('--augment-flip',\n",
    "            help=\"Augment the training data by randomly flipping the data left-right, up-down, and front-back.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "        parser.add_argument('--augment-offset',\n",
    "            help=\"Augment the training data by randomly offsetting the data slightly along the X and Y axes.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "        parser.add_argument('--augment-scale',\n",
    "            help=\"Augment the training data by randomly increasing or decreasing the size of the candidate.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "        parser.add_argument('--augment-rotate',\n",
    "            help=\"Augment the training data by randomly rotating the data around the head-foot axis.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "        parser.add_argument('--augment-noise',\n",
    "            help=\"Augment the training data by randomly adding noise to the data.\",\n",
    "            action='store_true',\n",
    "            default=False,\n",
    "        )\n",
    "\n",
    "        parser.add_argument('--tb-prefix',\n",
    "            default='udet',\n",
    "            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n",
    "        )\n",
    "\n",
    "        parser.add_argument('comment',\n",
    "            help=\"Comment suffix for Tensorboard run.\",\n",
    "            nargs='?',\n",
    "            default='none',\n",
    "        )\n",
    "\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "        self.totalTrainingSamples_count = 0\n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.changeLossWeight = False\n",
    "\n",
    "        #augumentation設定的值\n",
    "        self.augmentation_dict = {}\n",
    "        if self.cli_args.augmented or self.cli_args.augment_flip:\n",
    "            self.augmentation_dict['flip'] = True\n",
    "        if self.cli_args.augmented or self.cli_args.augment_offset:\n",
    "            self.augmentation_dict['offset'] = 0.03\n",
    "        if self.cli_args.augmented or self.cli_args.augment_scale:\n",
    "            self.augmentation_dict['scale'] = 0.2\n",
    "        if self.cli_args.augmented or self.cli_args.augment_rotate:\n",
    "            self.augmentation_dict['rotate'] = True\n",
    "        if self.cli_args.augmented or self.cli_args.augment_noise:\n",
    "            self.augmentation_dict['noise'] = 25.0\n",
    "            # self.augmentation_dict['noise'] = 25.0\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "        self.segmentation_model, self.augmentation_model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "\n",
    "\n",
    "    def initModel(self):\n",
    "        segmentation_model = UDetWrapper(\n",
    "            in_channels=7,\n",
    "            n_classes=1,\n",
    "            depth=4,  #how deep the U go\n",
    "            wf=6,   #2^4 filter\n",
    "            padding=True, #padding so that we get the output size as input size\n",
    "            batch_norm=True,\n",
    "            up_mode='upconv', #use  nn.ConvTranspose2d\n",
    "        )\n",
    "\n",
    "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1: #parallel data if we have much device\n",
    "                segmentation_model = nn.DataParallel(segmentation_model)\n",
    "                augmentation_model = nn.DataParallel(augmentation_model)\n",
    "            segmentation_model = segmentation_model.to(self.device)\n",
    "            augmentation_model = augmentation_model.to(self.device)\n",
    "\n",
    "        return segmentation_model, augmentation_model #回傳unet wrapper和augmentation\n",
    "\n",
    "    def initOptimizer(self):\n",
    "        return Adam(self.segmentation_model.parameters(), lr=0.0001, betas=(0.99,0.999), weight_decay=1e-6)\n",
    "        # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "\n",
    "    def initTrainDl(self):\n",
    "        train_ds = TrainingLuna2dSegmentationDataset(\n",
    "            val_stride=10,\n",
    "            set_class=\"Training\",\n",
    "            contextSlices_count=3,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = Luna2dSegmentationDataset(\n",
    "            val_stride=10,\n",
    "            set_class=\"Validation\",\n",
    "            contextSlices_count=3,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return val_dl\n",
    "\n",
    "    def initTensorboardWriters(self):\n",
    "        if self.trn_writer is None:\n",
    "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "\n",
    "            self.trn_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '_trn_seg_' + self.cli_args.comment)\n",
    "            self.val_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '_val_seg_' + self.cli_args.comment)\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        best_score = 0.0\n",
    "        self.validation_cadence = 5\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n",
    "            # if epoch_ndx % self.validation_cadence == 0:\n",
    "                # if validation is wanted\n",
    "                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "                best_score = max(score, best_score)\n",
    "\n",
    "                self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "\n",
    "                self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "                self.logImages(epoch_ndx, 'val', val_dl)\n",
    "\n",
    "        self.trn_writer.close()\n",
    "        self.val_writer.close()\n",
    "\n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=self.device)\n",
    "        self.segmentation_model.train()\n",
    "        train_dl.dataset.shuffleSamples()\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx=train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
    "            loss_var.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.totalTrainingSamples_count += trnMetrics_g.size(1)\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=self.device)\n",
    "            self.segmentation_model.eval()\n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation \".format(epoch_ndx),\n",
    "                start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "        return valMetrics_g.to('cpu')\n",
    "\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
    "                         classificationThreshold=0.5):\n",
    "        input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "\n",
    "        if self.segmentation_model.training and self.augmentation_dict:\n",
    "            input_g, label_g = self.augmentation_model(input_g, label_g)\n",
    "\n",
    "        prediction_g = self.segmentation_model(input_g)\n",
    "        \n",
    "        # pos_weight = torch.tensor([100]).to(self.device, non_blocking=True)\n",
    "        \n",
    "        \n",
    "        # loss = DiceLoss()\n",
    "        # DLoss = criterion(prediction_g, label_g.to(torch.float))\n",
    "        # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        # BCELoss = criterion(prediction_g, label_g.to(torch.float))\n",
    "        # fnLoss_g = criterion(prediction_g * label_g, label_g.to(torch.float))\n",
    "        # fpLoss_g = criterion(prediction_g * ~label_g, label_g.to(torch.float))\n",
    "        # print(BCELoss)\n",
    "        \n",
    "        # class_weights = torch.tensor([20, 1], device=self.device, dtype=torch.float) \n",
    "        # criterion = WeightedDiceLoss(weight=class_weights)\n",
    "        criterion= WeightedDiceLoss()\n",
    "        diceLoss_g = criterion(prediction_g, label_g.to(torch.float))\n",
    "        fnLoss_g = criterion(prediction_g * label_g, label_g.to(torch.float))\n",
    "        fpLoss_g = criterion(prediction_g * ~label_g, label_g.to(torch.float))\n",
    "        \n",
    "\n",
    "        # diceLoss_g = self.diceLoss(prediction_g, label_g)\n",
    "        # fnLoss_g = self.diceLoss(prediction_g * label_g, label_g) #只關心ground truth為true的部分\n",
    "#         # print(diceLoss_g.mean())\n",
    "#         # print(fnLoss_g)\n",
    "#         # print(diceLoss_g + fnLoss_g * 8)\n",
    "#         # print()\n",
    "\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + input_t.size(0)\n",
    "        \n",
    "        # total_loss = BCELoss\n",
    "        # total_loss = BCELoss\n",
    "        \n",
    "        # total_loss = diceLoss_g.mean() + fnLoss_g.mean() * 8\n",
    "        total_loss = diceLoss_g.mean()\n",
    "        # if self.changeLossWeight:\n",
    "        #     total_loss = diceLoss_g.mean() + fnLoss_g.mean() + fpLoss_g.mean() * 8\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictionBool_g = (prediction_g[:, 0:1]\n",
    "                                > classificationThreshold).to(torch.float32)\n",
    "\n",
    "            tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
    "            fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
    "            fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
    "\n",
    "            # metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = BCELoss\n",
    "            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = total_loss\n",
    "            metrics_g[METRICS_FNLOSS_NDX, start_ndx:end_ndx] = fnLoss_g\n",
    "            metrics_g[METRICS_FPLOSS_NDX, start_ndx:end_ndx] = fpLoss_g\n",
    "            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "            \n",
    "        # return BCELoss\n",
    "        # return BCELoss\n",
    "            \n",
    "        # return diceLoss_g.mean()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "        # return diceLoss_g.mean() + fnLoss_g.mean() + fpLoss_g.mean() * 8  #loss 的權重:positive pixel是negative的8倍, we should expect a large number of false positives in general\n",
    "        \n",
    "\n",
    "    def diceLoss(self, prediction_g, label_g, epsilon=1): #如果大部分的pixel是false，用dice會比較精準\n",
    "        diceLabel_g = label_g.sum(dim=[1,2,3])  #將所有mask裡計為nodule的點加起來(我們的dataset是4維, 後3維是index, row, column)\n",
    "        dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
    "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3]) #預測正確的總量\n",
    "\n",
    "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
    "            / (dicePrediction_g + diceLabel_g + epsilon)  #epsilon避免其值為0\n",
    "\n",
    "        return 1 - diceRatio_g  #為了最小化，要用1去扣\n",
    "\n",
    "\n",
    "    def logImages(self, epoch_ndx, mode_str, dl):\n",
    "        self.segmentation_model.eval()\n",
    "\n",
    "        images = sorted(dl.dataset.series_list)[:12]\n",
    "        for series_ndx, series_uid in enumerate(images):\n",
    "            ct = getCt(series_uid)\n",
    "\n",
    "            for slice_ndx in range(6):\n",
    "                ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n",
    "                sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n",
    "\n",
    "                ct_t, label_t, series_uid, ct_ndx = sample_tup\n",
    "\n",
    "                input_g = ct_t.to(self.device).unsqueeze(0)\n",
    "                label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n",
    "\n",
    "                prediction_g = self.segmentation_model(input_g)[0]\n",
    "                prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n",
    "                label_a = label_g.cpu().numpy()[0][0] > 0.5\n",
    "\n",
    "                ct_t[:-1,:,:] /= 2000\n",
    "                ct_t[:-1,:,:] += 0.5\n",
    "\n",
    "                ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n",
    "\n",
    "                image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "                image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
    "                image_a[:,:,0] += prediction_a & (1 - label_a)\n",
    "                image_a[:,:,0] += (1 - prediction_a) & label_a\n",
    "                image_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5\n",
    "\n",
    "                image_a[:,:,1] += prediction_a & label_a\n",
    "                image_a *= 0.5\n",
    "                image_a.clip(0, 1, image_a)\n",
    "\n",
    "                writer = getattr(self, mode_str + '_writer')\n",
    "                writer.add_image(\n",
    "                    f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n",
    "                    image_a,\n",
    "                    self.totalTrainingSamples_count,\n",
    "                    dataformats='HWC',\n",
    "                )\n",
    "\n",
    "                if epoch_ndx == 1:\n",
    "                    image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "                    image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
    "                    # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n",
    "                    image_a[:,:,1] += label_a  # Green\n",
    "                    # image_a[:,:,2] += neg_a  # Blue\n",
    "\n",
    "                    image_a *= 0.5\n",
    "                    image_a[image_a < 0] = 0\n",
    "                    image_a[image_a > 1] = 1\n",
    "                    writer.add_image(\n",
    "                        '{}/{}_label_{}'.format(\n",
    "                            mode_str,\n",
    "                            series_ndx,\n",
    "                            slice_ndx,\n",
    "                        ),\n",
    "                        image_a,\n",
    "                        self.totalTrainingSamples_count,\n",
    "                        dataformats='HWC',\n",
    "                    )\n",
    "                # This flush prevents TB from getting confused about which\n",
    "                # data item belongs where.\n",
    "                writer.flush()\n",
    "\n",
    "    def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n",
    "        log.info(\"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        ))\n",
    "\n",
    "        metrics_a = metrics_t.detach().numpy()\n",
    "        sum_a = metrics_a.sum(axis=1)\n",
    "        assert np.isfinite(metrics_a).all()\n",
    "\n",
    "        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
    "\n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['fnloss/all'] = metrics_a[METRICS_FNLOSS_NDX].mean()\n",
    "        metrics_dict['fploss/all'] = metrics_a[METRICS_FPLOSS_NDX].mean()   \n",
    "\n",
    "        metrics_dict['percent_all/tp'] = \\\n",
    "            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
    "        metrics_dict['percent_all/fn'] = \\\n",
    "            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
    "        metrics_dict['percent_all/fp'] = \\\n",
    "            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
    "\n",
    "\n",
    "        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
    "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
    "        recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n",
    "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
    "\n",
    "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
    "            / ((precision + recall) or 1)\n",
    "        \n",
    "        if precision > 0.7:\n",
    "            self.changeLossWeight = True\n",
    "\n",
    "        log.info((\"E{} {:8} \"\n",
    "                # + \"CL: {}\"\n",
    "                 + \"{loss/all:.4f} loss, \"\n",
    "                  + \"{fnloss/all:.4f} fnloss, \"\n",
    "                  + \"{fploss/all:.4f} fploss, \"\n",
    "                 + \"{pr/precision:.4f} precision, \"\n",
    "                 + \"{pr/recall:.4f} recall, \"\n",
    "                 + \"{pr/f1_score:.4f} f1 score\"\n",
    "                  ).format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            # self.changeLossWeight,\n",
    "            **metrics_dict,\n",
    "        ))\n",
    "        log.info((\"E{} {:8} \"\n",
    "                  + \"{loss/all:.4f} loss, \"\n",
    "                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
    "        ).format(\n",
    "            epoch_ndx,\n",
    "            mode_str + '_all',\n",
    "            **metrics_dict,\n",
    "        ))\n",
    "\n",
    "        self.initTensorboardWriters()\n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "\n",
    "        prefix_str = 'seg_'\n",
    "\n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(prefix_str + key, value, self.totalTrainingSamples_count)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        score = metrics_dict['pr/recall']\n",
    "\n",
    "        return score\n",
    "\n",
    "    def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
    "        file_path = os.path.join(\n",
    "            'models',\n",
    "            self.cli_args.tb_prefix,\n",
    "            '{}_{}_{}.{}.state'.format(\n",
    "                type_str,\n",
    "                self.time_str,\n",
    "                self.cli_args.comment,\n",
    "                self.totalTrainingSamples_count,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
    "\n",
    "        model = self.segmentation_model\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model = model.module\n",
    "\n",
    "        state = {\n",
    "            'sys_argv': sys.argv,\n",
    "            'time': str(datetime.datetime.now()),\n",
    "            'model_state': model.state_dict(),\n",
    "            'model_name': type(model).__name__,\n",
    "            'optimizer_state' : self.optimizer.state_dict(),\n",
    "            'optimizer_name': type(self.optimizer).__name__,\n",
    "            'epoch': epoch_ndx,\n",
    "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
    "        }\n",
    "        torch.save(state, file_path)\n",
    "\n",
    "        log.info(\"Saved model params to {}\".format(file_path))\n",
    "\n",
    "        if isBest:\n",
    "            best_path = os.path.join(\n",
    "               'models',\n",
    "                self.cli_args.tb_prefix,\n",
    "                f'{type_str}_{self.time_str}_{self.cli_args.comment}.best.state')\n",
    "            shutil.copyfile(file_path, best_path)\n",
    "\n",
    "            log.info(\"Saved model params to {}\".format(best_path))\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            log.info(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_ds = TrainingLuna2dSegmentationDataset(\n",
    "#             val_stride=10,\n",
    "#             isValSet_bool=False,\n",
    "#             contextSlices_count=2,\n",
    "#         )\n",
    "# # print(len(train_ds))\n",
    "# total_rate = 0\n",
    "# total = (512 * 512)\n",
    "# i = 0\n",
    "# average_rate = 0\n",
    "# for i in range(8001):\n",
    "#     # if (i % 10 == 0):\n",
    "#     #     # print(\"i = \", i)\n",
    "#     #     average_rate += (total_rate) / 10\n",
    "#     #     # print(\"avg = \", average_rate)\n",
    "#     #     total_rate = 0\n",
    "#     if (i % 1000 == 0):\n",
    "#         print(\"i = \", i)\n",
    "#         print(\"avg = \", average_rate)\n",
    "#     i += 1\n",
    "#     csum = (train_ds[i][1].sum())\n",
    "#     # print(\"sum = \", csum)\n",
    "#     total_rate += csum / total\n",
    "# average_rate = (total_rate) / 8000\n",
    "# print(average_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 00:05:25,824 INFO     pid:22692 __main__:027:main Starting LunaPrepCacheApp, Namespace(batch_size=1024, num_workers=16)\n",
      "2023-12-11 00:21:50,696 WARNING  pid:22692 util:109:enumerateWithEstimate Stuffing cache ----/17, starting\n",
      "2023-12-11 00:29:58,308 WARNING  pid:22692 util:139:enumerateWithEstimate Stuffing cache ----/17, done at 2023-12-11 00:29:58\n"
     ]
    }
   ],
   "source": [
    "LunaPrepCacheApp(sys_argv=[\"--num-workers=16\"]).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import SimpleITK as sitk\n",
    "# import numpy as np\n",
    "# import collections\n",
    "# from PIL import Image, ImageDraw\n",
    "\n",
    "# train_ds = TrainingLuna2dSegmentationDataset(\n",
    "#             series_uid=\"1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860\"\n",
    "#         )\n",
    "# hu_a = train_ds[0][0].numpy()\n",
    "# hu_mask = train_ds[0][1].numpy().astype(int)\n",
    "# print(hu_a.shape)\n",
    "# print(hu_mask.shape)\n",
    "# min_value = np.min(hu_a[3])\n",
    "# max_value = np.max(hu_a[3])\n",
    "# scaled_hu_a = (hu_a[3] - min_value) / (max_value - min_value) * 255\n",
    "# scaled_hu_a = scaled_hu_a.astype(np.uint8)\n",
    "# slice_ori = Image.fromarray(scaled_hu_a, mode='L')\n",
    "# slice_ori.save(\"origin.png\")\n",
    "# min_value_mask = np.min(hu_mask[0])\n",
    "# max_value_mask = np.max(hu_mask[0])\n",
    "# # print(min_value_mask)\n",
    "# # print(max_value_mask)\n",
    "# scaled_mask = hu_mask[0] * 255\n",
    "# slice_mask = Image.fromarray(scaled_mask, mode='L')\n",
    "# slice_mask.save(\"mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = TrainingLuna2dSegmentationDataset(\n",
    "#             series_uid=\"1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860\"\n",
    "#         )\n",
    "# print(train_ds[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct_slice = Image.fromarray(scaled_hu_a, mode='L')\n",
    "\n",
    "# # Create a drawing context on the image\n",
    "# draw = ImageDraw.Draw(ct_slice)\n",
    "\n",
    "# # Define the coordinates to mark (row 212, column 45) as a red rectangle\n",
    "# x1, y1, x2, y2 = 44, 211, 46, 213  # Adjust these coordinates as needed\n",
    "\n",
    "# # Define the outline color as \"red\"\n",
    "# outline_color = (0, 0, 255)  # Use grayscale value 255 for white outline\n",
    "\n",
    "# # Draw a red rectangle on the image to mark the specific row and column\n",
    "# draw.rectangle([x1, y1, x2, y2], outline=\"white\", width=3)  # Increase width for better visibility\n",
    "\n",
    "# # Save the marked slice as a PNG\n",
    "# ct_slice.save(\"marked_slice.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def np2Png(np_arr, target_name):\n",
    "#     min_value = np.min(np_arr[0])\n",
    "#     max_value = np.max(np_arr[0])\n",
    "#     scaled_np_arr = (np_arr[0] - min_value) / (max_value - min_value) * 255\n",
    "#     scaled_np_arr = scaled_np_arr.astype(np.uint8)\n",
    "#     slice_ori = Image.fromarray(scaled_np_arr, mode='L')\n",
    "#     slice_ori.save(target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_model = UNetWrapper(\n",
    "#             in_channels=7,\n",
    "#             n_classes=1,\n",
    "#             depth=2,  #how deep the U go\n",
    "#             wf=6,   #2^4 filter\n",
    "#             padding=True, #padding so that we get the output size as input size\n",
    "#             batch_norm=True,\n",
    "#             up_mode='upconv', #use  nn.ConvTranspose2d\n",
    "#         )\n",
    "# # model_state\n",
    "# # torch.load(\"F:\\\\udet\\\\models\\\\udet\\\\seg_2023-10-19_08.28.18_final-cls.best.state\")[\"model_state\"]\n",
    "# segmentation_model.load_state_dict(torch.load(\"F:\\\\udet\\\\models\\\\udet\\\\u_net_depth2_200epcoch_f1score0.2.state\")[\"model_state\"])\n",
    "# device = torch.device(\"cuda\")\n",
    "# segmentation_model.to(device)\n",
    "# segmentation_model.eval()\n",
    "# val_ds = Luna2dSegmentationDataset(\n",
    "#             val_stride=10,\n",
    "#             isValSet_bool=True,\n",
    "#             contextSlices_count=3,\n",
    "#         )\n",
    "\n",
    "# batch_size = 8\n",
    "\n",
    "# val_dl = DataLoader(\n",
    "#     val_ds,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "# batch_iter = enumerateWithEstimate(\n",
    "#     val_dl,\n",
    "#     \"E{} Validation \".format(1),\n",
    "#     start_ndx=val_dl.num_workers,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_ndx, batch_tup in batch_iter:\n",
    "#     input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
    "\n",
    "#     input_g = input_t.to(device, non_blocking=True)\n",
    "#     label_g = label_t.to(device, non_blocking=True)\n",
    "\n",
    "#     prediction_g = segmentation_model(input_g)\n",
    "#     # np2Png(input_g.cpu().numpy().astype(int), \"./test/test.png\")\n",
    "#     # np2Png(label_g.cpu().numpy().astype(int), \"./test/label.png\")\n",
    "#     # np2Png(prediction_g.cpu().detach().numpy().astype(float), \"./test/predict.png\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_ds = Luna2dSegmentationDataset(\n",
    "#             val_stride=1,\n",
    "#             isValSet_bool=True,\n",
    "#             contextSlices_count=0,\n",
    "#         )\n",
    "# print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# te_data2 = np.random.randint(0,10,(1, 4, 4))\n",
    "# print(te_data2)\n",
    "# print(te_data2.shape)\n",
    "# te_data2  = np.expand_dims(te_data2, axis=3)\n",
    "# print(te_data2.shape)\n",
    "# te_data2 = torch.tensor(te_data2).transpose(1, 3)\n",
    "# print(te_data2.size())\n",
    "# te_data2 = torch.cat([te_data2] * 3, dim=1).numpy()\n",
    "# print(te_data2.shape)\n",
    "# print(te_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(val_ds[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_ds[2][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(val_ds)):\n",
    "#     origin_n = val_ds[i][0].numpy()\n",
    "#     mask_n = val_ds[i][1].numpy()\n",
    "#     mask_ori_n = ((val_ds[i][0].float() + 1001) * val_ds[i][1]).numpy()\n",
    "#     # print(type(origin_n[0]))\n",
    "#     # np2Png(origin_n.astype(int), \"./origin/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))\n",
    "#     # np2Png(mask_n.astype(int), \"./mask/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))\n",
    "#     np2Png(mask_ori_n.astype(int), \"./mask_origin/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(len(val_ds)):\n",
    "#     origin_n = val_ds[i][0].numpy()\n",
    "#     mask_n = val_ds[i][1].numpy()\n",
    "#     mask_ori_n = ((val_ds[i][0].float() + 1001) * val_ds[i][1]).numpy()\n",
    "#     # print(type(origin_n[0]))\n",
    "#     np2Png(origin_n.astype(int), \"./origin_new/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))\n",
    "#     np2Png(mask_n.astype(int), \"./mask_new/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))\n",
    "#     np2Png(mask_ori_n.astype(int), \"./mask_origin_new/{}_{}_{}.png\".format(i, val_ds[i][2], val_ds[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_n = input_g.cpu().numpy()\n",
    "# np2Png(origin_n[0].astype(int), \"./test/test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_g.size()\n",
    "# label_g.size()\n",
    "# label_n = label_g.cpu().numpy()\n",
    "# np2Png(label_n[0].astype(int), \"./test/label.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_n = prediction_g.cpu().detach().numpy()\n",
    "# np2Png(prediction_n[0].astype(float), \"./test/predict.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(prediction_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hZFzy4E2Z6ui",
    "outputId": "39a224af-0945-456d-bdfc-7f1d6de7c73d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 00:29:58,666 INFO     pid:22692 __main__:119:initModel Using CUDA; 1 devices.\n",
      "2023-12-11 00:29:58,809 INFO     pid:22692 __main__:183:main Starting SegmentationTrainingApp, Namespace(batch_size=2, num_workers=16, epochs=200, augmented=True, augment_flip=False, augment_offset=False, augment_scale=False, augment_rotate=False, augment_noise=False, tb_prefix='udet', comment='final-cls')\n",
      "2023-12-11 00:29:58,871 INFO     pid:22692 dsetsFullCT:385:__init__ <dsetsFullCT.TrainingLuna2dSegmentationDataset object at 0x000002450000AED0>: 480 training series, 931 candidates, 13508 slices, 931 nodules\n",
      "2023-12-11 00:29:58,878 INFO     pid:22692 dsetsFullCT:385:__init__ <dsetsFullCT.Luna2dSegmentationDataset object at 0x0000024500179390>: 60 validation series, 124 candidates, 1723 slices, 124 nodules\n",
      "2023-12-11 00:29:58,879 INFO     pid:22692 __main__:191:main Epoch 1 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 00:30:00,487 WARNING  pid:22692 util:109:enumerateWithEstimate E1 Training ----/6754, starting\n",
      "2023-12-11 00:30:49,077 INFO     pid:22692 util:126:enumerateWithEstimate E1 Training   64/6754, done at 2023-12-11 00:37:53, 0:07:07\n",
      "2023-12-11 00:31:01,205 INFO     pid:22692 util:126:enumerateWithEstimate E1 Training  256/6754, done at 2023-12-11 00:37:51, 0:07:05\n",
      "2023-12-11 00:31:49,867 INFO     pid:22692 util:126:enumerateWithEstimate E1 Training 1024/6754, done at 2023-12-11 00:37:52, 0:07:06\n",
      "2023-12-11 00:35:04,490 INFO     pid:22692 util:126:enumerateWithEstimate E1 Training 4096/6754, done at 2023-12-11 00:37:52, 0:07:06\n",
      "2023-12-11 00:37:54,627 WARNING  pid:22692 util:139:enumerateWithEstimate E1 Training ----/6754, done at 2023-12-11 00:37:54\n",
      "2023-12-11 00:37:54,627 INFO     pid:22692 __main__:409:logMetrics E1 SegmentationTrainingApp\n",
      "2023-12-11 00:37:54,638 INFO     pid:22692 __main__:444:logMetrics E1 trn      0.7222 loss, 0.4993 fnloss, 0.9985 fploss, 0.0379 precision, 0.4385 recall, 0.0697 f1 score\n",
      "2023-12-11 00:37:54,638 INFO     pid:22692 __main__:458:logMetrics E1 trn_all  0.7222 loss,  43.8% tp,  56.2% fn,    1114.4% fp\n",
      "2023-12-11 00:37:54,660 WARNING  pid:22692 util:109:enumerateWithEstimate E1 Validation  ----/862, starting\n",
      "2023-12-11 00:38:00,997 INFO     pid:22692 util:126:enumerateWithEstimate E1 Validation    64/862, done at 2023-12-11 00:38:15, 0:00:15\n",
      "2023-12-11 00:38:04,463 INFO     pid:22692 util:126:enumerateWithEstimate E1 Validation   256/862, done at 2023-12-11 00:38:15, 0:00:15\n",
      "2023-12-11 00:38:16,907 WARNING  pid:22692 util:139:enumerateWithEstimate E1 Validation  ----/862, done at 2023-12-11 00:38:16\n",
      "2023-12-11 00:38:16,907 INFO     pid:22692 __main__:409:logMetrics E1 SegmentationTrainingApp\n",
      "2023-12-11 00:38:16,907 INFO     pid:22692 __main__:444:logMetrics E1 val      0.8090 loss, 0.7854 fnloss, 0.9927 fploss, 0.6850 precision, 0.3328 recall, 0.4480 f1 score\n",
      "2023-12-11 00:38:16,907 INFO     pid:22692 __main__:458:logMetrics E1 val_all  0.8090 loss,  33.3% tp,  66.7% fn,      15.3% fp\n",
      "2023-12-11 00:38:17,025 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.13508.state\n",
      "2023-12-11 00:38:17,067 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 00:38:17,126 INFO     pid:22692 __main__:523:saveModel SHA1: febbe9099f92d791ab0fe0f40cf9cdb3df64136b\n",
      "2023-12-11 00:39:12,559 INFO     pid:22692 __main__:191:main Epoch 2 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 00:39:12,578 WARNING  pid:22692 util:109:enumerateWithEstimate E2 Training ----/6754, starting\n",
      "2023-12-11 00:39:55,456 INFO     pid:22692 util:126:enumerateWithEstimate E2 Training   64/6754, done at 2023-12-11 00:46:51, 0:06:58\n",
      "2023-12-11 00:40:07,504 INFO     pid:22692 util:126:enumerateWithEstimate E2 Training  256/6754, done at 2023-12-11 00:46:54, 0:07:01\n",
      "2023-12-11 00:40:55,691 INFO     pid:22692 util:126:enumerateWithEstimate E2 Training 1024/6754, done at 2023-12-11 00:46:54, 0:07:02\n",
      "2023-12-11 00:44:07,210 INFO     pid:22692 util:126:enumerateWithEstimate E2 Training 4096/6754, done at 2023-12-11 00:46:53, 0:07:00\n",
      "2023-12-11 00:46:54,389 WARNING  pid:22692 util:139:enumerateWithEstimate E2 Training ----/6754, done at 2023-12-11 00:46:54\n",
      "2023-12-11 00:46:54,401 INFO     pid:22692 __main__:409:logMetrics E2 SegmentationTrainingApp\n",
      "2023-12-11 00:46:54,403 INFO     pid:22692 __main__:444:logMetrics E2 trn      0.5898 loss, 0.4394 fnloss, 0.9978 fploss, 0.4257 precision, 0.4577 recall, 0.4411 f1 score\n",
      "2023-12-11 00:46:54,404 INFO     pid:22692 __main__:458:logMetrics E2 trn_all  0.5898 loss,  45.8% tp,  54.2% fn,      61.8% fp\n",
      "2023-12-11 00:46:54,408 INFO     pid:22692 __main__:191:main Epoch 3 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 00:46:54,415 WARNING  pid:22692 util:109:enumerateWithEstimate E3 Training ----/6754, starting\n",
      "2023-12-11 00:47:37,693 INFO     pid:22692 util:126:enumerateWithEstimate E3 Training   64/6754, done at 2023-12-11 00:54:35, 0:07:01\n",
      "2023-12-11 00:47:49,798 INFO     pid:22692 util:126:enumerateWithEstimate E3 Training  256/6754, done at 2023-12-11 00:54:38, 0:07:04\n",
      "2023-12-11 00:48:37,958 INFO     pid:22692 util:126:enumerateWithEstimate E3 Training 1024/6754, done at 2023-12-11 00:54:37, 0:07:02\n",
      "2023-12-11 00:51:49,474 INFO     pid:22692 util:126:enumerateWithEstimate E3 Training 4096/6754, done at 2023-12-11 00:54:35, 0:07:00\n",
      "2023-12-11 00:54:36,785 WARNING  pid:22692 util:139:enumerateWithEstimate E3 Training ----/6754, done at 2023-12-11 00:54:36\n",
      "2023-12-11 00:54:36,787 INFO     pid:22692 __main__:409:logMetrics E3 SegmentationTrainingApp\n",
      "2023-12-11 00:54:36,788 INFO     pid:22692 __main__:444:logMetrics E3 trn      0.5372 loss, 0.3910 fnloss, 0.9974 fploss, 0.5305 precision, 0.4914 recall, 0.5102 f1 score\n",
      "2023-12-11 00:54:36,788 INFO     pid:22692 __main__:458:logMetrics E3 trn_all  0.5372 loss,  49.1% tp,  50.9% fn,      43.5% fp\n",
      "2023-12-11 00:54:36,792 INFO     pid:22692 __main__:191:main Epoch 4 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 00:54:36,798 WARNING  pid:22692 util:109:enumerateWithEstimate E4 Training ----/6754, starting\n",
      "2023-12-11 00:55:19,720 INFO     pid:22692 util:126:enumerateWithEstimate E4 Training   64/6754, done at 2023-12-11 01:02:19, 0:07:02\n",
      "2023-12-11 00:55:31,837 INFO     pid:22692 util:126:enumerateWithEstimate E4 Training  256/6754, done at 2023-12-11 01:02:21, 0:07:04\n",
      "2023-12-11 00:56:20,107 INFO     pid:22692 util:126:enumerateWithEstimate E4 Training 1024/6754, done at 2023-12-11 01:02:20, 0:07:03\n",
      "2023-12-11 00:59:31,721 INFO     pid:22692 util:126:enumerateWithEstimate E4 Training 4096/6754, done at 2023-12-11 01:02:17, 0:07:01\n",
      "2023-12-11 01:02:19,159 WARNING  pid:22692 util:139:enumerateWithEstimate E4 Training ----/6754, done at 2023-12-11 01:02:19\n",
      "2023-12-11 01:02:19,159 INFO     pid:22692 __main__:409:logMetrics E4 SegmentationTrainingApp\n",
      "2023-12-11 01:02:19,159 INFO     pid:22692 __main__:444:logMetrics E4 trn      0.4959 loss, 0.3512 fnloss, 0.9972 fploss, 0.5908 precision, 0.5264 recall, 0.5568 f1 score\n",
      "2023-12-11 01:02:19,170 INFO     pid:22692 __main__:458:logMetrics E4 trn_all  0.4959 loss,  52.6% tp,  47.4% fn,      36.5% fp\n",
      "2023-12-11 01:02:19,173 INFO     pid:22692 __main__:191:main Epoch 5 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:02:19,177 WARNING  pid:22692 util:109:enumerateWithEstimate E5 Training ----/6754, starting\n",
      "2023-12-11 01:03:02,236 INFO     pid:22692 util:126:enumerateWithEstimate E5 Training   64/6754, done at 2023-12-11 01:10:02, 0:07:03\n",
      "2023-12-11 01:03:14,330 INFO     pid:22692 util:126:enumerateWithEstimate E5 Training  256/6754, done at 2023-12-11 01:10:03, 0:07:04\n",
      "2023-12-11 01:04:02,651 INFO     pid:22692 util:126:enumerateWithEstimate E5 Training 1024/6754, done at 2023-12-11 01:10:03, 0:07:04\n",
      "2023-12-11 01:07:14,045 INFO     pid:22692 util:126:enumerateWithEstimate E5 Training 4096/6754, done at 2023-12-11 01:09:59, 0:07:00\n",
      "2023-12-11 01:10:01,251 WARNING  pid:22692 util:139:enumerateWithEstimate E5 Training ----/6754, done at 2023-12-11 01:10:01\n",
      "2023-12-11 01:10:01,258 INFO     pid:22692 __main__:409:logMetrics E5 SegmentationTrainingApp\n",
      "2023-12-11 01:10:01,259 INFO     pid:22692 __main__:444:logMetrics E5 trn      0.4564 loss, 0.3185 fnloss, 0.9969 fploss, 0.6443 precision, 0.5577 recall, 0.5979 f1 score\n",
      "2023-12-11 01:10:01,261 INFO     pid:22692 __main__:458:logMetrics E5 trn_all  0.4564 loss,  55.8% tp,  44.2% fn,      30.8% fp\n",
      "2023-12-11 01:10:01,267 WARNING  pid:22692 util:109:enumerateWithEstimate E5 Validation  ----/862, starting\n",
      "2023-12-11 01:10:07,748 INFO     pid:22692 util:126:enumerateWithEstimate E5 Validation    64/862, done at 2023-12-11 01:10:22, 0:00:15\n",
      "2023-12-11 01:10:11,277 INFO     pid:22692 util:126:enumerateWithEstimate E5 Validation   256/862, done at 2023-12-11 01:10:22, 0:00:15\n",
      "2023-12-11 01:10:23,648 WARNING  pid:22692 util:139:enumerateWithEstimate E5 Validation  ----/862, done at 2023-12-11 01:10:23\n",
      "2023-12-11 01:10:23,648 INFO     pid:22692 __main__:409:logMetrics E5 SegmentationTrainingApp\n",
      "2023-12-11 01:10:23,661 INFO     pid:22692 __main__:444:logMetrics E5 val      0.6712 loss, 0.6339 fnloss, 0.9918 fploss, 0.7962 precision, 0.5161 recall, 0.6263 f1 score\n",
      "2023-12-11 01:10:23,661 INFO     pid:22692 __main__:458:logMetrics E5 val_all  0.6712 loss,  51.6% tp,  48.4% fn,      13.2% fp\n",
      "2023-12-11 01:10:23,771 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.67540.state\n",
      "2023-12-11 01:10:23,807 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 01:10:23,860 INFO     pid:22692 __main__:523:saveModel SHA1: fa35692f31275f49ae3fd0a0a4ed657c77c42205\n",
      "2023-12-11 01:11:11,947 INFO     pid:22692 __main__:191:main Epoch 6 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:11:11,961 WARNING  pid:22692 util:109:enumerateWithEstimate E6 Training ----/6754, starting\n",
      "2023-12-11 01:11:54,791 INFO     pid:22692 util:126:enumerateWithEstimate E6 Training   64/6754, done at 2023-12-11 01:18:48, 0:06:57\n",
      "2023-12-11 01:12:06,831 INFO     pid:22692 util:126:enumerateWithEstimate E6 Training  256/6754, done at 2023-12-11 01:18:53, 0:07:01\n",
      "2023-12-11 01:12:55,071 INFO     pid:22692 util:126:enumerateWithEstimate E6 Training 1024/6754, done at 2023-12-11 01:18:54, 0:07:02\n",
      "2023-12-11 01:16:06,723 INFO     pid:22692 util:126:enumerateWithEstimate E6 Training 4096/6754, done at 2023-12-11 01:18:52, 0:07:00\n",
      "2023-12-11 01:18:53,895 WARNING  pid:22692 util:139:enumerateWithEstimate E6 Training ----/6754, done at 2023-12-11 01:18:53\n",
      "2023-12-11 01:18:53,895 INFO     pid:22692 __main__:409:logMetrics E6 SegmentationTrainingApp\n",
      "2023-12-11 01:18:53,895 INFO     pid:22692 __main__:444:logMetrics E6 trn      0.4341 loss, 0.2939 fnloss, 0.9969 fploss, 0.6549 precision, 0.5869 recall, 0.6191 f1 score\n",
      "2023-12-11 01:18:53,902 INFO     pid:22692 __main__:458:logMetrics E6 trn_all  0.4341 loss,  58.7% tp,  41.3% fn,      30.9% fp\n",
      "2023-12-11 01:18:53,906 INFO     pid:22692 __main__:191:main Epoch 7 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:18:53,911 WARNING  pid:22692 util:109:enumerateWithEstimate E7 Training ----/6754, starting\n",
      "2023-12-11 01:19:37,054 INFO     pid:22692 util:126:enumerateWithEstimate E7 Training   64/6754, done at 2023-12-11 01:26:37, 0:07:03\n",
      "2023-12-11 01:19:49,146 INFO     pid:22692 util:126:enumerateWithEstimate E7 Training  256/6754, done at 2023-12-11 01:26:38, 0:07:04\n",
      "2023-12-11 01:20:37,472 INFO     pid:22692 util:126:enumerateWithEstimate E7 Training 1024/6754, done at 2023-12-11 01:26:38, 0:07:04\n",
      "2023-12-11 01:23:48,894 INFO     pid:22692 util:126:enumerateWithEstimate E7 Training 4096/6754, done at 2023-12-11 01:26:34, 0:07:00\n",
      "2023-12-11 01:26:36,154 WARNING  pid:22692 util:139:enumerateWithEstimate E7 Training ----/6754, done at 2023-12-11 01:26:36\n",
      "2023-12-11 01:26:36,154 INFO     pid:22692 __main__:409:logMetrics E7 SegmentationTrainingApp\n",
      "2023-12-11 01:26:36,154 INFO     pid:22692 __main__:444:logMetrics E7 trn      0.4091 loss, 0.2777 fnloss, 0.9967 fploss, 0.6899 precision, 0.5998 recall, 0.6417 f1 score\n",
      "2023-12-11 01:26:36,168 INFO     pid:22692 __main__:458:logMetrics E7 trn_all  0.4091 loss,  60.0% tp,  40.0% fn,      27.0% fp\n",
      "2023-12-11 01:26:36,171 INFO     pid:22692 __main__:191:main Epoch 8 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:26:36,178 WARNING  pid:22692 util:109:enumerateWithEstimate E8 Training ----/6754, starting\n",
      "2023-12-11 01:27:19,119 INFO     pid:22692 util:126:enumerateWithEstimate E8 Training   64/6754, done at 2023-12-11 01:34:18, 0:07:02\n",
      "2023-12-11 01:27:31,217 INFO     pid:22692 util:126:enumerateWithEstimate E8 Training  256/6754, done at 2023-12-11 01:34:20, 0:07:04\n",
      "2023-12-11 01:28:19,600 INFO     pid:22692 util:126:enumerateWithEstimate E8 Training 1024/6754, done at 2023-12-11 01:34:20, 0:07:04\n",
      "2023-12-11 01:31:31,254 INFO     pid:22692 util:126:enumerateWithEstimate E8 Training 4096/6754, done at 2023-12-11 01:34:17, 0:07:01\n",
      "2023-12-11 01:34:18,480 WARNING  pid:22692 util:139:enumerateWithEstimate E8 Training ----/6754, done at 2023-12-11 01:34:18\n",
      "2023-12-11 01:34:18,480 INFO     pid:22692 __main__:409:logMetrics E8 SegmentationTrainingApp\n",
      "2023-12-11 01:34:18,488 INFO     pid:22692 __main__:444:logMetrics E8 trn      0.4025 loss, 0.2679 fnloss, 0.9966 fploss, 0.6838 precision, 0.6150 recall, 0.6476 f1 score\n",
      "2023-12-11 01:34:18,489 INFO     pid:22692 __main__:458:logMetrics E8 trn_all  0.4025 loss,  61.5% tp,  38.5% fn,      28.4% fp\n",
      "2023-12-11 01:34:18,492 INFO     pid:22692 __main__:191:main Epoch 9 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:34:18,498 WARNING  pid:22692 util:109:enumerateWithEstimate E9 Training ----/6754, starting\n",
      "2023-12-11 01:35:02,161 INFO     pid:22692 util:126:enumerateWithEstimate E9 Training   64/6754, done at 2023-12-11 01:42:04, 0:07:05\n",
      "2023-12-11 01:35:14,254 INFO     pid:22692 util:126:enumerateWithEstimate E9 Training  256/6754, done at 2023-12-11 01:42:03, 0:07:04\n",
      "2023-12-11 01:36:02,640 INFO     pid:22692 util:126:enumerateWithEstimate E9 Training 1024/6754, done at 2023-12-11 01:42:03, 0:07:04\n",
      "2023-12-11 01:39:14,200 INFO     pid:22692 util:126:enumerateWithEstimate E9 Training 4096/6754, done at 2023-12-11 01:42:00, 0:07:01\n",
      "2023-12-11 01:42:01,404 WARNING  pid:22692 util:139:enumerateWithEstimate E9 Training ----/6754, done at 2023-12-11 01:42:01\n",
      "2023-12-11 01:42:01,404 INFO     pid:22692 __main__:409:logMetrics E9 SegmentationTrainingApp\n",
      "2023-12-11 01:42:01,404 INFO     pid:22692 __main__:444:logMetrics E9 trn      0.3927 loss, 0.2663 fnloss, 0.9965 fploss, 0.7039 precision, 0.6215 recall, 0.6601 f1 score\n",
      "2023-12-11 01:42:01,419 INFO     pid:22692 __main__:458:logMetrics E9 trn_all  0.3927 loss,  62.2% tp,  37.8% fn,      26.1% fp\n",
      "2023-12-11 01:42:01,419 INFO     pid:22692 __main__:191:main Epoch 10 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:42:01,419 WARNING  pid:22692 util:109:enumerateWithEstimate E10 Training ----/6754, starting\n",
      "2023-12-11 01:42:44,399 INFO     pid:22692 util:126:enumerateWithEstimate E10 Training   64/6754, done at 2023-12-11 01:49:42, 0:07:01\n",
      "2023-12-11 01:42:56,492 INFO     pid:22692 util:126:enumerateWithEstimate E10 Training  256/6754, done at 2023-12-11 01:49:45, 0:07:03\n",
      "2023-12-11 01:43:44,567 INFO     pid:22692 util:126:enumerateWithEstimate E10 Training 1024/6754, done at 2023-12-11 01:49:43, 0:07:02\n",
      "2023-12-11 01:46:56,054 INFO     pid:22692 util:126:enumerateWithEstimate E10 Training 4096/6754, done at 2023-12-11 01:49:41, 0:07:00\n",
      "2023-12-11 01:49:43,279 WARNING  pid:22692 util:139:enumerateWithEstimate E10 Training ----/6754, done at 2023-12-11 01:49:43\n",
      "2023-12-11 01:49:43,279 INFO     pid:22692 __main__:409:logMetrics E10 SegmentationTrainingApp\n",
      "2023-12-11 01:49:43,279 INFO     pid:22692 __main__:444:logMetrics E10 trn      0.3790 loss, 0.2486 fnloss, 0.9965 fploss, 0.7112 precision, 0.6348 recall, 0.6708 f1 score\n",
      "2023-12-11 01:49:43,279 INFO     pid:22692 __main__:458:logMetrics E10 trn_all  0.3790 loss,  63.5% tp,  36.5% fn,      25.8% fp\n",
      "2023-12-11 01:49:43,279 WARNING  pid:22692 util:109:enumerateWithEstimate E10 Validation  ----/862, starting\n",
      "2023-12-11 01:49:49,718 INFO     pid:22692 util:126:enumerateWithEstimate E10 Validation    64/862, done at 2023-12-11 01:50:04, 0:00:15\n",
      "2023-12-11 01:49:53,171 INFO     pid:22692 util:126:enumerateWithEstimate E10 Validation   256/862, done at 2023-12-11 01:50:04, 0:00:15\n",
      "2023-12-11 01:50:05,623 WARNING  pid:22692 util:139:enumerateWithEstimate E10 Validation  ----/862, done at 2023-12-11 01:50:05\n",
      "2023-12-11 01:50:05,623 INFO     pid:22692 __main__:409:logMetrics E10 SegmentationTrainingApp\n",
      "2023-12-11 01:50:05,623 INFO     pid:22692 __main__:444:logMetrics E10 val      0.4756 loss, 0.3835 fnloss, 0.9929 fploss, 0.7397 precision, 0.7031 recall, 0.7209 f1 score\n",
      "2023-12-11 01:50:05,639 INFO     pid:22692 __main__:458:logMetrics E10 val_all  0.4756 loss,  70.3% tp,  29.7% fn,      24.7% fp\n",
      "2023-12-11 01:50:05,764 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.135080.state\n",
      "2023-12-11 01:50:05,795 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 01:50:05,857 INFO     pid:22692 __main__:523:saveModel SHA1: d52feec5485675bd6faa5be4edcf3ba815340cef\n",
      "2023-12-11 01:50:51,775 INFO     pid:22692 __main__:191:main Epoch 11 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:50:51,775 WARNING  pid:22692 util:109:enumerateWithEstimate E11 Training ----/6754, starting\n",
      "2023-12-11 01:51:34,568 INFO     pid:22692 util:126:enumerateWithEstimate E11 Training   64/6754, done at 2023-12-11 01:58:30, 0:06:58\n",
      "2023-12-11 01:51:46,568 INFO     pid:22692 util:126:enumerateWithEstimate E11 Training  256/6754, done at 2023-12-11 01:58:32, 0:07:00\n",
      "2023-12-11 01:52:34,815 INFO     pid:22692 util:126:enumerateWithEstimate E11 Training 1024/6754, done at 2023-12-11 01:58:34, 0:07:02\n",
      "2023-12-11 01:55:46,271 INFO     pid:22692 util:126:enumerateWithEstimate E11 Training 4096/6754, done at 2023-12-11 01:58:32, 0:07:00\n",
      "2023-12-11 01:58:33,508 WARNING  pid:22692 util:139:enumerateWithEstimate E11 Training ----/6754, done at 2023-12-11 01:58:33\n",
      "2023-12-11 01:58:33,508 INFO     pid:22692 __main__:409:logMetrics E11 SegmentationTrainingApp\n",
      "2023-12-11 01:58:33,508 INFO     pid:22692 __main__:444:logMetrics E11 trn      0.3722 loss, 0.2524 fnloss, 0.9964 fploss, 0.7320 precision, 0.6373 recall, 0.6814 f1 score\n",
      "2023-12-11 01:58:33,508 INFO     pid:22692 __main__:458:logMetrics E11 trn_all  0.3722 loss,  63.7% tp,  36.3% fn,      23.3% fp\n",
      "2023-12-11 01:58:33,508 INFO     pid:22692 __main__:191:main Epoch 12 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 01:58:33,508 WARNING  pid:22692 util:109:enumerateWithEstimate E12 Training ----/6754, starting\n",
      "2023-12-11 01:59:16,472 INFO     pid:22692 util:126:enumerateWithEstimate E12 Training   64/6754, done at 2023-12-11 02:06:16, 0:07:03\n",
      "2023-12-11 01:59:28,565 INFO     pid:22692 util:126:enumerateWithEstimate E12 Training  256/6754, done at 2023-12-11 02:06:17, 0:07:04\n",
      "2023-12-11 02:00:16,955 INFO     pid:22692 util:126:enumerateWithEstimate E12 Training 1024/6754, done at 2023-12-11 02:06:17, 0:07:04\n",
      "2023-12-11 02:03:30,926 INFO     pid:22692 util:126:enumerateWithEstimate E12 Training 4096/6754, done at 2023-12-11 02:06:18, 0:07:05\n",
      "2023-12-11 02:06:20,099 WARNING  pid:22692 util:139:enumerateWithEstimate E12 Training ----/6754, done at 2023-12-11 02:06:20\n",
      "2023-12-11 02:06:20,099 INFO     pid:22692 __main__:409:logMetrics E12 SegmentationTrainingApp\n",
      "2023-12-11 02:06:20,099 INFO     pid:22692 __main__:444:logMetrics E12 trn      0.3611 loss, 0.2354 fnloss, 0.9964 fploss, 0.6972 precision, 0.6609 recall, 0.6786 f1 score\n",
      "2023-12-11 02:06:20,099 INFO     pid:22692 __main__:458:logMetrics E12 trn_all  0.3611 loss,  66.1% tp,  33.9% fn,      28.7% fp\n",
      "2023-12-11 02:06:20,099 INFO     pid:22692 __main__:191:main Epoch 13 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:06:20,115 WARNING  pid:22692 util:109:enumerateWithEstimate E13 Training ----/6754, starting\n",
      "2023-12-11 02:07:03,266 INFO     pid:22692 util:126:enumerateWithEstimate E13 Training   64/6754, done at 2023-12-11 02:14:03, 0:07:03\n",
      "2023-12-11 02:07:15,359 INFO     pid:22692 util:126:enumerateWithEstimate E13 Training  256/6754, done at 2023-12-11 02:14:04, 0:07:04\n",
      "2023-12-11 02:08:03,779 INFO     pid:22692 util:126:enumerateWithEstimate E13 Training 1024/6754, done at 2023-12-11 02:14:04, 0:07:04\n",
      "2023-12-11 02:11:17,608 INFO     pid:22692 util:126:enumerateWithEstimate E13 Training 4096/6754, done at 2023-12-11 02:14:05, 0:07:05\n",
      "2023-12-11 02:14:06,751 WARNING  pid:22692 util:139:enumerateWithEstimate E13 Training ----/6754, done at 2023-12-11 02:14:06\n",
      "2023-12-11 02:14:06,751 INFO     pid:22692 __main__:409:logMetrics E13 SegmentationTrainingApp\n",
      "2023-12-11 02:14:06,751 INFO     pid:22692 __main__:444:logMetrics E13 trn      0.3580 loss, 0.2414 fnloss, 0.9963 fploss, 0.7084 precision, 0.6570 recall, 0.6817 f1 score\n",
      "2023-12-11 02:14:06,751 INFO     pid:22692 __main__:458:logMetrics E13 trn_all  0.3580 loss,  65.7% tp,  34.3% fn,      27.0% fp\n",
      "2023-12-11 02:14:06,767 INFO     pid:22692 __main__:191:main Epoch 14 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:14:06,767 WARNING  pid:22692 util:109:enumerateWithEstimate E14 Training ----/6754, starting\n",
      "2023-12-11 02:14:49,762 INFO     pid:22692 util:126:enumerateWithEstimate E14 Training   64/6754, done at 2023-12-11 02:21:49, 0:07:03\n",
      "2023-12-11 02:15:01,872 INFO     pid:22692 util:126:enumerateWithEstimate E14 Training  256/6754, done at 2023-12-11 02:21:51, 0:07:04\n",
      "2023-12-11 02:15:50,010 INFO     pid:22692 util:126:enumerateWithEstimate E14 Training 1024/6754, done at 2023-12-11 02:21:49, 0:07:02\n",
      "2023-12-11 02:19:01,417 INFO     pid:22692 util:126:enumerateWithEstimate E14 Training 4096/6754, done at 2023-12-11 02:21:47, 0:07:00\n",
      "2023-12-11 02:21:48,376 WARNING  pid:22692 util:139:enumerateWithEstimate E14 Training ----/6754, done at 2023-12-11 02:21:48\n",
      "2023-12-11 02:21:48,376 INFO     pid:22692 __main__:409:logMetrics E14 SegmentationTrainingApp\n",
      "2023-12-11 02:21:48,376 INFO     pid:22692 __main__:444:logMetrics E14 trn      0.3564 loss, 0.2352 fnloss, 0.9963 fploss, 0.7341 precision, 0.6667 recall, 0.6988 f1 score\n",
      "2023-12-11 02:21:48,376 INFO     pid:22692 __main__:458:logMetrics E14 trn_all  0.3564 loss,  66.7% tp,  33.3% fn,      24.1% fp\n",
      "2023-12-11 02:21:48,376 INFO     pid:22692 __main__:191:main Epoch 15 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:21:48,376 WARNING  pid:22692 util:109:enumerateWithEstimate E15 Training ----/6754, starting\n",
      "2023-12-11 02:22:31,605 INFO     pid:22692 util:126:enumerateWithEstimate E15 Training   64/6754, done at 2023-12-11 02:29:31, 0:07:03\n",
      "2023-12-11 02:22:43,714 INFO     pid:22692 util:126:enumerateWithEstimate E15 Training  256/6754, done at 2023-12-11 02:29:33, 0:07:04\n",
      "2023-12-11 02:23:31,928 INFO     pid:22692 util:126:enumerateWithEstimate E15 Training 1024/6754, done at 2023-12-11 02:29:31, 0:07:03\n",
      "2023-12-11 02:26:43,290 INFO     pid:22692 util:126:enumerateWithEstimate E15 Training 4096/6754, done at 2023-12-11 02:29:29, 0:07:00\n",
      "2023-12-11 02:29:30,515 WARNING  pid:22692 util:139:enumerateWithEstimate E15 Training ----/6754, done at 2023-12-11 02:29:30\n",
      "2023-12-11 02:29:30,515 INFO     pid:22692 __main__:409:logMetrics E15 SegmentationTrainingApp\n",
      "2023-12-11 02:29:30,515 INFO     pid:22692 __main__:444:logMetrics E15 trn      0.3422 loss, 0.2280 fnloss, 0.9962 fploss, 0.7591 precision, 0.6666 recall, 0.7099 f1 score\n",
      "2023-12-11 02:29:30,515 INFO     pid:22692 __main__:458:logMetrics E15 trn_all  0.3422 loss,  66.7% tp,  33.3% fn,      21.2% fp\n",
      "2023-12-11 02:29:30,531 WARNING  pid:22692 util:109:enumerateWithEstimate E15 Validation  ----/862, starting\n",
      "2023-12-11 02:29:36,722 INFO     pid:22692 util:126:enumerateWithEstimate E15 Validation    64/862, done at 2023-12-11 02:29:51, 0:00:15\n",
      "2023-12-11 02:29:40,174 INFO     pid:22692 util:126:enumerateWithEstimate E15 Validation   256/862, done at 2023-12-11 02:29:51, 0:00:15\n",
      "2023-12-11 02:29:52,470 WARNING  pid:22692 util:139:enumerateWithEstimate E15 Validation  ----/862, done at 2023-12-11 02:29:52\n",
      "2023-12-11 02:29:52,470 INFO     pid:22692 __main__:409:logMetrics E15 SegmentationTrainingApp\n",
      "2023-12-11 02:29:52,470 INFO     pid:22692 __main__:444:logMetrics E15 val      0.5020 loss, 0.4687 fnloss, 0.9916 fploss, 0.8817 precision, 0.6595 recall, 0.7546 f1 score\n",
      "2023-12-11 02:29:52,470 INFO     pid:22692 __main__:458:logMetrics E15 val_all  0.5020 loss,  65.9% tp,  34.1% fn,       8.8% fp\n",
      "2023-12-11 02:29:52,595 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.202620.state\n",
      "2023-12-11 02:29:52,657 INFO     pid:22692 __main__:523:saveModel SHA1: aa1a8216be9c1d9f614299d3c2b8420665c5b94d\n",
      "2023-12-11 02:30:38,378 INFO     pid:22692 __main__:191:main Epoch 16 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:30:38,378 WARNING  pid:22692 util:109:enumerateWithEstimate E16 Training ----/6754, starting\n",
      "2023-12-11 02:31:21,150 INFO     pid:22692 util:126:enumerateWithEstimate E16 Training   64/6754, done at 2023-12-11 02:38:17, 0:06:58\n",
      "2023-12-11 02:31:33,195 INFO     pid:22692 util:126:enumerateWithEstimate E16 Training  256/6754, done at 2023-12-11 02:38:20, 0:07:01\n",
      "2023-12-11 02:32:21,316 INFO     pid:22692 util:126:enumerateWithEstimate E16 Training 1024/6754, done at 2023-12-11 02:38:20, 0:07:02\n",
      "2023-12-11 02:35:32,905 INFO     pid:22692 util:126:enumerateWithEstimate E16 Training 4096/6754, done at 2023-12-11 02:38:18, 0:07:00\n",
      "2023-12-11 02:38:20,075 WARNING  pid:22692 util:139:enumerateWithEstimate E16 Training ----/6754, done at 2023-12-11 02:38:20\n",
      "2023-12-11 02:38:20,075 INFO     pid:22692 __main__:409:logMetrics E16 SegmentationTrainingApp\n",
      "2023-12-11 02:38:20,075 INFO     pid:22692 __main__:444:logMetrics E16 trn      0.3418 loss, 0.2155 fnloss, 0.9963 fploss, 0.7419 precision, 0.6890 recall, 0.7144 f1 score\n",
      "2023-12-11 02:38:20,091 INFO     pid:22692 __main__:458:logMetrics E16 trn_all  0.3418 loss,  68.9% tp,  31.1% fn,      24.0% fp\n",
      "2023-12-11 02:38:20,091 INFO     pid:22692 __main__:191:main Epoch 17 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:38:20,091 WARNING  pid:22692 util:109:enumerateWithEstimate E17 Training ----/6754, starting\n",
      "2023-12-11 02:39:03,055 INFO     pid:22692 util:126:enumerateWithEstimate E17 Training   64/6754, done at 2023-12-11 02:46:01, 0:07:01\n",
      "2023-12-11 02:39:15,132 INFO     pid:22692 util:126:enumerateWithEstimate E17 Training  256/6754, done at 2023-12-11 02:46:03, 0:07:03\n",
      "2023-12-11 02:40:03,268 INFO     pid:22692 util:126:enumerateWithEstimate E17 Training 1024/6754, done at 2023-12-11 02:46:02, 0:07:02\n",
      "2023-12-11 02:43:14,592 INFO     pid:22692 util:126:enumerateWithEstimate E17 Training 4096/6754, done at 2023-12-11 02:46:00, 0:07:00\n",
      "2023-12-11 02:46:01,762 WARNING  pid:22692 util:139:enumerateWithEstimate E17 Training ----/6754, done at 2023-12-11 02:46:01\n",
      "2023-12-11 02:46:01,762 INFO     pid:22692 __main__:409:logMetrics E17 SegmentationTrainingApp\n",
      "2023-12-11 02:46:01,762 INFO     pid:22692 __main__:444:logMetrics E17 trn      0.3428 loss, 0.2227 fnloss, 0.9963 fploss, 0.7513 precision, 0.6765 recall, 0.7119 f1 score\n",
      "2023-12-11 02:46:01,762 INFO     pid:22692 __main__:458:logMetrics E17 trn_all  0.3428 loss,  67.6% tp,  32.4% fn,      22.4% fp\n",
      "2023-12-11 02:46:01,777 INFO     pid:22692 __main__:191:main Epoch 18 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:46:01,777 WARNING  pid:22692 util:109:enumerateWithEstimate E18 Training ----/6754, starting\n",
      "2023-12-11 02:46:45,085 INFO     pid:22692 util:126:enumerateWithEstimate E18 Training   64/6754, done at 2023-12-11 02:53:45, 0:07:03\n",
      "2023-12-11 02:46:57,162 INFO     pid:22692 util:126:enumerateWithEstimate E18 Training  256/6754, done at 2023-12-11 02:53:45, 0:07:03\n",
      "2023-12-11 02:47:48,439 INFO     pid:22692 util:126:enumerateWithEstimate E18 Training 1024/6754, done at 2023-12-11 02:54:05, 0:07:23\n",
      "2023-12-11 02:50:59,825 INFO     pid:22692 util:126:enumerateWithEstimate E18 Training 4096/6754, done at 2023-12-11 02:53:47, 0:07:05\n",
      "2023-12-11 02:53:46,917 WARNING  pid:22692 util:139:enumerateWithEstimate E18 Training ----/6754, done at 2023-12-11 02:53:46\n",
      "2023-12-11 02:53:46,917 INFO     pid:22692 __main__:409:logMetrics E18 SegmentationTrainingApp\n",
      "2023-12-11 02:53:46,917 INFO     pid:22692 __main__:444:logMetrics E18 trn      0.3359 loss, 0.2197 fnloss, 0.9962 fploss, 0.6720 precision, 0.6795 recall, 0.6757 f1 score\n",
      "2023-12-11 02:53:46,917 INFO     pid:22692 __main__:458:logMetrics E18 trn_all  0.3359 loss,  67.9% tp,  32.1% fn,      33.2% fp\n",
      "2023-12-11 02:53:46,917 INFO     pid:22692 __main__:191:main Epoch 19 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 02:53:46,933 WARNING  pid:22692 util:109:enumerateWithEstimate E19 Training ----/6754, starting\n",
      "2023-12-11 02:54:29,929 INFO     pid:22692 util:126:enumerateWithEstimate E19 Training   64/6754, done at 2023-12-11 03:01:25, 0:06:58\n",
      "2023-12-11 02:54:42,022 INFO     pid:22692 util:126:enumerateWithEstimate E19 Training  256/6754, done at 2023-12-11 03:01:30, 0:07:03\n",
      "2023-12-11 02:55:30,173 INFO     pid:22692 util:126:enumerateWithEstimate E19 Training 1024/6754, done at 2023-12-11 03:01:29, 0:07:02\n",
      "2023-12-11 02:58:41,622 INFO     pid:22692 util:126:enumerateWithEstimate E19 Training 4096/6754, done at 2023-12-11 03:01:27, 0:07:00\n",
      "2023-12-11 03:01:28,729 WARNING  pid:22692 util:139:enumerateWithEstimate E19 Training ----/6754, done at 2023-12-11 03:01:28\n",
      "2023-12-11 03:01:28,729 INFO     pid:22692 __main__:409:logMetrics E19 SegmentationTrainingApp\n",
      "2023-12-11 03:01:28,729 INFO     pid:22692 __main__:444:logMetrics E19 trn      0.3309 loss, 0.2225 fnloss, 0.9961 fploss, 0.7044 precision, 0.6798 recall, 0.6918 f1 score\n",
      "2023-12-11 03:01:28,729 INFO     pid:22692 __main__:458:logMetrics E19 trn_all  0.3309 loss,  68.0% tp,  32.0% fn,      28.5% fp\n",
      "2023-12-11 03:01:28,729 INFO     pid:22692 __main__:191:main Epoch 20 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:01:28,745 WARNING  pid:22692 util:109:enumerateWithEstimate E20 Training ----/6754, starting\n",
      "2023-12-11 03:02:11,756 INFO     pid:22692 util:126:enumerateWithEstimate E20 Training   64/6754, done at 2023-12-11 03:09:11, 0:07:03\n",
      "2023-12-11 03:02:23,833 INFO     pid:22692 util:126:enumerateWithEstimate E20 Training  256/6754, done at 2023-12-11 03:09:12, 0:07:03\n",
      "2023-12-11 03:03:11,999 INFO     pid:22692 util:126:enumerateWithEstimate E20 Training 1024/6754, done at 2023-12-11 03:09:11, 0:07:02\n",
      "2023-12-11 03:06:23,496 INFO     pid:22692 util:126:enumerateWithEstimate E20 Training 4096/6754, done at 2023-12-11 03:09:09, 0:07:00\n",
      "2023-12-11 03:09:10,650 WARNING  pid:22692 util:139:enumerateWithEstimate E20 Training ----/6754, done at 2023-12-11 03:09:10\n",
      "2023-12-11 03:09:10,650 INFO     pid:22692 __main__:409:logMetrics E20 SegmentationTrainingApp\n",
      "2023-12-11 03:09:10,650 INFO     pid:22692 __main__:444:logMetrics E20 trn      0.3228 loss, 0.2101 fnloss, 0.9962 fploss, 0.7713 precision, 0.6943 recall, 0.7308 f1 score\n",
      "2023-12-11 03:09:10,650 INFO     pid:22692 __main__:458:logMetrics E20 trn_all  0.3228 loss,  69.4% tp,  30.6% fn,      20.6% fp\n",
      "2023-12-11 03:09:10,650 WARNING  pid:22692 util:109:enumerateWithEstimate E20 Validation  ----/862, starting\n",
      "2023-12-11 03:09:16,923 INFO     pid:22692 util:126:enumerateWithEstimate E20 Validation    64/862, done at 2023-12-11 03:09:31, 0:00:15\n",
      "2023-12-11 03:09:20,375 INFO     pid:22692 util:126:enumerateWithEstimate E20 Validation   256/862, done at 2023-12-11 03:09:31, 0:00:15\n",
      "2023-12-11 03:09:32,671 WARNING  pid:22692 util:139:enumerateWithEstimate E20 Validation  ----/862, done at 2023-12-11 03:09:32\n",
      "2023-12-11 03:09:32,671 INFO     pid:22692 __main__:409:logMetrics E20 SegmentationTrainingApp\n",
      "2023-12-11 03:09:32,671 INFO     pid:22692 __main__:444:logMetrics E20 val      0.4574 loss, 0.3684 fnloss, 0.9923 fploss, 0.7756 precision, 0.7104 recall, 0.7416 f1 score\n",
      "2023-12-11 03:09:32,671 INFO     pid:22692 __main__:458:logMetrics E20 val_all  0.4574 loss,  71.0% tp,  29.0% fn,      20.6% fp\n",
      "2023-12-11 03:09:32,796 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.270160.state\n",
      "2023-12-11 03:09:32,827 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 03:09:32,874 INFO     pid:22692 __main__:523:saveModel SHA1: 8d47348ea18d6e1cde5958550f52029cef81268d\n",
      "2023-12-11 03:10:19,666 INFO     pid:22692 __main__:191:main Epoch 21 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:10:19,682 WARNING  pid:22692 util:109:enumerateWithEstimate E21 Training ----/6754, starting\n",
      "2023-12-11 03:11:02,193 INFO     pid:22692 util:126:enumerateWithEstimate E21 Training   64/6754, done at 2023-12-11 03:18:02, 0:07:03\n",
      "2023-12-11 03:11:14,301 INFO     pid:22692 util:126:enumerateWithEstimate E21 Training  256/6754, done at 2023-12-11 03:18:03, 0:07:04\n",
      "2023-12-11 03:12:02,734 INFO     pid:22692 util:126:enumerateWithEstimate E21 Training 1024/6754, done at 2023-12-11 03:18:03, 0:07:04\n",
      "2023-12-11 03:15:14,714 INFO     pid:22692 util:126:enumerateWithEstimate E21 Training 4096/6754, done at 2023-12-11 03:18:01, 0:07:02\n",
      "2023-12-11 03:18:02,087 WARNING  pid:22692 util:139:enumerateWithEstimate E21 Training ----/6754, done at 2023-12-11 03:18:02\n",
      "2023-12-11 03:18:02,087 INFO     pid:22692 __main__:409:logMetrics E21 SegmentationTrainingApp\n",
      "2023-12-11 03:18:02,087 INFO     pid:22692 __main__:444:logMetrics E21 trn      0.3293 loss, 0.2156 fnloss, 0.9962 fploss, 0.7646 precision, 0.6917 recall, 0.7263 f1 score\n",
      "2023-12-11 03:18:02,087 INFO     pid:22692 __main__:458:logMetrics E21 trn_all  0.3293 loss,  69.2% tp,  30.8% fn,      21.3% fp\n",
      "2023-12-11 03:18:02,103 INFO     pid:22692 __main__:191:main Epoch 22 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:18:02,103 WARNING  pid:22692 util:109:enumerateWithEstimate E22 Training ----/6754, starting\n",
      "2023-12-11 03:18:45,225 INFO     pid:22692 util:126:enumerateWithEstimate E22 Training   64/6754, done at 2023-12-11 03:25:41, 0:06:58\n",
      "2023-12-11 03:18:57,286 INFO     pid:22692 util:126:enumerateWithEstimate E22 Training  256/6754, done at 2023-12-11 03:25:44, 0:07:02\n",
      "2023-12-11 03:19:45,438 INFO     pid:22692 util:126:enumerateWithEstimate E22 Training 1024/6754, done at 2023-12-11 03:25:44, 0:07:02\n",
      "2023-12-11 03:22:56,901 INFO     pid:22692 util:126:enumerateWithEstimate E22 Training 4096/6754, done at 2023-12-11 03:25:42, 0:07:00\n",
      "2023-12-11 03:25:43,900 WARNING  pid:22692 util:139:enumerateWithEstimate E22 Training ----/6754, done at 2023-12-11 03:25:43\n",
      "2023-12-11 03:25:43,916 INFO     pid:22692 __main__:409:logMetrics E22 SegmentationTrainingApp\n",
      "2023-12-11 03:25:43,916 INFO     pid:22692 __main__:444:logMetrics E22 trn      0.3243 loss, 0.2138 fnloss, 0.9961 fploss, 0.7741 precision, 0.6926 recall, 0.7311 f1 score\n",
      "2023-12-11 03:25:43,916 INFO     pid:22692 __main__:458:logMetrics E22 trn_all  0.3243 loss,  69.3% tp,  30.7% fn,      20.2% fp\n",
      "2023-12-11 03:25:43,916 INFO     pid:22692 __main__:191:main Epoch 23 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:25:43,916 WARNING  pid:22692 util:109:enumerateWithEstimate E23 Training ----/6754, starting\n",
      "2023-12-11 03:26:26,849 INFO     pid:22692 util:126:enumerateWithEstimate E23 Training   64/6754, done at 2023-12-11 03:33:26, 0:07:03\n",
      "2023-12-11 03:26:38,941 INFO     pid:22692 util:126:enumerateWithEstimate E23 Training  256/6754, done at 2023-12-11 03:33:27, 0:07:04\n",
      "2023-12-11 03:27:27,171 INFO     pid:22692 util:126:enumerateWithEstimate E23 Training 1024/6754, done at 2023-12-11 03:33:27, 0:07:03\n",
      "2023-12-11 03:30:38,525 INFO     pid:22692 util:126:enumerateWithEstimate E23 Training 4096/6754, done at 2023-12-11 03:33:24, 0:07:00\n",
      "2023-12-11 03:33:25,676 WARNING  pid:22692 util:139:enumerateWithEstimate E23 Training ----/6754, done at 2023-12-11 03:33:25\n",
      "2023-12-11 03:33:25,676 INFO     pid:22692 __main__:409:logMetrics E23 SegmentationTrainingApp\n",
      "2023-12-11 03:33:25,676 INFO     pid:22692 __main__:444:logMetrics E23 trn      0.3207 loss, 0.2040 fnloss, 0.9961 fploss, 0.7681 precision, 0.7054 recall, 0.7354 f1 score\n",
      "2023-12-11 03:33:25,676 INFO     pid:22692 __main__:458:logMetrics E23 trn_all  0.3207 loss,  70.5% tp,  29.5% fn,      21.3% fp\n",
      "2023-12-11 03:33:25,676 INFO     pid:22692 __main__:191:main Epoch 24 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:33:25,676 WARNING  pid:22692 util:109:enumerateWithEstimate E24 Training ----/6754, starting\n",
      "2023-12-11 03:34:08,734 INFO     pid:22692 util:126:enumerateWithEstimate E24 Training   64/6754, done at 2023-12-11 03:41:06, 0:07:01\n",
      "2023-12-11 03:34:20,812 INFO     pid:22692 util:126:enumerateWithEstimate E24 Training  256/6754, done at 2023-12-11 03:41:08, 0:07:03\n",
      "2023-12-11 03:35:08,873 INFO     pid:22692 util:126:enumerateWithEstimate E24 Training 1024/6754, done at 2023-12-11 03:41:07, 0:07:02\n",
      "2023-12-11 03:38:20,361 INFO     pid:22692 util:126:enumerateWithEstimate E24 Training 4096/6754, done at 2023-12-11 03:41:06, 0:07:00\n",
      "2023-12-11 03:41:07,521 WARNING  pid:22692 util:139:enumerateWithEstimate E24 Training ----/6754, done at 2023-12-11 03:41:07\n",
      "2023-12-11 03:41:07,521 INFO     pid:22692 __main__:409:logMetrics E24 SegmentationTrainingApp\n",
      "2023-12-11 03:41:07,521 INFO     pid:22692 __main__:444:logMetrics E24 trn      0.3154 loss, 0.1983 fnloss, 0.9962 fploss, 0.7705 precision, 0.7078 recall, 0.7378 f1 score\n",
      "2023-12-11 03:41:07,521 INFO     pid:22692 __main__:458:logMetrics E24 trn_all  0.3154 loss,  70.8% tp,  29.2% fn,      21.1% fp\n",
      "2023-12-11 03:41:07,521 INFO     pid:22692 __main__:191:main Epoch 25 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:41:07,539 WARNING  pid:22692 util:109:enumerateWithEstimate E25 Training ----/6754, starting\n",
      "2023-12-11 03:41:50,580 INFO     pid:22692 util:126:enumerateWithEstimate E25 Training   64/6754, done at 2023-12-11 03:48:50, 0:07:03\n",
      "2023-12-11 03:42:02,689 INFO     pid:22692 util:126:enumerateWithEstimate E25 Training  256/6754, done at 2023-12-11 03:48:52, 0:07:04\n",
      "2023-12-11 03:42:51,077 INFO     pid:22692 util:126:enumerateWithEstimate E25 Training 1024/6754, done at 2023-12-11 03:48:52, 0:07:04\n",
      "2023-12-11 03:46:02,643 INFO     pid:22692 util:126:enumerateWithEstimate E25 Training 4096/6754, done at 2023-12-11 03:48:48, 0:07:01\n",
      "2023-12-11 03:48:49,961 WARNING  pid:22692 util:139:enumerateWithEstimate E25 Training ----/6754, done at 2023-12-11 03:48:49\n",
      "2023-12-11 03:48:49,961 INFO     pid:22692 __main__:409:logMetrics E25 SegmentationTrainingApp\n",
      "2023-12-11 03:48:49,961 INFO     pid:22692 __main__:444:logMetrics E25 trn      0.3162 loss, 0.1970 fnloss, 0.9963 fploss, 0.7668 precision, 0.7139 recall, 0.7394 f1 score\n",
      "2023-12-11 03:48:49,961 INFO     pid:22692 __main__:458:logMetrics E25 trn_all  0.3162 loss,  71.4% tp,  28.6% fn,      21.7% fp\n",
      "2023-12-11 03:48:49,961 WARNING  pid:22692 util:109:enumerateWithEstimate E25 Validation  ----/862, starting\n",
      "2023-12-11 03:48:56,338 INFO     pid:22692 util:126:enumerateWithEstimate E25 Validation    64/862, done at 2023-12-11 03:49:10, 0:00:15\n",
      "2023-12-11 03:48:59,791 INFO     pid:22692 util:126:enumerateWithEstimate E25 Validation   256/862, done at 2023-12-11 03:49:10, 0:00:15\n",
      "2023-12-11 03:49:12,165 WARNING  pid:22692 util:139:enumerateWithEstimate E25 Validation  ----/862, done at 2023-12-11 03:49:12\n",
      "2023-12-11 03:49:12,165 INFO     pid:22692 __main__:409:logMetrics E25 SegmentationTrainingApp\n",
      "2023-12-11 03:49:12,165 INFO     pid:22692 __main__:444:logMetrics E25 val      0.4152 loss, 0.2908 fnloss, 0.9931 fploss, 0.7388 precision, 0.7711 recall, 0.7546 f1 score\n",
      "2023-12-11 03:49:12,165 INFO     pid:22692 __main__:458:logMetrics E25 val_all  0.4152 loss,  77.1% tp,  22.9% fn,      27.3% fp\n",
      "2023-12-11 03:49:12,290 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.337700.state\n",
      "2023-12-11 03:49:12,337 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 03:49:12,384 INFO     pid:22692 __main__:523:saveModel SHA1: 7db14fabb521a8446632ab9abe3f0aba104b20b2\n",
      "2023-12-11 03:50:00,025 INFO     pid:22692 __main__:191:main Epoch 26 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:50:00,025 WARNING  pid:22692 util:109:enumerateWithEstimate E26 Training ----/6754, starting\n",
      "2023-12-11 03:50:42,787 INFO     pid:22692 util:126:enumerateWithEstimate E26 Training   64/6754, done at 2023-12-11 03:57:38, 0:06:58\n",
      "2023-12-11 03:50:54,756 INFO     pid:22692 util:126:enumerateWithEstimate E26 Training  256/6754, done at 2023-12-11 03:57:39, 0:06:59\n",
      "2023-12-11 03:51:43,207 INFO     pid:22692 util:126:enumerateWithEstimate E26 Training 1024/6754, done at 2023-12-11 03:57:43, 0:07:03\n",
      "2023-12-11 03:54:54,803 INFO     pid:22692 util:126:enumerateWithEstimate E26 Training 4096/6754, done at 2023-12-11 03:57:40, 0:07:01\n",
      "2023-12-11 03:57:41,979 WARNING  pid:22692 util:139:enumerateWithEstimate E26 Training ----/6754, done at 2023-12-11 03:57:41\n",
      "2023-12-11 03:57:41,995 INFO     pid:22692 __main__:409:logMetrics E26 SegmentationTrainingApp\n",
      "2023-12-11 03:57:41,995 INFO     pid:22692 __main__:444:logMetrics E26 trn      0.3097 loss, 0.1943 fnloss, 0.9962 fploss, 0.7758 precision, 0.7148 recall, 0.7441 f1 score\n",
      "2023-12-11 03:57:41,995 INFO     pid:22692 __main__:458:logMetrics E26 trn_all  0.3097 loss,  71.5% tp,  28.5% fn,      20.7% fp\n",
      "2023-12-11 03:57:41,995 INFO     pid:22692 __main__:191:main Epoch 27 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 03:57:41,995 WARNING  pid:22692 util:109:enumerateWithEstimate E27 Training ----/6754, starting\n",
      "2023-12-11 03:58:25,068 INFO     pid:22692 util:126:enumerateWithEstimate E27 Training   64/6754, done at 2023-12-11 04:05:27, 0:07:05\n",
      "2023-12-11 03:58:37,163 INFO     pid:22692 util:126:enumerateWithEstimate E27 Training  256/6754, done at 2023-12-11 04:05:26, 0:07:04\n",
      "2023-12-11 03:59:25,506 INFO     pid:22692 util:126:enumerateWithEstimate E27 Training 1024/6754, done at 2023-12-11 04:05:26, 0:07:04\n",
      "2023-12-11 04:02:36,786 INFO     pid:22692 util:126:enumerateWithEstimate E27 Training 4096/6754, done at 2023-12-11 04:05:22, 0:07:00\n",
      "2023-12-11 04:05:23,993 WARNING  pid:22692 util:139:enumerateWithEstimate E27 Training ----/6754, done at 2023-12-11 04:05:23\n",
      "2023-12-11 04:05:23,993 INFO     pid:22692 __main__:409:logMetrics E27 SegmentationTrainingApp\n",
      "2023-12-11 04:05:23,993 INFO     pid:22692 __main__:444:logMetrics E27 trn      0.3141 loss, 0.2048 fnloss, 0.9961 fploss, 0.7813 precision, 0.7060 recall, 0.7417 f1 score\n",
      "2023-12-11 04:05:23,993 INFO     pid:22692 __main__:458:logMetrics E27 trn_all  0.3141 loss,  70.6% tp,  29.4% fn,      19.8% fp\n",
      "2023-12-11 04:05:24,009 INFO     pid:22692 __main__:191:main Epoch 28 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:05:24,009 WARNING  pid:22692 util:109:enumerateWithEstimate E28 Training ----/6754, starting\n",
      "2023-12-11 04:06:07,083 INFO     pid:22692 util:126:enumerateWithEstimate E28 Training   64/6754, done at 2023-12-11 04:13:07, 0:07:03\n",
      "2023-12-11 04:06:19,176 INFO     pid:22692 util:126:enumerateWithEstimate E28 Training  256/6754, done at 2023-12-11 04:13:08, 0:07:04\n",
      "2023-12-11 04:07:07,567 INFO     pid:22692 util:126:enumerateWithEstimate E28 Training 1024/6754, done at 2023-12-11 04:13:08, 0:07:04\n",
      "2023-12-11 04:10:19,617 INFO     pid:22692 util:126:enumerateWithEstimate E28 Training 4096/6754, done at 2023-12-11 04:13:06, 0:07:02\n",
      "2023-12-11 04:13:06,886 WARNING  pid:22692 util:139:enumerateWithEstimate E28 Training ----/6754, done at 2023-12-11 04:13:06\n",
      "2023-12-11 04:13:06,886 INFO     pid:22692 __main__:409:logMetrics E28 SegmentationTrainingApp\n",
      "2023-12-11 04:13:06,901 INFO     pid:22692 __main__:444:logMetrics E28 trn      0.3081 loss, 0.1936 fnloss, 0.9961 fploss, 0.7759 precision, 0.7204 recall, 0.7471 f1 score\n",
      "2023-12-11 04:13:06,901 INFO     pid:22692 __main__:458:logMetrics E28 trn_all  0.3081 loss,  72.0% tp,  28.0% fn,      20.8% fp\n",
      "2023-12-11 04:13:06,901 INFO     pid:22692 __main__:191:main Epoch 29 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:13:06,901 WARNING  pid:22692 util:109:enumerateWithEstimate E29 Training ----/6754, starting\n",
      "2023-12-11 04:13:49,944 INFO     pid:22692 util:126:enumerateWithEstimate E29 Training   64/6754, done at 2023-12-11 04:20:50, 0:07:03\n",
      "2023-12-11 04:14:02,053 INFO     pid:22692 util:126:enumerateWithEstimate E29 Training  256/6754, done at 2023-12-11 04:20:51, 0:07:04\n",
      "2023-12-11 04:14:50,428 INFO     pid:22692 util:126:enumerateWithEstimate E29 Training 1024/6754, done at 2023-12-11 04:20:51, 0:07:04\n",
      "2023-12-11 04:18:02,209 INFO     pid:22692 util:126:enumerateWithEstimate E29 Training 4096/6754, done at 2023-12-11 04:20:48, 0:07:01\n",
      "2023-12-11 04:20:49,430 WARNING  pid:22692 util:139:enumerateWithEstimate E29 Training ----/6754, done at 2023-12-11 04:20:49\n",
      "2023-12-11 04:20:49,430 INFO     pid:22692 __main__:409:logMetrics E29 SegmentationTrainingApp\n",
      "2023-12-11 04:20:49,430 INFO     pid:22692 __main__:444:logMetrics E29 trn      0.3045 loss, 0.1957 fnloss, 0.9961 fploss, 0.7861 precision, 0.7221 recall, 0.7527 f1 score\n",
      "2023-12-11 04:20:49,430 INFO     pid:22692 __main__:458:logMetrics E29 trn_all  0.3045 loss,  72.2% tp,  27.8% fn,      19.7% fp\n",
      "2023-12-11 04:20:49,446 INFO     pid:22692 __main__:191:main Epoch 30 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:20:49,446 WARNING  pid:22692 util:109:enumerateWithEstimate E30 Training ----/6754, starting\n",
      "2023-12-11 04:21:32,395 INFO     pid:22692 util:126:enumerateWithEstimate E30 Training   64/6754, done at 2023-12-11 04:28:32, 0:07:03\n",
      "2023-12-11 04:21:44,473 INFO     pid:22692 util:126:enumerateWithEstimate E30 Training  256/6754, done at 2023-12-11 04:28:33, 0:07:03\n",
      "2023-12-11 04:22:32,830 INFO     pid:22692 util:126:enumerateWithEstimate E30 Training 1024/6754, done at 2023-12-11 04:28:33, 0:07:04\n",
      "2023-12-11 04:25:44,224 INFO     pid:22692 util:126:enumerateWithEstimate E30 Training 4096/6754, done at 2023-12-11 04:28:30, 0:07:00\n",
      "2023-12-11 04:28:31,651 WARNING  pid:22692 util:139:enumerateWithEstimate E30 Training ----/6754, done at 2023-12-11 04:28:31\n",
      "2023-12-11 04:28:31,651 INFO     pid:22692 __main__:409:logMetrics E30 SegmentationTrainingApp\n",
      "2023-12-11 04:28:31,651 INFO     pid:22692 __main__:444:logMetrics E30 trn      0.3061 loss, 0.2013 fnloss, 0.9960 fploss, 0.7902 precision, 0.7104 recall, 0.7482 f1 score\n",
      "2023-12-11 04:28:31,666 INFO     pid:22692 __main__:458:logMetrics E30 trn_all  0.3061 loss,  71.0% tp,  29.0% fn,      18.9% fp\n",
      "2023-12-11 04:28:31,666 WARNING  pid:22692 util:109:enumerateWithEstimate E30 Validation  ----/862, starting\n",
      "2023-12-11 04:28:37,718 INFO     pid:22692 util:126:enumerateWithEstimate E30 Validation    64/862, done at 2023-12-11 04:28:51, 0:00:15\n",
      "2023-12-11 04:28:41,171 INFO     pid:22692 util:126:enumerateWithEstimate E30 Validation   256/862, done at 2023-12-11 04:28:52, 0:00:15\n",
      "2023-12-11 04:28:53,483 WARNING  pid:22692 util:139:enumerateWithEstimate E30 Validation  ----/862, done at 2023-12-11 04:28:53\n",
      "2023-12-11 04:28:53,483 INFO     pid:22692 __main__:409:logMetrics E30 SegmentationTrainingApp\n",
      "2023-12-11 04:28:53,483 INFO     pid:22692 __main__:444:logMetrics E30 val      0.3920 loss, 0.3320 fnloss, 0.9920 fploss, 0.8568 precision, 0.7342 recall, 0.7908 f1 score\n",
      "2023-12-11 04:28:53,483 INFO     pid:22692 __main__:458:logMetrics E30 val_all  0.3920 loss,  73.4% tp,  26.6% fn,      12.3% fp\n",
      "2023-12-11 04:28:53,592 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.405240.state\n",
      "2023-12-11 04:28:53,655 INFO     pid:22692 __main__:523:saveModel SHA1: b2be1798598e26d5dfee0e2d439617d6a72ff351\n",
      "2023-12-11 04:29:40,402 INFO     pid:22692 __main__:191:main Epoch 31 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:29:40,402 WARNING  pid:22692 util:109:enumerateWithEstimate E31 Training ----/6754, starting\n",
      "2023-12-11 04:30:23,164 INFO     pid:22692 util:126:enumerateWithEstimate E31 Training   64/6754, done at 2023-12-11 04:37:19, 0:06:58\n",
      "2023-12-11 04:30:35,147 INFO     pid:22692 util:126:enumerateWithEstimate E31 Training  256/6754, done at 2023-12-11 04:37:20, 0:07:00\n",
      "2023-12-11 04:31:23,513 INFO     pid:22692 util:126:enumerateWithEstimate E31 Training 1024/6754, done at 2023-12-11 04:37:23, 0:07:03\n",
      "2023-12-11 04:34:35,156 INFO     pid:22692 util:126:enumerateWithEstimate E31 Training 4096/6754, done at 2023-12-11 04:37:21, 0:07:01\n",
      "2023-12-11 04:37:22,552 WARNING  pid:22692 util:139:enumerateWithEstimate E31 Training ----/6754, done at 2023-12-11 04:37:22\n",
      "2023-12-11 04:37:22,552 INFO     pid:22692 __main__:409:logMetrics E31 SegmentationTrainingApp\n",
      "2023-12-11 04:37:22,552 INFO     pid:22692 __main__:444:logMetrics E31 trn      0.3088 loss, 0.1963 fnloss, 0.9961 fploss, 0.7490 precision, 0.7195 recall, 0.7340 f1 score\n",
      "2023-12-11 04:37:22,552 INFO     pid:22692 __main__:458:logMetrics E31 trn_all  0.3088 loss,  72.0% tp,  28.0% fn,      24.1% fp\n",
      "2023-12-11 04:37:22,552 INFO     pid:22692 __main__:191:main Epoch 32 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:37:22,567 WARNING  pid:22692 util:109:enumerateWithEstimate E32 Training ----/6754, starting\n",
      "2023-12-11 04:38:05,408 INFO     pid:22692 util:126:enumerateWithEstimate E32 Training   64/6754, done at 2023-12-11 04:45:07, 0:07:05\n",
      "2023-12-11 04:38:17,581 INFO     pid:22692 util:126:enumerateWithEstimate E32 Training  256/6754, done at 2023-12-11 04:45:09, 0:07:06\n",
      "2023-12-11 04:39:06,161 INFO     pid:22692 util:126:enumerateWithEstimate E32 Training 1024/6754, done at 2023-12-11 04:45:08, 0:07:06\n",
      "2023-12-11 04:42:18,475 INFO     pid:22692 util:126:enumerateWithEstimate E32 Training 4096/6754, done at 2023-12-11 04:45:05, 0:07:02\n",
      "2023-12-11 04:45:06,371 WARNING  pid:22692 util:139:enumerateWithEstimate E32 Training ----/6754, done at 2023-12-11 04:45:06\n",
      "2023-12-11 04:45:06,386 INFO     pid:22692 __main__:409:logMetrics E32 SegmentationTrainingApp\n",
      "2023-12-11 04:45:06,386 INFO     pid:22692 __main__:444:logMetrics E32 trn      0.3003 loss, 0.1893 fnloss, 0.9962 fploss, 0.7212 precision, 0.7256 recall, 0.7234 f1 score\n",
      "2023-12-11 04:45:06,386 INFO     pid:22692 __main__:458:logMetrics E32 trn_all  0.3003 loss,  72.6% tp,  27.4% fn,      28.1% fp\n",
      "2023-12-11 04:45:06,386 INFO     pid:22692 __main__:191:main Epoch 33 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:45:06,386 WARNING  pid:22692 util:109:enumerateWithEstimate E33 Training ----/6754, starting\n",
      "2023-12-11 04:45:49,258 INFO     pid:22692 util:126:enumerateWithEstimate E33 Training   64/6754, done at 2023-12-11 04:52:51, 0:07:05\n",
      "2023-12-11 04:46:01,367 INFO     pid:22692 util:126:enumerateWithEstimate E33 Training  256/6754, done at 2023-12-11 04:52:51, 0:07:05\n",
      "2023-12-11 04:46:49,601 INFO     pid:22692 util:126:enumerateWithEstimate E33 Training 1024/6754, done at 2023-12-11 04:52:49, 0:07:03\n",
      "2023-12-11 04:50:01,135 INFO     pid:22692 util:126:enumerateWithEstimate E33 Training 4096/6754, done at 2023-12-11 04:52:47, 0:07:00\n",
      "2023-12-11 04:52:48,403 WARNING  pid:22692 util:139:enumerateWithEstimate E33 Training ----/6754, done at 2023-12-11 04:52:48\n",
      "2023-12-11 04:52:48,403 INFO     pid:22692 __main__:409:logMetrics E33 SegmentationTrainingApp\n",
      "2023-12-11 04:52:48,418 INFO     pid:22692 __main__:444:logMetrics E33 trn      0.3035 loss, 0.1866 fnloss, 0.9961 fploss, 0.7777 precision, 0.7320 recall, 0.7541 f1 score\n",
      "2023-12-11 04:52:48,418 INFO     pid:22692 __main__:458:logMetrics E33 trn_all  0.3035 loss,  73.2% tp,  26.8% fn,      20.9% fp\n",
      "2023-12-11 04:52:48,418 INFO     pid:22692 __main__:191:main Epoch 34 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 04:52:48,418 WARNING  pid:22692 util:109:enumerateWithEstimate E34 Training ----/6754, starting\n",
      "2023-12-11 04:53:31,336 INFO     pid:22692 util:126:enumerateWithEstimate E34 Training   64/6754, done at 2023-12-11 05:00:31, 0:07:03\n",
      "2023-12-11 04:53:43,461 INFO     pid:22692 util:126:enumerateWithEstimate E34 Training  256/6754, done at 2023-12-11 05:00:33, 0:07:05\n",
      "2023-12-11 04:54:31,881 INFO     pid:22692 util:126:enumerateWithEstimate E34 Training 1024/6754, done at 2023-12-11 05:00:33, 0:07:04\n",
      "2023-12-11 04:57:43,632 INFO     pid:22692 util:126:enumerateWithEstimate E34 Training 4096/6754, done at 2023-12-11 05:00:29, 0:07:01\n",
      "2023-12-11 05:00:31,262 WARNING  pid:22692 util:139:enumerateWithEstimate E34 Training ----/6754, done at 2023-12-11 05:00:31\n",
      "2023-12-11 05:00:31,278 INFO     pid:22692 __main__:409:logMetrics E34 SegmentationTrainingApp\n",
      "2023-12-11 05:00:31,278 INFO     pid:22692 __main__:444:logMetrics E34 trn      0.3005 loss, 0.1960 fnloss, 0.9959 fploss, 0.7362 precision, 0.7178 recall, 0.7269 f1 score\n",
      "2023-12-11 05:00:31,278 INFO     pid:22692 __main__:458:logMetrics E34 trn_all  0.3005 loss,  71.8% tp,  28.2% fn,      25.7% fp\n",
      "2023-12-11 05:00:31,278 INFO     pid:22692 __main__:191:main Epoch 35 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:00:31,278 WARNING  pid:22692 util:109:enumerateWithEstimate E35 Training ----/6754, starting\n",
      "2023-12-11 05:01:14,133 INFO     pid:22692 util:126:enumerateWithEstimate E35 Training   64/6754, done at 2023-12-11 05:08:14, 0:07:03\n",
      "2023-12-11 05:01:26,241 INFO     pid:22692 util:126:enumerateWithEstimate E35 Training  256/6754, done at 2023-12-11 05:08:15, 0:07:04\n",
      "2023-12-11 05:02:14,600 INFO     pid:22692 util:126:enumerateWithEstimate E35 Training 1024/6754, done at 2023-12-11 05:08:15, 0:07:04\n",
      "2023-12-11 05:05:26,403 INFO     pid:22692 util:126:enumerateWithEstimate E35 Training 4096/6754, done at 2023-12-11 05:08:12, 0:07:01\n",
      "2023-12-11 05:08:13,894 WARNING  pid:22692 util:139:enumerateWithEstimate E35 Training ----/6754, done at 2023-12-11 05:08:13\n",
      "2023-12-11 05:08:13,909 INFO     pid:22692 __main__:409:logMetrics E35 SegmentationTrainingApp\n",
      "2023-12-11 05:08:13,909 INFO     pid:22692 __main__:444:logMetrics E35 trn      0.2932 loss, 0.1834 fnloss, 0.9960 fploss, 0.7210 precision, 0.7346 recall, 0.7278 f1 score\n",
      "2023-12-11 05:08:13,909 INFO     pid:22692 __main__:458:logMetrics E35 trn_all  0.2932 loss,  73.5% tp,  26.5% fn,      28.4% fp\n",
      "2023-12-11 05:08:13,909 WARNING  pid:22692 util:109:enumerateWithEstimate E35 Validation  ----/862, starting\n",
      "2023-12-11 05:08:20,464 INFO     pid:22692 util:126:enumerateWithEstimate E35 Validation    64/862, done at 2023-12-11 05:08:34, 0:00:15\n",
      "2023-12-11 05:08:23,901 INFO     pid:22692 util:126:enumerateWithEstimate E35 Validation   256/862, done at 2023-12-11 05:08:34, 0:00:15\n",
      "2023-12-11 05:08:36,322 WARNING  pid:22692 util:139:enumerateWithEstimate E35 Validation  ----/862, done at 2023-12-11 05:08:36\n",
      "2023-12-11 05:08:36,322 INFO     pid:22692 __main__:409:logMetrics E35 SegmentationTrainingApp\n",
      "2023-12-11 05:08:36,322 INFO     pid:22692 __main__:444:logMetrics E35 val      0.4136 loss, 0.3652 fnloss, 0.9918 fploss, 0.8527 precision, 0.6920 recall, 0.7640 f1 score\n",
      "2023-12-11 05:08:36,322 INFO     pid:22692 __main__:458:logMetrics E35 val_all  0.4136 loss,  69.2% tp,  30.8% fn,      12.0% fp\n",
      "2023-12-11 05:08:36,447 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.472780.state\n",
      "2023-12-11 05:08:36,510 INFO     pid:22692 __main__:523:saveModel SHA1: 72598ad0ffe36ed57e3e38973ba2ec1a7b8d3309\n",
      "2023-12-11 05:09:25,884 INFO     pid:22692 __main__:191:main Epoch 36 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:09:25,884 WARNING  pid:22692 util:109:enumerateWithEstimate E36 Training ----/6754, starting\n",
      "2023-12-11 05:10:08,302 INFO     pid:22692 util:126:enumerateWithEstimate E36 Training   64/6754, done at 2023-12-11 05:17:02, 0:06:56\n",
      "2023-12-11 05:10:20,411 INFO     pid:22692 util:126:enumerateWithEstimate E36 Training  256/6754, done at 2023-12-11 05:17:08, 0:07:03\n",
      "2023-12-11 05:11:08,909 INFO     pid:22692 util:126:enumerateWithEstimate E36 Training 1024/6754, done at 2023-12-11 05:17:10, 0:07:04\n",
      "2023-12-11 05:14:23,051 INFO     pid:22692 util:126:enumerateWithEstimate E36 Training 4096/6754, done at 2023-12-11 05:17:10, 0:07:05\n",
      "2023-12-11 05:17:12,322 WARNING  pid:22692 util:139:enumerateWithEstimate E36 Training ----/6754, done at 2023-12-11 05:17:12\n",
      "2023-12-11 05:17:12,322 INFO     pid:22692 __main__:409:logMetrics E36 SegmentationTrainingApp\n",
      "2023-12-11 05:17:12,322 INFO     pid:22692 __main__:444:logMetrics E36 trn      0.2926 loss, 0.1871 fnloss, 0.9960 fploss, 0.7202 precision, 0.7306 recall, 0.7254 f1 score\n",
      "2023-12-11 05:17:12,322 INFO     pid:22692 __main__:458:logMetrics E36 trn_all  0.2926 loss,  73.1% tp,  26.9% fn,      28.4% fp\n",
      "2023-12-11 05:17:12,322 INFO     pid:22692 __main__:191:main Epoch 37 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:17:12,338 WARNING  pid:22692 util:109:enumerateWithEstimate E37 Training ----/6754, starting\n",
      "2023-12-11 05:17:55,443 INFO     pid:22692 util:126:enumerateWithEstimate E37 Training   64/6754, done at 2023-12-11 05:24:55, 0:07:03\n",
      "2023-12-11 05:18:07,553 INFO     pid:22692 util:126:enumerateWithEstimate E37 Training  256/6754, done at 2023-12-11 05:24:57, 0:07:04\n",
      "2023-12-11 05:18:55,521 INFO     pid:22692 util:126:enumerateWithEstimate E37 Training 1024/6754, done at 2023-12-11 05:24:54, 0:07:01\n",
      "2023-12-11 05:22:07,242 INFO     pid:22692 util:126:enumerateWithEstimate E37 Training 4096/6754, done at 2023-12-11 05:24:53, 0:07:00\n",
      "2023-12-11 05:24:54,544 WARNING  pid:22692 util:139:enumerateWithEstimate E37 Training ----/6754, done at 2023-12-11 05:24:54\n",
      "2023-12-11 05:24:54,544 INFO     pid:22692 __main__:409:logMetrics E37 SegmentationTrainingApp\n",
      "2023-12-11 05:24:54,544 INFO     pid:22692 __main__:444:logMetrics E37 trn      0.2911 loss, 0.1839 fnloss, 0.9960 fploss, 0.7990 precision, 0.7324 recall, 0.7642 f1 score\n",
      "2023-12-11 05:24:54,544 INFO     pid:22692 __main__:458:logMetrics E37 trn_all  0.2911 loss,  73.2% tp,  26.8% fn,      18.4% fp\n",
      "2023-12-11 05:24:54,544 INFO     pid:22692 __main__:191:main Epoch 38 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:24:54,560 WARNING  pid:22692 util:109:enumerateWithEstimate E38 Training ----/6754, starting\n",
      "2023-12-11 05:25:37,446 INFO     pid:22692 util:126:enumerateWithEstimate E38 Training   64/6754, done at 2023-12-11 05:32:39, 0:07:05\n",
      "2023-12-11 05:25:49,539 INFO     pid:22692 util:126:enumerateWithEstimate E38 Training  256/6754, done at 2023-12-11 05:32:38, 0:07:04\n",
      "2023-12-11 05:26:37,758 INFO     pid:22692 util:126:enumerateWithEstimate E38 Training 1024/6754, done at 2023-12-11 05:32:37, 0:07:03\n",
      "2023-12-11 05:29:49,493 INFO     pid:22692 util:126:enumerateWithEstimate E38 Training 4096/6754, done at 2023-12-11 05:32:35, 0:07:01\n",
      "2023-12-11 05:32:36,912 WARNING  pid:22692 util:139:enumerateWithEstimate E38 Training ----/6754, done at 2023-12-11 05:32:36\n",
      "2023-12-11 05:32:36,912 INFO     pid:22692 __main__:409:logMetrics E38 SegmentationTrainingApp\n",
      "2023-12-11 05:32:36,912 INFO     pid:22692 __main__:444:logMetrics E38 trn      0.2961 loss, 0.1915 fnloss, 0.9960 fploss, 0.7940 precision, 0.7254 recall, 0.7582 f1 score\n",
      "2023-12-11 05:32:36,912 INFO     pid:22692 __main__:458:logMetrics E38 trn_all  0.2961 loss,  72.5% tp,  27.5% fn,      18.8% fp\n",
      "2023-12-11 05:32:36,912 INFO     pid:22692 __main__:191:main Epoch 39 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:32:36,912 WARNING  pid:22692 util:109:enumerateWithEstimate E39 Training ----/6754, starting\n",
      "2023-12-11 05:33:19,690 INFO     pid:22692 util:126:enumerateWithEstimate E39 Training   64/6754, done at 2023-12-11 05:40:15, 0:06:58\n",
      "2023-12-11 05:33:31,783 INFO     pid:22692 util:126:enumerateWithEstimate E39 Training  256/6754, done at 2023-12-11 05:40:19, 0:07:03\n",
      "2023-12-11 05:34:20,061 INFO     pid:22692 util:126:enumerateWithEstimate E39 Training 1024/6754, done at 2023-12-11 05:40:20, 0:07:03\n",
      "2023-12-11 05:37:31,763 INFO     pid:22692 util:126:enumerateWithEstimate E39 Training 4096/6754, done at 2023-12-11 05:40:17, 0:07:01\n",
      "2023-12-11 05:40:19,345 WARNING  pid:22692 util:139:enumerateWithEstimate E39 Training ----/6754, done at 2023-12-11 05:40:19\n",
      "2023-12-11 05:40:19,345 INFO     pid:22692 __main__:409:logMetrics E39 SegmentationTrainingApp\n",
      "2023-12-11 05:40:19,345 INFO     pid:22692 __main__:444:logMetrics E39 trn      0.2892 loss, 0.1860 fnloss, 0.9960 fploss, 0.8013 precision, 0.7344 recall, 0.7664 f1 score\n",
      "2023-12-11 05:40:19,345 INFO     pid:22692 __main__:458:logMetrics E39 trn_all  0.2892 loss,  73.4% tp,  26.6% fn,      18.2% fp\n",
      "2023-12-11 05:40:19,345 INFO     pid:22692 __main__:191:main Epoch 40 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:40:19,361 WARNING  pid:22692 util:109:enumerateWithEstimate E40 Training ----/6754, starting\n",
      "2023-12-11 05:41:02,419 INFO     pid:22692 util:126:enumerateWithEstimate E40 Training   64/6754, done at 2023-12-11 05:48:08, 0:07:09\n",
      "2023-12-11 05:41:14,543 INFO     pid:22692 util:126:enumerateWithEstimate E40 Training  256/6754, done at 2023-12-11 05:48:05, 0:07:06\n",
      "2023-12-11 05:42:02,792 INFO     pid:22692 util:126:enumerateWithEstimate E40 Training 1024/6754, done at 2023-12-11 05:48:03, 0:07:04\n",
      "2023-12-11 05:45:14,639 INFO     pid:22692 util:126:enumerateWithEstimate E40 Training 4096/6754, done at 2023-12-11 05:48:00, 0:07:01\n",
      "2023-12-11 05:48:01,943 WARNING  pid:22692 util:139:enumerateWithEstimate E40 Training ----/6754, done at 2023-12-11 05:48:01\n",
      "2023-12-11 05:48:01,943 INFO     pid:22692 __main__:409:logMetrics E40 SegmentationTrainingApp\n",
      "2023-12-11 05:48:01,958 INFO     pid:22692 __main__:444:logMetrics E40 trn      0.2897 loss, 0.1792 fnloss, 0.9960 fploss, 0.7925 precision, 0.7381 recall, 0.7643 f1 score\n",
      "2023-12-11 05:48:01,958 INFO     pid:22692 __main__:458:logMetrics E40 trn_all  0.2897 loss,  73.8% tp,  26.2% fn,      19.3% fp\n",
      "2023-12-11 05:48:01,958 WARNING  pid:22692 util:109:enumerateWithEstimate E40 Validation  ----/862, starting\n",
      "2023-12-11 05:48:08,104 INFO     pid:22692 util:126:enumerateWithEstimate E40 Validation    64/862, done at 2023-12-11 05:48:22, 0:00:15\n",
      "2023-12-11 05:48:11,557 INFO     pid:22692 util:126:enumerateWithEstimate E40 Validation   256/862, done at 2023-12-11 05:48:22, 0:00:15\n",
      "2023-12-11 05:48:23,947 WARNING  pid:22692 util:139:enumerateWithEstimate E40 Validation  ----/862, done at 2023-12-11 05:48:23\n",
      "2023-12-11 05:48:23,947 INFO     pid:22692 __main__:409:logMetrics E40 SegmentationTrainingApp\n",
      "2023-12-11 05:48:23,947 INFO     pid:22692 __main__:444:logMetrics E40 val      0.4144 loss, 0.3701 fnloss, 0.9917 fploss, 0.8678 precision, 0.7283 recall, 0.7919 f1 score\n",
      "2023-12-11 05:48:23,947 INFO     pid:22692 __main__:458:logMetrics E40 val_all  0.4144 loss,  72.8% tp,  27.2% fn,      11.1% fp\n",
      "2023-12-11 05:48:24,072 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.540320.state\n",
      "2023-12-11 05:48:24,134 INFO     pid:22692 __main__:523:saveModel SHA1: a754b7b12b71f4086b0ddb01811c3d5ab2456af5\n",
      "2023-12-11 05:49:10,084 INFO     pid:22692 __main__:191:main Epoch 41 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:49:10,100 WARNING  pid:22692 util:109:enumerateWithEstimate E41 Training ----/6754, starting\n",
      "2023-12-11 05:49:52,752 INFO     pid:22692 util:126:enumerateWithEstimate E41 Training   64/6754, done at 2023-12-11 05:56:48, 0:06:58\n",
      "2023-12-11 05:50:04,829 INFO     pid:22692 util:126:enumerateWithEstimate E41 Training  256/6754, done at 2023-12-11 05:56:52, 0:07:02\n",
      "2023-12-11 05:50:52,984 INFO     pid:22692 util:126:enumerateWithEstimate E41 Training 1024/6754, done at 2023-12-11 05:56:52, 0:07:02\n",
      "2023-12-11 05:54:04,797 INFO     pid:22692 util:126:enumerateWithEstimate E41 Training 4096/6754, done at 2023-12-11 05:56:50, 0:07:01\n",
      "2023-12-11 05:56:53,424 WARNING  pid:22692 util:139:enumerateWithEstimate E41 Training ----/6754, done at 2023-12-11 05:56:53\n",
      "2023-12-11 05:56:53,424 INFO     pid:22692 __main__:409:logMetrics E41 SegmentationTrainingApp\n",
      "2023-12-11 05:56:53,424 INFO     pid:22692 __main__:444:logMetrics E41 trn      0.2882 loss, 0.1832 fnloss, 0.9960 fploss, 0.7946 precision, 0.7368 recall, 0.7646 f1 score\n",
      "2023-12-11 05:56:53,424 INFO     pid:22692 __main__:458:logMetrics E41 trn_all  0.2882 loss,  73.7% tp,  26.3% fn,      19.0% fp\n",
      "2023-12-11 05:56:53,424 INFO     pid:22692 __main__:191:main Epoch 42 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 05:56:53,440 WARNING  pid:22692 util:109:enumerateWithEstimate E42 Training ----/6754, starting\n",
      "2023-12-11 05:57:36,216 INFO     pid:22692 util:126:enumerateWithEstimate E42 Training   64/6754, done at 2023-12-11 06:04:34, 0:07:01\n",
      "2023-12-11 05:57:48,311 INFO     pid:22692 util:126:enumerateWithEstimate E42 Training  256/6754, done at 2023-12-11 06:04:36, 0:07:03\n",
      "2023-12-11 05:58:36,575 INFO     pid:22692 util:126:enumerateWithEstimate E42 Training 1024/6754, done at 2023-12-11 06:04:36, 0:07:03\n",
      "2023-12-11 06:01:48,110 INFO     pid:22692 util:126:enumerateWithEstimate E42 Training 4096/6754, done at 2023-12-11 06:04:34, 0:07:00\n",
      "2023-12-11 06:04:35,817 WARNING  pid:22692 util:139:enumerateWithEstimate E42 Training ----/6754, done at 2023-12-11 06:04:35\n",
      "2023-12-11 06:04:35,817 INFO     pid:22692 __main__:409:logMetrics E42 SegmentationTrainingApp\n",
      "2023-12-11 06:04:35,817 INFO     pid:22692 __main__:444:logMetrics E42 trn      0.2849 loss, 0.1759 fnloss, 0.9960 fploss, 0.7960 precision, 0.7463 recall, 0.7704 f1 score\n",
      "2023-12-11 06:04:35,817 INFO     pid:22692 __main__:458:logMetrics E42 trn_all  0.2849 loss,  74.6% tp,  25.4% fn,      19.1% fp\n",
      "2023-12-11 06:04:35,832 INFO     pid:22692 __main__:191:main Epoch 43 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:04:35,832 WARNING  pid:22692 util:109:enumerateWithEstimate E43 Training ----/6754, starting\n",
      "2023-12-11 06:05:18,657 INFO     pid:22692 util:126:enumerateWithEstimate E43 Training   64/6754, done at 2023-12-11 06:12:18, 0:07:03\n",
      "2023-12-11 06:05:30,766 INFO     pid:22692 util:126:enumerateWithEstimate E43 Training  256/6754, done at 2023-12-11 06:12:20, 0:07:04\n",
      "2023-12-11 06:06:19,138 INFO     pid:22692 util:126:enumerateWithEstimate E43 Training 1024/6754, done at 2023-12-11 06:12:20, 0:07:04\n",
      "2023-12-11 06:09:30,766 INFO     pid:22692 util:126:enumerateWithEstimate E43 Training 4096/6754, done at 2023-12-11 06:12:16, 0:07:01\n",
      "2023-12-11 06:12:18,025 WARNING  pid:22692 util:139:enumerateWithEstimate E43 Training ----/6754, done at 2023-12-11 06:12:18\n",
      "2023-12-11 06:12:18,025 INFO     pid:22692 __main__:409:logMetrics E43 SegmentationTrainingApp\n",
      "2023-12-11 06:12:18,025 INFO     pid:22692 __main__:444:logMetrics E43 trn      0.2890 loss, 0.1886 fnloss, 0.9959 fploss, 0.8062 precision, 0.7350 recall, 0.7690 f1 score\n",
      "2023-12-11 06:12:18,025 INFO     pid:22692 __main__:458:logMetrics E43 trn_all  0.2890 loss,  73.5% tp,  26.5% fn,      17.7% fp\n",
      "2023-12-11 06:12:18,040 INFO     pid:22692 __main__:191:main Epoch 44 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:12:18,040 WARNING  pid:22692 util:109:enumerateWithEstimate E44 Training ----/6754, starting\n",
      "2023-12-11 06:13:01,130 INFO     pid:22692 util:126:enumerateWithEstimate E44 Training   64/6754, done at 2023-12-11 06:20:01, 0:07:03\n",
      "2023-12-11 06:13:13,223 INFO     pid:22692 util:126:enumerateWithEstimate E44 Training  256/6754, done at 2023-12-11 06:20:02, 0:07:04\n",
      "2023-12-11 06:14:01,672 INFO     pid:22692 util:126:enumerateWithEstimate E44 Training 1024/6754, done at 2023-12-11 06:20:02, 0:07:04\n",
      "2023-12-11 06:17:13,518 INFO     pid:22692 util:126:enumerateWithEstimate E44 Training 4096/6754, done at 2023-12-11 06:19:59, 0:07:01\n",
      "2023-12-11 06:20:01,039 WARNING  pid:22692 util:139:enumerateWithEstimate E44 Training ----/6754, done at 2023-12-11 06:20:01\n",
      "2023-12-11 06:20:01,039 INFO     pid:22692 __main__:409:logMetrics E44 SegmentationTrainingApp\n",
      "2023-12-11 06:20:01,039 INFO     pid:22692 __main__:444:logMetrics E44 trn      0.2868 loss, 0.1805 fnloss, 0.9959 fploss, 0.7950 precision, 0.7444 recall, 0.7688 f1 score\n",
      "2023-12-11 06:20:01,039 INFO     pid:22692 __main__:458:logMetrics E44 trn_all  0.2868 loss,  74.4% tp,  25.6% fn,      19.2% fp\n",
      "2023-12-11 06:20:01,039 INFO     pid:22692 __main__:191:main Epoch 45 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:20:01,054 WARNING  pid:22692 util:109:enumerateWithEstimate E45 Training ----/6754, starting\n",
      "2023-12-11 06:20:43,753 INFO     pid:22692 util:126:enumerateWithEstimate E45 Training   64/6754, done at 2023-12-11 06:27:43, 0:07:03\n",
      "2023-12-11 06:20:55,846 INFO     pid:22692 util:126:enumerateWithEstimate E45 Training  256/6754, done at 2023-12-11 06:27:44, 0:07:04\n",
      "2023-12-11 06:21:43,704 INFO     pid:22692 util:126:enumerateWithEstimate E45 Training 1024/6754, done at 2023-12-11 06:27:41, 0:07:00\n",
      "2023-12-11 06:24:55,319 INFO     pid:22692 util:126:enumerateWithEstimate E45 Training 4096/6754, done at 2023-12-11 06:27:41, 0:07:00\n",
      "2023-12-11 06:27:42,763 WARNING  pid:22692 util:139:enumerateWithEstimate E45 Training ----/6754, done at 2023-12-11 06:27:42\n",
      "2023-12-11 06:27:42,763 INFO     pid:22692 __main__:409:logMetrics E45 SegmentationTrainingApp\n",
      "2023-12-11 06:27:42,763 INFO     pid:22692 __main__:444:logMetrics E45 trn      0.2828 loss, 0.1818 fnloss, 0.9958 fploss, 0.7946 precision, 0.7423 recall, 0.7676 f1 score\n",
      "2023-12-11 06:27:42,763 INFO     pid:22692 __main__:458:logMetrics E45 trn_all  0.2828 loss,  74.2% tp,  25.8% fn,      19.2% fp\n",
      "2023-12-11 06:27:42,763 WARNING  pid:22692 util:109:enumerateWithEstimate E45 Validation  ----/862, starting\n",
      "2023-12-11 06:27:48,860 INFO     pid:22692 util:126:enumerateWithEstimate E45 Validation    64/862, done at 2023-12-11 06:28:03, 0:00:15\n",
      "2023-12-11 06:27:52,328 INFO     pid:22692 util:126:enumerateWithEstimate E45 Validation   256/862, done at 2023-12-11 06:28:03, 0:00:15\n",
      "2023-12-11 06:28:04,702 WARNING  pid:22692 util:139:enumerateWithEstimate E45 Validation  ----/862, done at 2023-12-11 06:28:04\n",
      "2023-12-11 06:28:04,702 INFO     pid:22692 __main__:409:logMetrics E45 SegmentationTrainingApp\n",
      "2023-12-11 06:28:04,702 INFO     pid:22692 __main__:444:logMetrics E45 val      0.3850 loss, 0.3316 fnloss, 0.9918 fploss, 0.8623 precision, 0.7619 recall, 0.8090 f1 score\n",
      "2023-12-11 06:28:04,702 INFO     pid:22692 __main__:458:logMetrics E45 val_all  0.3850 loss,  76.2% tp,  23.8% fn,      12.2% fp\n",
      "2023-12-11 06:28:04,827 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.607860.state\n",
      "2023-12-11 06:28:04,890 INFO     pid:22692 __main__:523:saveModel SHA1: d2b3ff0ccffd7c22ae2d889e47bdfda8ae8cd13c\n",
      "2023-12-11 06:28:51,339 INFO     pid:22692 __main__:191:main Epoch 46 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:28:51,355 WARNING  pid:22692 util:109:enumerateWithEstimate E46 Training ----/6754, starting\n",
      "2023-12-11 06:29:33,929 INFO     pid:22692 util:126:enumerateWithEstimate E46 Training   64/6754, done at 2023-12-11 06:36:29, 0:06:58\n",
      "2023-12-11 06:29:45,992 INFO     pid:22692 util:126:enumerateWithEstimate E46 Training  256/6754, done at 2023-12-11 06:36:33, 0:07:02\n",
      "2023-12-11 06:30:34,366 INFO     pid:22692 util:126:enumerateWithEstimate E46 Training 1024/6754, done at 2023-12-11 06:36:34, 0:07:03\n",
      "2023-12-11 06:33:46,157 INFO     pid:22692 util:126:enumerateWithEstimate E46 Training 4096/6754, done at 2023-12-11 06:36:32, 0:07:01\n",
      "2023-12-11 06:36:33,537 WARNING  pid:22692 util:139:enumerateWithEstimate E46 Training ----/6754, done at 2023-12-11 06:36:33\n",
      "2023-12-11 06:36:33,537 INFO     pid:22692 __main__:409:logMetrics E46 SegmentationTrainingApp\n",
      "2023-12-11 06:36:33,537 INFO     pid:22692 __main__:444:logMetrics E46 trn      0.2809 loss, 0.1723 fnloss, 0.9960 fploss, 0.7999 precision, 0.7494 recall, 0.7738 f1 score\n",
      "2023-12-11 06:36:33,537 INFO     pid:22692 __main__:458:logMetrics E46 trn_all  0.2809 loss,  74.9% tp,  25.1% fn,      18.7% fp\n",
      "2023-12-11 06:36:33,537 INFO     pid:22692 __main__:191:main Epoch 47 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:36:33,537 WARNING  pid:22692 util:109:enumerateWithEstimate E47 Training ----/6754, starting\n",
      "2023-12-11 06:37:16,377 INFO     pid:22692 util:126:enumerateWithEstimate E47 Training   64/6754, done at 2023-12-11 06:44:16, 0:07:03\n",
      "2023-12-11 06:37:28,453 INFO     pid:22692 util:126:enumerateWithEstimate E47 Training  256/6754, done at 2023-12-11 06:44:17, 0:07:03\n",
      "2023-12-11 06:38:16,875 INFO     pid:22692 util:126:enumerateWithEstimate E47 Training 1024/6754, done at 2023-12-11 06:44:17, 0:07:04\n",
      "2023-12-11 06:41:28,734 INFO     pid:22692 util:126:enumerateWithEstimate E47 Training 4096/6754, done at 2023-12-11 06:44:15, 0:07:01\n",
      "2023-12-11 06:44:16,195 WARNING  pid:22692 util:139:enumerateWithEstimate E47 Training ----/6754, done at 2023-12-11 06:44:16\n",
      "2023-12-11 06:44:16,195 INFO     pid:22692 __main__:409:logMetrics E47 SegmentationTrainingApp\n",
      "2023-12-11 06:44:16,195 INFO     pid:22692 __main__:444:logMetrics E47 trn      0.2815 loss, 0.1814 fnloss, 0.9958 fploss, 0.8100 precision, 0.7407 recall, 0.7738 f1 score\n",
      "2023-12-11 06:44:16,195 INFO     pid:22692 __main__:458:logMetrics E47 trn_all  0.2815 loss,  74.1% tp,  25.9% fn,      17.4% fp\n",
      "2023-12-11 06:44:16,210 INFO     pid:22692 __main__:191:main Epoch 48 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:44:16,210 WARNING  pid:22692 util:109:enumerateWithEstimate E48 Training ----/6754, starting\n",
      "2023-12-11 06:44:59,224 INFO     pid:22692 util:126:enumerateWithEstimate E48 Training   64/6754, done at 2023-12-11 06:51:59, 0:07:03\n",
      "2023-12-11 06:45:11,316 INFO     pid:22692 util:126:enumerateWithEstimate E48 Training  256/6754, done at 2023-12-11 06:52:00, 0:07:04\n",
      "2023-12-11 06:45:59,798 INFO     pid:22692 util:126:enumerateWithEstimate E48 Training 1024/6754, done at 2023-12-11 06:52:01, 0:07:05\n",
      "2023-12-11 06:49:11,424 INFO     pid:22692 util:126:enumerateWithEstimate E48 Training 4096/6754, done at 2023-12-11 06:51:57, 0:07:01\n",
      "2023-12-11 06:51:58,774 WARNING  pid:22692 util:139:enumerateWithEstimate E48 Training ----/6754, done at 2023-12-11 06:51:58\n",
      "2023-12-11 06:51:58,774 INFO     pid:22692 __main__:409:logMetrics E48 SegmentationTrainingApp\n",
      "2023-12-11 06:51:58,774 INFO     pid:22692 __main__:444:logMetrics E48 trn      0.2797 loss, 0.1748 fnloss, 0.9959 fploss, 0.8036 precision, 0.7509 recall, 0.7764 f1 score\n",
      "2023-12-11 06:51:58,774 INFO     pid:22692 __main__:458:logMetrics E48 trn_all  0.2797 loss,  75.1% tp,  24.9% fn,      18.4% fp\n",
      "2023-12-11 06:51:58,774 INFO     pid:22692 __main__:191:main Epoch 49 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:51:58,790 WARNING  pid:22692 util:109:enumerateWithEstimate E49 Training ----/6754, starting\n",
      "2023-12-11 06:52:41,708 INFO     pid:22692 util:126:enumerateWithEstimate E49 Training   64/6754, done at 2023-12-11 06:59:41, 0:07:03\n",
      "2023-12-11 06:52:53,816 INFO     pid:22692 util:126:enumerateWithEstimate E49 Training  256/6754, done at 2023-12-11 06:59:43, 0:07:04\n",
      "2023-12-11 06:53:42,235 INFO     pid:22692 util:126:enumerateWithEstimate E49 Training 1024/6754, done at 2023-12-11 06:59:43, 0:07:04\n",
      "2023-12-11 06:56:54,067 INFO     pid:22692 util:126:enumerateWithEstimate E49 Training 4096/6754, done at 2023-12-11 06:59:40, 0:07:01\n",
      "2023-12-11 06:59:41,404 WARNING  pid:22692 util:139:enumerateWithEstimate E49 Training ----/6754, done at 2023-12-11 06:59:41\n",
      "2023-12-11 06:59:41,404 INFO     pid:22692 __main__:409:logMetrics E49 SegmentationTrainingApp\n",
      "2023-12-11 06:59:41,404 INFO     pid:22692 __main__:444:logMetrics E49 trn      0.2797 loss, 0.1734 fnloss, 0.9959 fploss, 0.8019 precision, 0.7494 recall, 0.7748 f1 score\n",
      "2023-12-11 06:59:41,404 INFO     pid:22692 __main__:458:logMetrics E49 trn_all  0.2797 loss,  74.9% tp,  25.1% fn,      18.5% fp\n",
      "2023-12-11 06:59:41,419 INFO     pid:22692 __main__:191:main Epoch 50 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 06:59:41,419 WARNING  pid:22692 util:109:enumerateWithEstimate E50 Training ----/6754, starting\n",
      "2023-12-11 07:00:24,305 INFO     pid:22692 util:126:enumerateWithEstimate E50 Training   64/6754, done at 2023-12-11 07:07:24, 0:07:03\n",
      "2023-12-11 07:00:36,398 INFO     pid:22692 util:126:enumerateWithEstimate E50 Training  256/6754, done at 2023-12-11 07:07:25, 0:07:04\n",
      "2023-12-11 07:01:24,800 INFO     pid:22692 util:126:enumerateWithEstimate E50 Training 1024/6754, done at 2023-12-11 07:07:25, 0:07:04\n",
      "2023-12-11 07:04:36,755 INFO     pid:22692 util:126:enumerateWithEstimate E50 Training 4096/6754, done at 2023-12-11 07:07:23, 0:07:01\n",
      "2023-12-11 07:07:24,228 WARNING  pid:22692 util:139:enumerateWithEstimate E50 Training ----/6754, done at 2023-12-11 07:07:24\n",
      "2023-12-11 07:07:24,228 INFO     pid:22692 __main__:409:logMetrics E50 SegmentationTrainingApp\n",
      "2023-12-11 07:07:24,228 INFO     pid:22692 __main__:444:logMetrics E50 trn      0.2769 loss, 0.1754 fnloss, 0.9959 fploss, 0.8037 precision, 0.7448 recall, 0.7732 f1 score\n",
      "2023-12-11 07:07:24,228 INFO     pid:22692 __main__:458:logMetrics E50 trn_all  0.2769 loss,  74.5% tp,  25.5% fn,      18.2% fp\n",
      "2023-12-11 07:07:24,244 WARNING  pid:22692 util:109:enumerateWithEstimate E50 Validation  ----/862, starting\n",
      "2023-12-11 07:07:30,497 INFO     pid:22692 util:126:enumerateWithEstimate E50 Validation    64/862, done at 2023-12-11 07:07:44, 0:00:15\n",
      "2023-12-11 07:07:33,965 INFO     pid:22692 util:126:enumerateWithEstimate E50 Validation   256/862, done at 2023-12-11 07:07:44, 0:00:15\n",
      "2023-12-11 07:07:46,277 WARNING  pid:22692 util:139:enumerateWithEstimate E50 Validation  ----/862, done at 2023-12-11 07:07:46\n",
      "2023-12-11 07:07:46,277 INFO     pid:22692 __main__:409:logMetrics E50 SegmentationTrainingApp\n",
      "2023-12-11 07:07:46,277 INFO     pid:22692 __main__:444:logMetrics E50 val      0.3457 loss, 0.2873 fnloss, 0.9919 fploss, 0.8579 precision, 0.7830 recall, 0.8187 f1 score\n",
      "2023-12-11 07:07:46,277 INFO     pid:22692 __main__:458:logMetrics E50 val_all  0.3457 loss,  78.3% tp,  21.7% fn,      13.0% fp\n",
      "2023-12-11 07:07:46,386 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.675400.state\n",
      "2023-12-11 07:07:46,433 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 07:07:46,480 INFO     pid:22692 __main__:523:saveModel SHA1: 5ec2287960e7cad37b455e2351d9bfd11a5b1fc4\n",
      "2023-12-11 07:08:33,101 INFO     pid:22692 __main__:191:main Epoch 51 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:08:33,101 WARNING  pid:22692 util:109:enumerateWithEstimate E51 Training ----/6754, starting\n",
      "2023-12-11 07:09:15,565 INFO     pid:22692 util:126:enumerateWithEstimate E51 Training   64/6754, done at 2023-12-11 07:16:13, 0:07:01\n",
      "2023-12-11 07:09:27,689 INFO     pid:22692 util:126:enumerateWithEstimate E51 Training  256/6754, done at 2023-12-11 07:16:17, 0:07:04\n",
      "2023-12-11 07:10:16,175 INFO     pid:22692 util:126:enumerateWithEstimate E51 Training 1024/6754, done at 2023-12-11 07:16:17, 0:07:05\n",
      "2023-12-11 07:13:28,721 INFO     pid:22692 util:126:enumerateWithEstimate E51 Training 4096/6754, done at 2023-12-11 07:16:15, 0:07:03\n",
      "2023-12-11 07:16:16,571 WARNING  pid:22692 util:139:enumerateWithEstimate E51 Training ----/6754, done at 2023-12-11 07:16:16\n",
      "2023-12-11 07:16:16,571 INFO     pid:22692 __main__:409:logMetrics E51 SegmentationTrainingApp\n",
      "2023-12-11 07:16:16,571 INFO     pid:22692 __main__:444:logMetrics E51 trn      0.2707 loss, 0.1655 fnloss, 0.9959 fploss, 0.8069 precision, 0.7584 recall, 0.7819 f1 score\n",
      "2023-12-11 07:16:16,571 INFO     pid:22692 __main__:458:logMetrics E51 trn_all  0.2707 loss,  75.8% tp,  24.2% fn,      18.2% fp\n",
      "2023-12-11 07:16:16,571 INFO     pid:22692 __main__:191:main Epoch 52 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:16:16,587 WARNING  pid:22692 util:109:enumerateWithEstimate E52 Training ----/6754, starting\n",
      "2023-12-11 07:16:59,817 INFO     pid:22692 util:126:enumerateWithEstimate E52 Training   64/6754, done at 2023-12-11 07:24:02, 0:07:05\n",
      "2023-12-11 07:17:11,909 INFO     pid:22692 util:126:enumerateWithEstimate E52 Training  256/6754, done at 2023-12-11 07:24:01, 0:07:04\n",
      "2023-12-11 07:18:00,344 INFO     pid:22692 util:126:enumerateWithEstimate E52 Training 1024/6754, done at 2023-12-11 07:24:01, 0:07:04\n",
      "2023-12-11 07:21:12,160 INFO     pid:22692 util:126:enumerateWithEstimate E52 Training 4096/6754, done at 2023-12-11 07:23:58, 0:07:01\n",
      "2023-12-11 07:23:59,604 WARNING  pid:22692 util:139:enumerateWithEstimate E52 Training ----/6754, done at 2023-12-11 07:23:59\n",
      "2023-12-11 07:23:59,604 INFO     pid:22692 __main__:409:logMetrics E52 SegmentationTrainingApp\n",
      "2023-12-11 07:23:59,604 INFO     pid:22692 __main__:444:logMetrics E52 trn      0.2739 loss, 0.1704 fnloss, 0.9959 fploss, 0.8046 precision, 0.7560 recall, 0.7796 f1 score\n",
      "2023-12-11 07:23:59,604 INFO     pid:22692 __main__:458:logMetrics E52 trn_all  0.2739 loss,  75.6% tp,  24.4% fn,      18.4% fp\n",
      "2023-12-11 07:23:59,604 INFO     pid:22692 __main__:191:main Epoch 53 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:23:59,604 WARNING  pid:22692 util:109:enumerateWithEstimate E53 Training ----/6754, starting\n",
      "2023-12-11 07:24:42,459 INFO     pid:22692 util:126:enumerateWithEstimate E53 Training   64/6754, done at 2023-12-11 07:31:42, 0:07:03\n",
      "2023-12-11 07:24:54,551 INFO     pid:22692 util:126:enumerateWithEstimate E53 Training  256/6754, done at 2023-12-11 07:31:43, 0:07:04\n",
      "2023-12-11 07:25:42,861 INFO     pid:22692 util:126:enumerateWithEstimate E53 Training 1024/6754, done at 2023-12-11 07:31:43, 0:07:03\n",
      "2023-12-11 07:28:54,677 INFO     pid:22692 util:126:enumerateWithEstimate E53 Training 4096/6754, done at 2023-12-11 07:31:40, 0:07:01\n",
      "2023-12-11 07:31:42,130 WARNING  pid:22692 util:139:enumerateWithEstimate E53 Training ----/6754, done at 2023-12-11 07:31:42\n",
      "2023-12-11 07:31:42,130 INFO     pid:22692 __main__:409:logMetrics E53 SegmentationTrainingApp\n",
      "2023-12-11 07:31:42,130 INFO     pid:22692 __main__:444:logMetrics E53 trn      0.2790 loss, 0.1792 fnloss, 0.9959 fploss, 0.8087 precision, 0.7444 recall, 0.7752 f1 score\n",
      "2023-12-11 07:31:42,130 INFO     pid:22692 __main__:458:logMetrics E53 trn_all  0.2790 loss,  74.4% tp,  25.6% fn,      17.6% fp\n",
      "2023-12-11 07:31:42,130 INFO     pid:22692 __main__:191:main Epoch 54 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:31:42,145 WARNING  pid:22692 util:109:enumerateWithEstimate E54 Training ----/6754, starting\n",
      "2023-12-11 07:32:25,047 INFO     pid:22692 util:126:enumerateWithEstimate E54 Training   64/6754, done at 2023-12-11 07:39:25, 0:07:03\n",
      "2023-12-11 07:32:37,140 INFO     pid:22692 util:126:enumerateWithEstimate E54 Training  256/6754, done at 2023-12-11 07:39:26, 0:07:04\n",
      "2023-12-11 07:33:25,544 INFO     pid:22692 util:126:enumerateWithEstimate E54 Training 1024/6754, done at 2023-12-11 07:39:26, 0:07:04\n",
      "2023-12-11 07:36:37,268 INFO     pid:22692 util:126:enumerateWithEstimate E54 Training 4096/6754, done at 2023-12-11 07:39:23, 0:07:01\n",
      "2023-12-11 07:39:24,679 WARNING  pid:22692 util:139:enumerateWithEstimate E54 Training ----/6754, done at 2023-12-11 07:39:24\n",
      "2023-12-11 07:39:24,679 INFO     pid:22692 __main__:409:logMetrics E54 SegmentationTrainingApp\n",
      "2023-12-11 07:39:24,679 INFO     pid:22692 __main__:444:logMetrics E54 trn      0.2741 loss, 0.1708 fnloss, 0.9958 fploss, 0.8045 precision, 0.7597 recall, 0.7815 f1 score\n",
      "2023-12-11 07:39:24,679 INFO     pid:22692 __main__:458:logMetrics E54 trn_all  0.2741 loss,  76.0% tp,  24.0% fn,      18.5% fp\n",
      "2023-12-11 07:39:24,679 INFO     pid:22692 __main__:191:main Epoch 55 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:39:24,679 WARNING  pid:22692 util:109:enumerateWithEstimate E55 Training ----/6754, starting\n",
      "2023-12-11 07:40:07,752 INFO     pid:22692 util:126:enumerateWithEstimate E55 Training   64/6754, done at 2023-12-11 07:47:10, 0:07:05\n",
      "2023-12-11 07:40:19,861 INFO     pid:22692 util:126:enumerateWithEstimate E55 Training  256/6754, done at 2023-12-11 07:47:09, 0:07:05\n",
      "2023-12-11 07:41:08,106 INFO     pid:22692 util:126:enumerateWithEstimate E55 Training 1024/6754, done at 2023-12-11 07:47:08, 0:07:03\n",
      "2023-12-11 07:44:19,659 INFO     pid:22692 util:126:enumerateWithEstimate E55 Training 4096/6754, done at 2023-12-11 07:47:05, 0:07:01\n",
      "2023-12-11 07:47:07,211 WARNING  pid:22692 util:139:enumerateWithEstimate E55 Training ----/6754, done at 2023-12-11 07:47:07\n",
      "2023-12-11 07:47:07,227 INFO     pid:22692 __main__:409:logMetrics E55 SegmentationTrainingApp\n",
      "2023-12-11 07:47:07,227 INFO     pid:22692 __main__:444:logMetrics E55 trn      0.2755 loss, 0.1690 fnloss, 0.9959 fploss, 0.8077 precision, 0.7561 recall, 0.7810 f1 score\n",
      "2023-12-11 07:47:07,227 INFO     pid:22692 __main__:458:logMetrics E55 trn_all  0.2755 loss,  75.6% tp,  24.4% fn,      18.0% fp\n",
      "2023-12-11 07:47:07,227 WARNING  pid:22692 util:109:enumerateWithEstimate E55 Validation  ----/862, starting\n",
      "2023-12-11 07:47:13,513 INFO     pid:22692 util:126:enumerateWithEstimate E55 Validation    64/862, done at 2023-12-11 07:47:27, 0:00:15\n",
      "2023-12-11 07:47:16,966 INFO     pid:22692 util:126:enumerateWithEstimate E55 Validation   256/862, done at 2023-12-11 07:47:27, 0:00:15\n",
      "2023-12-11 07:47:29,324 WARNING  pid:22692 util:139:enumerateWithEstimate E55 Validation  ----/862, done at 2023-12-11 07:47:29\n",
      "2023-12-11 07:47:29,324 INFO     pid:22692 __main__:409:logMetrics E55 SegmentationTrainingApp\n",
      "2023-12-11 07:47:29,324 INFO     pid:22692 __main__:444:logMetrics E55 val      0.3864 loss, 0.3407 fnloss, 0.9920 fploss, 0.8806 precision, 0.7224 recall, 0.7937 f1 score\n",
      "2023-12-11 07:47:29,324 INFO     pid:22692 __main__:458:logMetrics E55 val_all  0.3864 loss,  72.2% tp,  27.8% fn,       9.8% fp\n",
      "2023-12-11 07:47:29,465 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.742940.state\n",
      "2023-12-11 07:47:29,527 INFO     pid:22692 __main__:523:saveModel SHA1: f3401a8f8b78e385173be4f1133c46f0d9775460\n",
      "2023-12-11 07:48:15,523 INFO     pid:22692 __main__:191:main Epoch 56 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:48:15,539 WARNING  pid:22692 util:109:enumerateWithEstimate E56 Training ----/6754, starting\n",
      "2023-12-11 07:48:57,925 INFO     pid:22692 util:126:enumerateWithEstimate E56 Training   64/6754, done at 2023-12-11 07:55:51, 0:06:56\n",
      "2023-12-11 07:49:09,909 INFO     pid:22692 util:126:enumerateWithEstimate E56 Training  256/6754, done at 2023-12-11 07:55:54, 0:06:59\n",
      "2023-12-11 07:49:58,347 INFO     pid:22692 util:126:enumerateWithEstimate E56 Training 1024/6754, done at 2023-12-11 07:55:58, 0:07:03\n",
      "2023-12-11 07:53:10,192 INFO     pid:22692 util:126:enumerateWithEstimate E56 Training 4096/6754, done at 2023-12-11 07:55:56, 0:07:01\n",
      "2023-12-11 07:55:57,682 WARNING  pid:22692 util:139:enumerateWithEstimate E56 Training ----/6754, done at 2023-12-11 07:55:57\n",
      "2023-12-11 07:55:57,682 INFO     pid:22692 __main__:409:logMetrics E56 SegmentationTrainingApp\n",
      "2023-12-11 07:55:57,698 INFO     pid:22692 __main__:444:logMetrics E56 trn      0.2757 loss, 0.1720 fnloss, 0.9959 fploss, 0.8058 precision, 0.7587 recall, 0.7815 f1 score\n",
      "2023-12-11 07:55:57,698 INFO     pid:22692 __main__:458:logMetrics E56 trn_all  0.2757 loss,  75.9% tp,  24.1% fn,      18.3% fp\n",
      "2023-12-11 07:55:57,698 INFO     pid:22692 __main__:191:main Epoch 57 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 07:55:57,698 WARNING  pid:22692 util:109:enumerateWithEstimate E57 Training ----/6754, starting\n",
      "2023-12-11 07:56:40,834 INFO     pid:22692 util:126:enumerateWithEstimate E57 Training   64/6754, done at 2023-12-11 08:03:40, 0:07:03\n",
      "2023-12-11 07:56:52,942 INFO     pid:22692 util:126:enumerateWithEstimate E57 Training  256/6754, done at 2023-12-11 08:03:42, 0:07:04\n",
      "2023-12-11 07:57:41,423 INFO     pid:22692 util:126:enumerateWithEstimate E57 Training 1024/6754, done at 2023-12-11 08:03:42, 0:07:05\n",
      "2023-12-11 08:00:53,208 INFO     pid:22692 util:126:enumerateWithEstimate E57 Training 4096/6754, done at 2023-12-11 08:03:39, 0:07:01\n",
      "2023-12-11 08:03:40,736 WARNING  pid:22692 util:139:enumerateWithEstimate E57 Training ----/6754, done at 2023-12-11 08:03:40\n",
      "2023-12-11 08:03:40,736 INFO     pid:22692 __main__:409:logMetrics E57 SegmentationTrainingApp\n",
      "2023-12-11 08:03:40,736 INFO     pid:22692 __main__:444:logMetrics E57 trn      0.2699 loss, 0.1649 fnloss, 0.9959 fploss, 0.8100 precision, 0.7616 recall, 0.7851 f1 score\n",
      "2023-12-11 08:03:40,736 INFO     pid:22692 __main__:458:logMetrics E57 trn_all  0.2699 loss,  76.2% tp,  23.8% fn,      17.9% fp\n",
      "2023-12-11 08:03:40,736 INFO     pid:22692 __main__:191:main Epoch 58 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:03:40,736 WARNING  pid:22692 util:109:enumerateWithEstimate E58 Training ----/6754, starting\n",
      "2023-12-11 08:04:23,653 INFO     pid:22692 util:126:enumerateWithEstimate E58 Training   64/6754, done at 2023-12-11 08:11:25, 0:07:05\n",
      "2023-12-11 08:04:35,730 INFO     pid:22692 util:126:enumerateWithEstimate E58 Training  256/6754, done at 2023-12-11 08:11:24, 0:07:04\n",
      "2023-12-11 08:05:24,101 INFO     pid:22692 util:126:enumerateWithEstimate E58 Training 1024/6754, done at 2023-12-11 08:11:24, 0:07:04\n",
      "2023-12-11 08:08:35,745 INFO     pid:22692 util:126:enumerateWithEstimate E58 Training 4096/6754, done at 2023-12-11 08:11:21, 0:07:01\n",
      "2023-12-11 08:11:23,208 WARNING  pid:22692 util:139:enumerateWithEstimate E58 Training ----/6754, done at 2023-12-11 08:11:23\n",
      "2023-12-11 08:11:23,208 INFO     pid:22692 __main__:409:logMetrics E58 SegmentationTrainingApp\n",
      "2023-12-11 08:11:23,208 INFO     pid:22692 __main__:444:logMetrics E58 trn      0.2678 loss, 0.1621 fnloss, 0.9960 fploss, 0.8097 precision, 0.7614 recall, 0.7849 f1 score\n",
      "2023-12-11 08:11:23,208 INFO     pid:22692 __main__:458:logMetrics E58 trn_all  0.2678 loss,  76.1% tp,  23.9% fn,      17.9% fp\n",
      "2023-12-11 08:11:23,208 INFO     pid:22692 __main__:191:main Epoch 59 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:11:23,224 WARNING  pid:22692 util:109:enumerateWithEstimate E59 Training ----/6754, starting\n",
      "2023-12-11 08:12:06,453 INFO     pid:22692 util:126:enumerateWithEstimate E59 Training   64/6754, done at 2023-12-11 08:19:06, 0:07:03\n",
      "2023-12-11 08:12:18,561 INFO     pid:22692 util:126:enumerateWithEstimate E59 Training  256/6754, done at 2023-12-11 08:19:07, 0:07:04\n",
      "2023-12-11 08:13:06,979 INFO     pid:22692 util:126:enumerateWithEstimate E59 Training 1024/6754, done at 2023-12-11 08:19:08, 0:07:04\n",
      "2023-12-11 08:16:18,750 INFO     pid:22692 util:126:enumerateWithEstimate E59 Training 4096/6754, done at 2023-12-11 08:19:05, 0:07:01\n",
      "2023-12-11 08:19:06,056 WARNING  pid:22692 util:139:enumerateWithEstimate E59 Training ----/6754, done at 2023-12-11 08:19:06\n",
      "2023-12-11 08:19:06,056 INFO     pid:22692 __main__:409:logMetrics E59 SegmentationTrainingApp\n",
      "2023-12-11 08:19:06,056 INFO     pid:22692 __main__:444:logMetrics E59 trn      0.2702 loss, 0.1663 fnloss, 0.9959 fploss, 0.8120 precision, 0.7568 recall, 0.7834 f1 score\n",
      "2023-12-11 08:19:06,056 INFO     pid:22692 __main__:458:logMetrics E59 trn_all  0.2702 loss,  75.7% tp,  24.3% fn,      17.5% fp\n",
      "2023-12-11 08:19:06,056 INFO     pid:22692 __main__:191:main Epoch 60 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:19:06,056 WARNING  pid:22692 util:109:enumerateWithEstimate E60 Training ----/6754, starting\n",
      "2023-12-11 08:19:48,942 INFO     pid:22692 util:126:enumerateWithEstimate E60 Training   64/6754, done at 2023-12-11 08:26:49, 0:07:03\n",
      "2023-12-11 08:20:01,050 INFO     pid:22692 util:126:enumerateWithEstimate E60 Training  256/6754, done at 2023-12-11 08:26:50, 0:07:04\n",
      "2023-12-11 08:20:49,452 INFO     pid:22692 util:126:enumerateWithEstimate E60 Training 1024/6754, done at 2023-12-11 08:26:50, 0:07:04\n",
      "2023-12-11 08:24:01,238 INFO     pid:22692 util:126:enumerateWithEstimate E60 Training 4096/6754, done at 2023-12-11 08:26:47, 0:07:01\n",
      "2023-12-11 08:26:48,600 WARNING  pid:22692 util:139:enumerateWithEstimate E60 Training ----/6754, done at 2023-12-11 08:26:48\n",
      "2023-12-11 08:26:48,600 INFO     pid:22692 __main__:409:logMetrics E60 SegmentationTrainingApp\n",
      "2023-12-11 08:26:48,600 INFO     pid:22692 __main__:444:logMetrics E60 trn      0.2677 loss, 0.1656 fnloss, 0.9959 fploss, 0.8156 precision, 0.7615 recall, 0.7876 f1 score\n",
      "2023-12-11 08:26:48,600 INFO     pid:22692 __main__:458:logMetrics E60 trn_all  0.2677 loss,  76.1% tp,  23.9% fn,      17.2% fp\n",
      "2023-12-11 08:26:48,600 WARNING  pid:22692 util:109:enumerateWithEstimate E60 Validation  ----/862, starting\n",
      "2023-12-11 08:26:54,824 INFO     pid:22692 util:126:enumerateWithEstimate E60 Validation    64/862, done at 2023-12-11 08:27:09, 0:00:15\n",
      "2023-12-11 08:26:58,277 INFO     pid:22692 util:126:enumerateWithEstimate E60 Validation   256/862, done at 2023-12-11 08:27:09, 0:00:15\n",
      "2023-12-11 08:27:10,792 WARNING  pid:22692 util:139:enumerateWithEstimate E60 Validation  ----/862, done at 2023-12-11 08:27:10\n",
      "2023-12-11 08:27:10,792 INFO     pid:22692 __main__:409:logMetrics E60 SegmentationTrainingApp\n",
      "2023-12-11 08:27:10,792 INFO     pid:22692 __main__:444:logMetrics E60 val      0.3612 loss, 0.2406 fnloss, 0.9933 fploss, 0.7465 precision, 0.8269 recall, 0.7847 f1 score\n",
      "2023-12-11 08:27:10,792 INFO     pid:22692 __main__:458:logMetrics E60 val_all  0.3612 loss,  82.7% tp,  17.3% fn,      28.1% fp\n",
      "2023-12-11 08:27:10,917 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.810480.state\n",
      "2023-12-11 08:27:10,964 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 08:27:11,027 INFO     pid:22692 __main__:523:saveModel SHA1: ebe50fdd8fe7be76f174092db0d3ed68b8f3896e\n",
      "2023-12-11 08:27:56,852 INFO     pid:22692 __main__:191:main Epoch 61 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:27:56,867 WARNING  pid:22692 util:109:enumerateWithEstimate E61 Training ----/6754, starting\n",
      "2023-12-11 08:28:39,770 INFO     pid:22692 util:126:enumerateWithEstimate E61 Training   64/6754, done at 2023-12-11 08:35:39, 0:07:03\n",
      "2023-12-11 08:28:51,894 INFO     pid:22692 util:126:enumerateWithEstimate E61 Training  256/6754, done at 2023-12-11 08:35:41, 0:07:05\n",
      "2023-12-11 08:29:40,422 INFO     pid:22692 util:126:enumerateWithEstimate E61 Training 1024/6754, done at 2023-12-11 08:35:42, 0:07:05\n",
      "2023-12-11 08:32:53,027 INFO     pid:22692 util:126:enumerateWithEstimate E61 Training 4096/6754, done at 2023-12-11 08:35:39, 0:07:03\n",
      "2023-12-11 08:35:40,516 WARNING  pid:22692 util:139:enumerateWithEstimate E61 Training ----/6754, done at 2023-12-11 08:35:40\n",
      "2023-12-11 08:35:40,516 INFO     pid:22692 __main__:409:logMetrics E61 SegmentationTrainingApp\n",
      "2023-12-11 08:35:40,516 INFO     pid:22692 __main__:444:logMetrics E61 trn      0.2709 loss, 0.1747 fnloss, 0.9958 fploss, 0.8164 precision, 0.7586 recall, 0.7864 f1 score\n",
      "2023-12-11 08:35:40,516 INFO     pid:22692 __main__:458:logMetrics E61 trn_all  0.2709 loss,  75.9% tp,  24.1% fn,      17.1% fp\n",
      "2023-12-11 08:35:40,516 INFO     pid:22692 __main__:191:main Epoch 62 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:35:40,532 WARNING  pid:22692 util:109:enumerateWithEstimate E62 Training ----/6754, starting\n",
      "2023-12-11 08:36:23,622 INFO     pid:22692 util:126:enumerateWithEstimate E62 Training   64/6754, done at 2023-12-11 08:43:25, 0:07:05\n",
      "2023-12-11 08:36:35,824 INFO     pid:22692 util:126:enumerateWithEstimate E62 Training  256/6754, done at 2023-12-11 08:43:28, 0:07:07\n",
      "2023-12-11 08:37:23,884 INFO     pid:22692 util:126:enumerateWithEstimate E62 Training 1024/6754, done at 2023-12-11 08:43:23, 0:07:03\n",
      "2023-12-11 08:40:35,544 INFO     pid:22692 util:126:enumerateWithEstimate E62 Training 4096/6754, done at 2023-12-11 08:43:21, 0:07:01\n",
      "2023-12-11 08:43:22,975 WARNING  pid:22692 util:139:enumerateWithEstimate E62 Training ----/6754, done at 2023-12-11 08:43:22\n",
      "2023-12-11 08:43:22,990 INFO     pid:22692 __main__:409:logMetrics E62 SegmentationTrainingApp\n",
      "2023-12-11 08:43:22,990 INFO     pid:22692 __main__:444:logMetrics E62 trn      0.2614 loss, 0.1632 fnloss, 0.9959 fploss, 0.8198 precision, 0.7648 recall, 0.7914 f1 score\n",
      "2023-12-11 08:43:22,990 INFO     pid:22692 __main__:458:logMetrics E62 trn_all  0.2614 loss,  76.5% tp,  23.5% fn,      16.8% fp\n",
      "2023-12-11 08:43:22,990 INFO     pid:22692 __main__:191:main Epoch 63 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:43:22,990 WARNING  pid:22692 util:109:enumerateWithEstimate E63 Training ----/6754, starting\n",
      "2023-12-11 08:44:06,220 INFO     pid:22692 util:126:enumerateWithEstimate E63 Training   64/6754, done at 2023-12-11 08:51:04, 0:07:01\n",
      "2023-12-11 08:44:18,313 INFO     pid:22692 util:126:enumerateWithEstimate E63 Training  256/6754, done at 2023-12-11 08:51:06, 0:07:03\n",
      "2023-12-11 08:45:06,341 INFO     pid:22692 util:126:enumerateWithEstimate E63 Training 1024/6754, done at 2023-12-11 08:51:05, 0:07:01\n",
      "2023-12-11 08:48:18,078 INFO     pid:22692 util:126:enumerateWithEstimate E63 Training 4096/6754, done at 2023-12-11 08:51:04, 0:07:00\n",
      "2023-12-11 08:51:05,505 WARNING  pid:22692 util:139:enumerateWithEstimate E63 Training ----/6754, done at 2023-12-11 08:51:05\n",
      "2023-12-11 08:51:05,505 INFO     pid:22692 __main__:409:logMetrics E63 SegmentationTrainingApp\n",
      "2023-12-11 08:51:05,505 INFO     pid:22692 __main__:444:logMetrics E63 trn      0.2645 loss, 0.1586 fnloss, 0.9959 fploss, 0.8102 precision, 0.7740 recall, 0.7917 f1 score\n",
      "2023-12-11 08:51:05,505 INFO     pid:22692 __main__:458:logMetrics E63 trn_all  0.2645 loss,  77.4% tp,  22.6% fn,      18.1% fp\n",
      "2023-12-11 08:51:05,520 INFO     pid:22692 __main__:191:main Epoch 64 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:51:05,520 WARNING  pid:22692 util:109:enumerateWithEstimate E64 Training ----/6754, starting\n",
      "2023-12-11 08:51:48,266 INFO     pid:22692 util:126:enumerateWithEstimate E64 Training   64/6754, done at 2023-12-11 08:58:48, 0:07:03\n",
      "2023-12-11 08:52:00,358 INFO     pid:22692 util:126:enumerateWithEstimate E64 Training  256/6754, done at 2023-12-11 08:58:49, 0:07:04\n",
      "2023-12-11 08:52:48,665 INFO     pid:22692 util:126:enumerateWithEstimate E64 Training 1024/6754, done at 2023-12-11 08:58:49, 0:07:03\n",
      "2023-12-11 08:56:00,436 INFO     pid:22692 util:126:enumerateWithEstimate E64 Training 4096/6754, done at 2023-12-11 08:58:46, 0:07:01\n",
      "2023-12-11 08:58:47,896 WARNING  pid:22692 util:139:enumerateWithEstimate E64 Training ----/6754, done at 2023-12-11 08:58:47\n",
      "2023-12-11 08:58:47,896 INFO     pid:22692 __main__:409:logMetrics E64 SegmentationTrainingApp\n",
      "2023-12-11 08:58:47,896 INFO     pid:22692 __main__:444:logMetrics E64 trn      0.2650 loss, 0.1694 fnloss, 0.9957 fploss, 0.7950 precision, 0.7544 recall, 0.7742 f1 score\n",
      "2023-12-11 08:58:47,896 INFO     pid:22692 __main__:458:logMetrics E64 trn_all  0.2650 loss,  75.4% tp,  24.6% fn,      19.5% fp\n",
      "2023-12-11 08:58:47,896 INFO     pid:22692 __main__:191:main Epoch 65 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 08:58:47,896 WARNING  pid:22692 util:109:enumerateWithEstimate E65 Training ----/6754, starting\n",
      "2023-12-11 08:59:30,938 INFO     pid:22692 util:126:enumerateWithEstimate E65 Training   64/6754, done at 2023-12-11 09:06:33, 0:07:05\n",
      "2023-12-11 08:59:43,030 INFO     pid:22692 util:126:enumerateWithEstimate E65 Training  256/6754, done at 2023-12-11 09:06:32, 0:07:04\n",
      "2023-12-11 09:00:31,400 INFO     pid:22692 util:126:enumerateWithEstimate E65 Training 1024/6754, done at 2023-12-11 09:06:32, 0:07:04\n",
      "2023-12-11 09:03:43,236 INFO     pid:22692 util:126:enumerateWithEstimate E65 Training 4096/6754, done at 2023-12-11 09:06:29, 0:07:01\n",
      "2023-12-11 09:06:30,710 WARNING  pid:22692 util:139:enumerateWithEstimate E65 Training ----/6754, done at 2023-12-11 09:06:30\n",
      "2023-12-11 09:06:30,710 INFO     pid:22692 __main__:409:logMetrics E65 SegmentationTrainingApp\n",
      "2023-12-11 09:06:30,710 INFO     pid:22692 __main__:444:logMetrics E65 trn      0.2648 loss, 0.1635 fnloss, 0.9959 fploss, 0.8154 precision, 0.7666 recall, 0.7903 f1 score\n",
      "2023-12-11 09:06:30,710 INFO     pid:22692 __main__:458:logMetrics E65 trn_all  0.2648 loss,  76.7% tp,  23.3% fn,      17.4% fp\n",
      "2023-12-11 09:06:30,710 WARNING  pid:22692 util:109:enumerateWithEstimate E65 Validation  ----/862, starting\n",
      "2023-12-11 09:06:37,229 INFO     pid:22692 util:126:enumerateWithEstimate E65 Validation    64/862, done at 2023-12-11 09:06:51, 0:00:15\n",
      "2023-12-11 09:06:40,666 INFO     pid:22692 util:126:enumerateWithEstimate E65 Validation   256/862, done at 2023-12-11 09:06:51, 0:00:15\n",
      "2023-12-11 09:06:53,150 WARNING  pid:22692 util:139:enumerateWithEstimate E65 Validation  ----/862, done at 2023-12-11 09:06:53\n",
      "2023-12-11 09:06:53,150 INFO     pid:22692 __main__:409:logMetrics E65 SegmentationTrainingApp\n",
      "2023-12-11 09:06:53,150 INFO     pid:22692 __main__:444:logMetrics E65 val      0.4017 loss, 0.3529 fnloss, 0.9920 fploss, 0.8867 precision, 0.7103 recall, 0.7887 f1 score\n",
      "2023-12-11 09:06:53,150 INFO     pid:22692 __main__:458:logMetrics E65 val_all  0.4017 loss,  71.0% tp,  29.0% fn,       9.1% fp\n",
      "2023-12-11 09:06:53,259 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.878020.state\n",
      "2023-12-11 09:06:53,322 INFO     pid:22692 __main__:523:saveModel SHA1: 1be5eb881fc51d1bd9db0bdd0bfbdad517e8c862\n",
      "2023-12-11 09:07:38,687 INFO     pid:22692 __main__:191:main Epoch 66 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:07:38,687 WARNING  pid:22692 util:109:enumerateWithEstimate E66 Training ----/6754, starting\n",
      "2023-12-11 09:08:21,295 INFO     pid:22692 util:126:enumerateWithEstimate E66 Training   64/6754, done at 2023-12-11 09:15:19, 0:07:01\n",
      "2023-12-11 09:08:33,404 INFO     pid:22692 util:126:enumerateWithEstimate E66 Training  256/6754, done at 2023-12-11 09:15:22, 0:07:04\n",
      "2023-12-11 09:09:21,570 INFO     pid:22692 util:126:enumerateWithEstimate E66 Training 1024/6754, done at 2023-12-11 09:15:21, 0:07:02\n",
      "2023-12-11 09:12:33,348 INFO     pid:22692 util:126:enumerateWithEstimate E66 Training 4096/6754, done at 2023-12-11 09:15:19, 0:07:01\n",
      "2023-12-11 09:15:20,972 WARNING  pid:22692 util:139:enumerateWithEstimate E66 Training ----/6754, done at 2023-12-11 09:15:20\n",
      "2023-12-11 09:15:20,972 INFO     pid:22692 __main__:409:logMetrics E66 SegmentationTrainingApp\n",
      "2023-12-11 09:15:20,972 INFO     pid:22692 __main__:444:logMetrics E66 trn      0.2630 loss, 0.1588 fnloss, 0.9959 fploss, 0.8123 precision, 0.7697 recall, 0.7904 f1 score\n",
      "2023-12-11 09:15:20,972 INFO     pid:22692 __main__:458:logMetrics E66 trn_all  0.2630 loss,  77.0% tp,  23.0% fn,      17.8% fp\n",
      "2023-12-11 09:15:20,987 INFO     pid:22692 __main__:191:main Epoch 67 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:15:20,987 WARNING  pid:22692 util:109:enumerateWithEstimate E67 Training ----/6754, starting\n",
      "2023-12-11 09:16:04,014 INFO     pid:22692 util:126:enumerateWithEstimate E67 Training   64/6754, done at 2023-12-11 09:23:04, 0:07:03\n",
      "2023-12-11 09:16:16,107 INFO     pid:22692 util:126:enumerateWithEstimate E67 Training  256/6754, done at 2023-12-11 09:23:05, 0:07:04\n",
      "2023-12-11 09:17:04,305 INFO     pid:22692 util:126:enumerateWithEstimate E67 Training 1024/6754, done at 2023-12-11 09:23:04, 0:07:03\n",
      "2023-12-11 09:20:16,162 INFO     pid:22692 util:126:enumerateWithEstimate E67 Training 4096/6754, done at 2023-12-11 09:23:02, 0:07:01\n",
      "2023-12-11 09:23:03,598 WARNING  pid:22692 util:139:enumerateWithEstimate E67 Training ----/6754, done at 2023-12-11 09:23:03\n",
      "2023-12-11 09:23:03,613 INFO     pid:22692 __main__:409:logMetrics E67 SegmentationTrainingApp\n",
      "2023-12-11 09:23:03,613 INFO     pid:22692 __main__:444:logMetrics E67 trn      0.2652 loss, 0.1663 fnloss, 0.9959 fploss, 0.8181 precision, 0.7619 recall, 0.7890 f1 score\n",
      "2023-12-11 09:23:03,613 INFO     pid:22692 __main__:458:logMetrics E67 trn_all  0.2652 loss,  76.2% tp,  23.8% fn,      16.9% fp\n",
      "2023-12-11 09:23:03,613 INFO     pid:22692 __main__:191:main Epoch 68 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:23:03,613 WARNING  pid:22692 util:109:enumerateWithEstimate E68 Training ----/6754, starting\n",
      "2023-12-11 09:23:46,610 INFO     pid:22692 util:126:enumerateWithEstimate E68 Training   64/6754, done at 2023-12-11 09:30:46, 0:07:03\n",
      "2023-12-11 09:23:58,718 INFO     pid:22692 util:126:enumerateWithEstimate E68 Training  256/6754, done at 2023-12-11 09:30:48, 0:07:04\n",
      "2023-12-11 09:24:47,136 INFO     pid:22692 util:126:enumerateWithEstimate E68 Training 1024/6754, done at 2023-12-11 09:30:48, 0:07:04\n",
      "2023-12-11 09:28:01,414 INFO     pid:22692 util:126:enumerateWithEstimate E68 Training 4096/6754, done at 2023-12-11 09:30:49, 0:07:05\n",
      "2023-12-11 09:30:50,993 WARNING  pid:22692 util:139:enumerateWithEstimate E68 Training ----/6754, done at 2023-12-11 09:30:50\n",
      "2023-12-11 09:30:50,993 INFO     pid:22692 __main__:409:logMetrics E68 SegmentationTrainingApp\n",
      "2023-12-11 09:30:51,008 INFO     pid:22692 __main__:444:logMetrics E68 trn      0.2644 loss, 0.1663 fnloss, 0.9958 fploss, 0.8153 precision, 0.7672 recall, 0.7905 f1 score\n",
      "2023-12-11 09:30:51,008 INFO     pid:22692 __main__:458:logMetrics E68 trn_all  0.2644 loss,  76.7% tp,  23.3% fn,      17.4% fp\n",
      "2023-12-11 09:30:51,008 INFO     pid:22692 __main__:191:main Epoch 69 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:30:51,008 WARNING  pid:22692 util:109:enumerateWithEstimate E69 Training ----/6754, starting\n",
      "2023-12-11 09:31:33,895 INFO     pid:22692 util:126:enumerateWithEstimate E69 Training   64/6754, done at 2023-12-11 09:38:37, 0:07:07\n",
      "2023-12-11 09:31:45,993 INFO     pid:22692 util:126:enumerateWithEstimate E69 Training  256/6754, done at 2023-12-11 09:38:35, 0:07:05\n",
      "2023-12-11 09:32:34,364 INFO     pid:22692 util:126:enumerateWithEstimate E69 Training 1024/6754, done at 2023-12-11 09:38:35, 0:07:04\n",
      "2023-12-11 09:35:46,282 INFO     pid:22692 util:126:enumerateWithEstimate E69 Training 4096/6754, done at 2023-12-11 09:38:32, 0:07:01\n",
      "2023-12-11 09:38:33,924 WARNING  pid:22692 util:139:enumerateWithEstimate E69 Training ----/6754, done at 2023-12-11 09:38:33\n",
      "2023-12-11 09:38:33,924 INFO     pid:22692 __main__:409:logMetrics E69 SegmentationTrainingApp\n",
      "2023-12-11 09:38:33,924 INFO     pid:22692 __main__:444:logMetrics E69 trn      0.2627 loss, 0.1611 fnloss, 0.9958 fploss, 0.7471 precision, 0.7689 recall, 0.7578 f1 score\n",
      "2023-12-11 09:38:33,924 INFO     pid:22692 __main__:458:logMetrics E69 trn_all  0.2627 loss,  76.9% tp,  23.1% fn,      26.0% fp\n",
      "2023-12-11 09:38:33,924 INFO     pid:22692 __main__:191:main Epoch 70 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:38:33,939 WARNING  pid:22692 util:109:enumerateWithEstimate E70 Training ----/6754, starting\n",
      "2023-12-11 09:39:17,122 INFO     pid:22692 util:126:enumerateWithEstimate E70 Training   64/6754, done at 2023-12-11 09:46:17, 0:07:03\n",
      "2023-12-11 09:39:29,231 INFO     pid:22692 util:126:enumerateWithEstimate E70 Training  256/6754, done at 2023-12-11 09:46:18, 0:07:04\n",
      "2023-12-11 09:40:17,664 INFO     pid:22692 util:126:enumerateWithEstimate E70 Training 1024/6754, done at 2023-12-11 09:46:18, 0:07:04\n",
      "2023-12-11 09:43:29,473 INFO     pid:22692 util:126:enumerateWithEstimate E70 Training 4096/6754, done at 2023-12-11 09:46:15, 0:07:01\n",
      "2023-12-11 09:46:16,971 WARNING  pid:22692 util:139:enumerateWithEstimate E70 Training ----/6754, done at 2023-12-11 09:46:16\n",
      "2023-12-11 09:46:16,971 INFO     pid:22692 __main__:409:logMetrics E70 SegmentationTrainingApp\n",
      "2023-12-11 09:46:16,971 INFO     pid:22692 __main__:444:logMetrics E70 trn      0.2602 loss, 0.1691 fnloss, 0.9957 fploss, 0.7894 precision, 0.7665 recall, 0.7778 f1 score\n",
      "2023-12-11 09:46:16,971 INFO     pid:22692 __main__:458:logMetrics E70 trn_all  0.2602 loss,  76.6% tp,  23.4% fn,      20.5% fp\n",
      "2023-12-11 09:46:16,987 WARNING  pid:22692 util:109:enumerateWithEstimate E70 Validation  ----/862, starting\n",
      "2023-12-11 09:46:23,475 INFO     pid:22692 util:126:enumerateWithEstimate E70 Validation    64/862, done at 2023-12-11 09:46:37, 0:00:15\n",
      "2023-12-11 09:46:26,928 INFO     pid:22692 util:126:enumerateWithEstimate E70 Validation   256/862, done at 2023-12-11 09:46:37, 0:00:15\n",
      "2023-12-11 09:46:39,302 WARNING  pid:22692 util:139:enumerateWithEstimate E70 Validation  ----/862, done at 2023-12-11 09:46:39\n",
      "2023-12-11 09:46:39,317 INFO     pid:22692 __main__:409:logMetrics E70 SegmentationTrainingApp\n",
      "2023-12-11 09:46:39,317 INFO     pid:22692 __main__:444:logMetrics E70 val      0.3959 loss, 0.3343 fnloss, 0.9921 fploss, 0.8373 precision, 0.7343 recall, 0.7824 f1 score\n",
      "2023-12-11 09:46:39,317 INFO     pid:22692 __main__:458:logMetrics E70 val_all  0.3959 loss,  73.4% tp,  26.6% fn,      14.3% fp\n",
      "2023-12-11 09:46:39,442 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.945560.state\n",
      "2023-12-11 09:46:39,505 INFO     pid:22692 __main__:523:saveModel SHA1: f802d8815e17a5d2b0b528fb3ccfc06611131b6e\n",
      "2023-12-11 09:47:25,798 INFO     pid:22692 __main__:191:main Epoch 71 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:47:25,798 WARNING  pid:22692 util:109:enumerateWithEstimate E71 Training ----/6754, starting\n",
      "2023-12-11 09:48:08,528 INFO     pid:22692 util:126:enumerateWithEstimate E71 Training   64/6754, done at 2023-12-11 09:55:06, 0:07:01\n",
      "2023-12-11 09:48:20,683 INFO     pid:22692 util:126:enumerateWithEstimate E71 Training  256/6754, done at 2023-12-11 09:55:10, 0:07:05\n",
      "2023-12-11 09:49:09,241 INFO     pid:22692 util:126:enumerateWithEstimate E71 Training 1024/6754, done at 2023-12-11 09:55:11, 0:07:05\n",
      "2023-12-11 09:52:23,502 INFO     pid:22692 util:126:enumerateWithEstimate E71 Training 4096/6754, done at 2023-12-11 09:55:11, 0:07:06\n",
      "2023-12-11 09:55:13,203 WARNING  pid:22692 util:139:enumerateWithEstimate E71 Training ----/6754, done at 2023-12-11 09:55:13\n",
      "2023-12-11 09:55:13,203 INFO     pid:22692 __main__:409:logMetrics E71 SegmentationTrainingApp\n",
      "2023-12-11 09:55:13,203 INFO     pid:22692 __main__:444:logMetrics E71 trn      0.2591 loss, 0.1660 fnloss, 0.9957 fploss, 0.8236 precision, 0.7655 recall, 0.7935 f1 score\n",
      "2023-12-11 09:55:13,203 INFO     pid:22692 __main__:458:logMetrics E71 trn_all  0.2591 loss,  76.6% tp,  23.4% fn,      16.4% fp\n",
      "2023-12-11 09:55:13,203 INFO     pid:22692 __main__:191:main Epoch 72 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 09:55:13,219 WARNING  pid:22692 util:109:enumerateWithEstimate E72 Training ----/6754, starting\n",
      "2023-12-11 09:55:56,390 INFO     pid:22692 util:126:enumerateWithEstimate E72 Training   64/6754, done at 2023-12-11 10:02:56, 0:07:03\n",
      "2023-12-11 09:56:08,499 INFO     pid:22692 util:126:enumerateWithEstimate E72 Training  256/6754, done at 2023-12-11 10:02:57, 0:07:04\n",
      "2023-12-11 09:56:57,088 INFO     pid:22692 util:126:enumerateWithEstimate E72 Training 1024/6754, done at 2023-12-11 10:02:59, 0:07:05\n",
      "2023-12-11 10:00:09,240 INFO     pid:22692 util:126:enumerateWithEstimate E72 Training 4096/6754, done at 2023-12-11 10:02:55, 0:07:02\n",
      "2023-12-11 10:02:56,678 WARNING  pid:22692 util:139:enumerateWithEstimate E72 Training ----/6754, done at 2023-12-11 10:02:56\n",
      "2023-12-11 10:02:56,678 INFO     pid:22692 __main__:409:logMetrics E72 SegmentationTrainingApp\n",
      "2023-12-11 10:02:56,678 INFO     pid:22692 __main__:444:logMetrics E72 trn      0.2618 loss, 0.1643 fnloss, 0.9958 fploss, 0.8188 precision, 0.7654 recall, 0.7912 f1 score\n",
      "2023-12-11 10:02:56,694 INFO     pid:22692 __main__:458:logMetrics E72 trn_all  0.2618 loss,  76.5% tp,  23.5% fn,      16.9% fp\n",
      "2023-12-11 10:02:56,694 INFO     pid:22692 __main__:191:main Epoch 73 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:02:56,694 WARNING  pid:22692 util:109:enumerateWithEstimate E73 Training ----/6754, starting\n",
      "2023-12-11 10:03:39,658 INFO     pid:22692 util:126:enumerateWithEstimate E73 Training   64/6754, done at 2023-12-11 10:10:44, 0:07:07\n",
      "2023-12-11 10:03:51,751 INFO     pid:22692 util:126:enumerateWithEstimate E73 Training  256/6754, done at 2023-12-11 10:10:41, 0:07:05\n",
      "2023-12-11 10:04:40,074 INFO     pid:22692 util:126:enumerateWithEstimate E73 Training 1024/6754, done at 2023-12-11 10:10:40, 0:07:04\n",
      "2023-12-11 10:07:51,882 INFO     pid:22692 util:126:enumerateWithEstimate E73 Training 4096/6754, done at 2023-12-11 10:10:38, 0:07:01\n",
      "2023-12-11 10:10:39,516 WARNING  pid:22692 util:139:enumerateWithEstimate E73 Training ----/6754, done at 2023-12-11 10:10:39\n",
      "2023-12-11 10:10:39,521 INFO     pid:22692 __main__:409:logMetrics E73 SegmentationTrainingApp\n",
      "2023-12-11 10:10:39,521 INFO     pid:22692 __main__:444:logMetrics E73 trn      0.2613 loss, 0.1620 fnloss, 0.9958 fploss, 0.8155 precision, 0.7693 recall, 0.7917 f1 score\n",
      "2023-12-11 10:10:39,521 INFO     pid:22692 __main__:458:logMetrics E73 trn_all  0.2613 loss,  76.9% tp,  23.1% fn,      17.4% fp\n",
      "2023-12-11 10:10:39,521 INFO     pid:22692 __main__:191:main Epoch 74 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:10:39,521 WARNING  pid:22692 util:109:enumerateWithEstimate E74 Training ----/6754, starting\n",
      "2023-12-11 10:11:22,876 INFO     pid:22692 util:126:enumerateWithEstimate E74 Training   64/6754, done at 2023-12-11 10:18:23, 0:07:03\n",
      "2023-12-11 10:11:34,984 INFO     pid:22692 util:126:enumerateWithEstimate E74 Training  256/6754, done at 2023-12-11 10:18:24, 0:07:04\n",
      "2023-12-11 10:12:23,417 INFO     pid:22692 util:126:enumerateWithEstimate E74 Training 1024/6754, done at 2023-12-11 10:18:24, 0:07:04\n",
      "2023-12-11 10:15:35,445 INFO     pid:22692 util:126:enumerateWithEstimate E74 Training 4096/6754, done at 2023-12-11 10:18:21, 0:07:02\n",
      "2023-12-11 10:18:23,020 WARNING  pid:22692 util:139:enumerateWithEstimate E74 Training ----/6754, done at 2023-12-11 10:18:23\n",
      "2023-12-11 10:18:23,020 INFO     pid:22692 __main__:409:logMetrics E74 SegmentationTrainingApp\n",
      "2023-12-11 10:18:23,020 INFO     pid:22692 __main__:444:logMetrics E74 trn      0.2555 loss, 0.1540 fnloss, 0.9958 fploss, 0.8165 precision, 0.7813 recall, 0.7985 f1 score\n",
      "2023-12-11 10:18:23,020 INFO     pid:22692 __main__:458:logMetrics E74 trn_all  0.2555 loss,  78.1% tp,  21.9% fn,      17.6% fp\n",
      "2023-12-11 10:18:23,020 INFO     pid:22692 __main__:191:main Epoch 75 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:18:23,036 WARNING  pid:22692 util:109:enumerateWithEstimate E75 Training ----/6754, starting\n",
      "2023-12-11 10:19:06,127 INFO     pid:22692 util:126:enumerateWithEstimate E75 Training   64/6754, done at 2023-12-11 10:26:08, 0:07:05\n",
      "2023-12-11 10:19:18,204 INFO     pid:22692 util:126:enumerateWithEstimate E75 Training  256/6754, done at 2023-12-11 10:26:07, 0:07:04\n",
      "2023-12-11 10:20:06,653 INFO     pid:22692 util:126:enumerateWithEstimate E75 Training 1024/6754, done at 2023-12-11 10:26:07, 0:07:04\n",
      "2023-12-11 10:23:18,571 INFO     pid:22692 util:126:enumerateWithEstimate E75 Training 4096/6754, done at 2023-12-11 10:26:04, 0:07:01\n",
      "2023-12-11 10:26:06,226 WARNING  pid:22692 util:139:enumerateWithEstimate E75 Training ----/6754, done at 2023-12-11 10:26:06\n",
      "2023-12-11 10:26:06,226 INFO     pid:22692 __main__:409:logMetrics E75 SegmentationTrainingApp\n",
      "2023-12-11 10:26:06,226 INFO     pid:22692 __main__:444:logMetrics E75 trn      0.2560 loss, 0.1525 fnloss, 0.9959 fploss, 0.8177 precision, 0.7807 recall, 0.7988 f1 score\n",
      "2023-12-11 10:26:06,226 INFO     pid:22692 __main__:458:logMetrics E75 trn_all  0.2560 loss,  78.1% tp,  21.9% fn,      17.4% fp\n",
      "2023-12-11 10:26:06,226 WARNING  pid:22692 util:109:enumerateWithEstimate E75 Validation  ----/862, starting\n",
      "2023-12-11 10:26:12,602 INFO     pid:22692 util:126:enumerateWithEstimate E75 Validation    64/862, done at 2023-12-11 10:26:26, 0:00:15\n",
      "2023-12-11 10:26:16,054 INFO     pid:22692 util:126:enumerateWithEstimate E75 Validation   256/862, done at 2023-12-11 10:26:26, 0:00:15\n",
      "2023-12-11 10:26:28,428 WARNING  pid:22692 util:139:enumerateWithEstimate E75 Validation  ----/862, done at 2023-12-11 10:26:28\n",
      "2023-12-11 10:26:28,428 INFO     pid:22692 __main__:409:logMetrics E75 SegmentationTrainingApp\n",
      "2023-12-11 10:26:28,428 INFO     pid:22692 __main__:444:logMetrics E75 val      0.3567 loss, 0.2552 fnloss, 0.9927 fploss, 0.7923 precision, 0.8295 recall, 0.8105 f1 score\n",
      "2023-12-11 10:26:28,428 INFO     pid:22692 __main__:458:logMetrics E75 val_all  0.3567 loss,  83.0% tp,  17.0% fn,      21.8% fp\n",
      "2023-12-11 10:26:28,553 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1013100.state\n",
      "2023-12-11 10:26:28,600 INFO     pid:22692 __main__:520:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.best.state\n",
      "2023-12-11 10:26:28,663 INFO     pid:22692 __main__:523:saveModel SHA1: c1331354196458b14adb90dd3c442b02063e13c0\n",
      "2023-12-11 10:27:14,799 INFO     pid:22692 __main__:191:main Epoch 76 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:27:14,799 WARNING  pid:22692 util:109:enumerateWithEstimate E76 Training ----/6754, starting\n",
      "2023-12-11 10:27:57,747 INFO     pid:22692 util:126:enumerateWithEstimate E76 Training   64/6754, done at 2023-12-11 10:35:00, 0:07:05\n",
      "2023-12-11 10:28:09,871 INFO     pid:22692 util:126:enumerateWithEstimate E76 Training  256/6754, done at 2023-12-11 10:35:00, 0:07:05\n",
      "2023-12-11 10:28:58,397 INFO     pid:22692 util:126:enumerateWithEstimate E76 Training 1024/6754, done at 2023-12-11 10:35:00, 0:07:05\n",
      "2023-12-11 10:32:10,700 INFO     pid:22692 util:126:enumerateWithEstimate E76 Training 4096/6754, done at 2023-12-11 10:34:57, 0:07:02\n",
      "2023-12-11 10:34:58,245 WARNING  pid:22692 util:139:enumerateWithEstimate E76 Training ----/6754, done at 2023-12-11 10:34:58\n",
      "2023-12-11 10:34:58,245 INFO     pid:22692 __main__:409:logMetrics E76 SegmentationTrainingApp\n",
      "2023-12-11 10:34:58,261 INFO     pid:22692 __main__:444:logMetrics E76 trn      0.2569 loss, 0.1585 fnloss, 0.9958 fploss, 0.8236 precision, 0.7743 recall, 0.7982 f1 score\n",
      "2023-12-11 10:34:58,261 INFO     pid:22692 __main__:458:logMetrics E76 trn_all  0.2569 loss,  77.4% tp,  22.6% fn,      16.6% fp\n",
      "2023-12-11 10:34:58,261 INFO     pid:22692 __main__:191:main Epoch 77 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:34:58,261 WARNING  pid:22692 util:109:enumerateWithEstimate E77 Training ----/6754, starting\n",
      "2023-12-11 10:35:41,335 INFO     pid:22692 util:126:enumerateWithEstimate E77 Training   64/6754, done at 2023-12-11 10:42:41, 0:07:03\n",
      "2023-12-11 10:35:53,444 INFO     pid:22692 util:126:enumerateWithEstimate E77 Training  256/6754, done at 2023-12-11 10:42:42, 0:07:04\n",
      "2023-12-11 10:36:41,860 INFO     pid:22692 util:126:enumerateWithEstimate E77 Training 1024/6754, done at 2023-12-11 10:42:42, 0:07:04\n",
      "2023-12-11 10:39:53,996 INFO     pid:22692 util:126:enumerateWithEstimate E77 Training 4096/6754, done at 2023-12-11 10:42:40, 0:07:02\n",
      "2023-12-11 10:42:41,526 WARNING  pid:22692 util:139:enumerateWithEstimate E77 Training ----/6754, done at 2023-12-11 10:42:41\n",
      "2023-12-11 10:42:41,526 INFO     pid:22692 __main__:409:logMetrics E77 SegmentationTrainingApp\n",
      "2023-12-11 10:42:41,526 INFO     pid:22692 __main__:444:logMetrics E77 trn      0.2580 loss, 0.1606 fnloss, 0.9959 fploss, 0.8219 precision, 0.7666 recall, 0.7933 f1 score\n",
      "2023-12-11 10:42:41,526 INFO     pid:22692 __main__:458:logMetrics E77 trn_all  0.2580 loss,  76.7% tp,  23.3% fn,      16.6% fp\n",
      "2023-12-11 10:42:41,526 INFO     pid:22692 __main__:191:main Epoch 78 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:42:41,541 WARNING  pid:22692 util:109:enumerateWithEstimate E78 Training ----/6754, starting\n",
      "2023-12-11 10:43:24,761 INFO     pid:22692 util:126:enumerateWithEstimate E78 Training   64/6754, done at 2023-12-11 10:50:22, 0:07:01\n",
      "2023-12-11 10:43:36,900 INFO     pid:22692 util:126:enumerateWithEstimate E78 Training  256/6754, done at 2023-12-11 10:50:26, 0:07:05\n",
      "2023-12-11 10:44:25,285 INFO     pid:22692 util:126:enumerateWithEstimate E78 Training 1024/6754, done at 2023-12-11 10:50:26, 0:07:04\n",
      "2023-12-11 10:47:37,094 INFO     pid:22692 util:126:enumerateWithEstimate E78 Training 4096/6754, done at 2023-12-11 10:50:23, 0:07:01\n",
      "2023-12-11 10:50:24,673 WARNING  pid:22692 util:139:enumerateWithEstimate E78 Training ----/6754, done at 2023-12-11 10:50:24\n",
      "2023-12-11 10:50:24,673 INFO     pid:22692 __main__:409:logMetrics E78 SegmentationTrainingApp\n",
      "2023-12-11 10:50:24,688 INFO     pid:22692 __main__:444:logMetrics E78 trn      0.2587 loss, 0.1588 fnloss, 0.9958 fploss, 0.8213 precision, 0.7758 recall, 0.7979 f1 score\n",
      "2023-12-11 10:50:24,688 INFO     pid:22692 __main__:458:logMetrics E78 trn_all  0.2587 loss,  77.6% tp,  22.4% fn,      16.9% fp\n",
      "2023-12-11 10:50:24,688 INFO     pid:22692 __main__:191:main Epoch 79 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:50:24,688 WARNING  pid:22692 util:109:enumerateWithEstimate E79 Training ----/6754, starting\n",
      "2023-12-11 10:51:07,668 INFO     pid:22692 util:126:enumerateWithEstimate E79 Training   64/6754, done at 2023-12-11 10:58:09, 0:07:05\n",
      "2023-12-11 10:51:19,760 INFO     pid:22692 util:126:enumerateWithEstimate E79 Training  256/6754, done at 2023-12-11 10:58:09, 0:07:04\n",
      "2023-12-11 10:52:08,211 INFO     pid:22692 util:126:enumerateWithEstimate E79 Training 1024/6754, done at 2023-12-11 10:58:09, 0:07:04\n",
      "2023-12-11 10:55:20,081 INFO     pid:22692 util:126:enumerateWithEstimate E79 Training 4096/6754, done at 2023-12-11 10:58:06, 0:07:01\n",
      "2023-12-11 10:58:07,596 WARNING  pid:22692 util:139:enumerateWithEstimate E79 Training ----/6754, done at 2023-12-11 10:58:07\n",
      "2023-12-11 10:58:07,612 INFO     pid:22692 __main__:409:logMetrics E79 SegmentationTrainingApp\n",
      "2023-12-11 10:58:07,612 INFO     pid:22692 __main__:444:logMetrics E79 trn      0.2532 loss, 0.1537 fnloss, 0.9957 fploss, 0.8224 precision, 0.7789 recall, 0.8001 f1 score\n",
      "2023-12-11 10:58:07,612 INFO     pid:22692 __main__:458:logMetrics E79 trn_all  0.2532 loss,  77.9% tp,  22.1% fn,      16.8% fp\n",
      "2023-12-11 10:58:07,612 INFO     pid:22692 __main__:191:main Epoch 80 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 10:58:07,612 WARNING  pid:22692 util:109:enumerateWithEstimate E80 Training ----/6754, starting\n",
      "2023-12-11 10:58:50,717 INFO     pid:22692 util:126:enumerateWithEstimate E80 Training   64/6754, done at 2023-12-11 11:05:50, 0:07:03\n",
      "2023-12-11 10:59:02,794 INFO     pid:22692 util:126:enumerateWithEstimate E80 Training  256/6754, done at 2023-12-11 11:05:51, 0:07:03\n",
      "2023-12-11 10:59:51,210 INFO     pid:22692 util:126:enumerateWithEstimate E80 Training 1024/6754, done at 2023-12-11 11:05:52, 0:07:04\n",
      "2023-12-11 11:03:03,035 INFO     pid:22692 util:126:enumerateWithEstimate E80 Training 4096/6754, done at 2023-12-11 11:05:49, 0:07:01\n",
      "2023-12-11 11:05:50,534 WARNING  pid:22692 util:139:enumerateWithEstimate E80 Training ----/6754, done at 2023-12-11 11:05:50\n",
      "2023-12-11 11:05:50,534 INFO     pid:22692 __main__:409:logMetrics E80 SegmentationTrainingApp\n",
      "2023-12-11 11:05:50,534 INFO     pid:22692 __main__:444:logMetrics E80 trn      0.2552 loss, 0.1535 fnloss, 0.9958 fploss, 0.8216 precision, 0.7783 recall, 0.7993 f1 score\n",
      "2023-12-11 11:05:50,534 INFO     pid:22692 __main__:458:logMetrics E80 trn_all  0.2552 loss,  77.8% tp,  22.2% fn,      16.9% fp\n",
      "2023-12-11 11:05:50,534 WARNING  pid:22692 util:109:enumerateWithEstimate E80 Validation  ----/862, starting\n",
      "2023-12-11 11:05:56,805 INFO     pid:22692 util:126:enumerateWithEstimate E80 Validation    64/862, done at 2023-12-11 11:06:11, 0:00:15\n",
      "2023-12-11 11:06:00,258 INFO     pid:22692 util:126:enumerateWithEstimate E80 Validation   256/862, done at 2023-12-11 11:06:11, 0:00:15\n",
      "2023-12-11 11:06:12,601 WARNING  pid:22692 util:139:enumerateWithEstimate E80 Validation  ----/862, done at 2023-12-11 11:06:12\n",
      "2023-12-11 11:06:12,601 INFO     pid:22692 __main__:409:logMetrics E80 SegmentationTrainingApp\n",
      "2023-12-11 11:06:12,601 INFO     pid:22692 __main__:444:logMetrics E80 val      0.3634 loss, 0.2995 fnloss, 0.9923 fploss, 0.8564 precision, 0.7507 recall, 0.8001 f1 score\n",
      "2023-12-11 11:06:12,601 INFO     pid:22692 __main__:458:logMetrics E80 val_all  0.3634 loss,  75.1% tp,  24.9% fn,      12.6% fp\n",
      "2023-12-11 11:06:12,726 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1080640.state\n",
      "2023-12-11 11:06:12,789 INFO     pid:22692 __main__:523:saveModel SHA1: c5c3c7ba97712cba3b6bb3b06b290d51ee7e7249\n",
      "2023-12-11 11:06:59,475 INFO     pid:22692 __main__:191:main Epoch 81 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:06:59,475 WARNING  pid:22692 util:109:enumerateWithEstimate E81 Training ----/6754, starting\n",
      "2023-12-11 11:07:42,039 INFO     pid:22692 util:126:enumerateWithEstimate E81 Training   64/6754, done at 2023-12-11 11:14:37, 0:06:58\n",
      "2023-12-11 11:07:54,163 INFO     pid:22692 util:126:enumerateWithEstimate E81 Training  256/6754, done at 2023-12-11 11:14:43, 0:07:04\n",
      "2023-12-11 11:08:42,689 INFO     pid:22692 util:126:enumerateWithEstimate E81 Training 1024/6754, done at 2023-12-11 11:14:44, 0:07:05\n",
      "2023-12-11 11:11:56,951 INFO     pid:22692 util:126:enumerateWithEstimate E81 Training 4096/6754, done at 2023-12-11 11:14:44, 0:07:05\n",
      "2023-12-11 11:14:46,592 WARNING  pid:22692 util:139:enumerateWithEstimate E81 Training ----/6754, done at 2023-12-11 11:14:46\n",
      "2023-12-11 11:14:46,607 INFO     pid:22692 __main__:409:logMetrics E81 SegmentationTrainingApp\n",
      "2023-12-11 11:14:46,607 INFO     pid:22692 __main__:444:logMetrics E81 trn      0.2556 loss, 0.1578 fnloss, 0.9957 fploss, 0.8251 precision, 0.7768 recall, 0.8002 f1 score\n",
      "2023-12-11 11:14:46,607 INFO     pid:22692 __main__:458:logMetrics E81 trn_all  0.2556 loss,  77.7% tp,  22.3% fn,      16.5% fp\n",
      "2023-12-11 11:14:46,607 INFO     pid:22692 __main__:191:main Epoch 82 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:14:46,607 WARNING  pid:22692 util:109:enumerateWithEstimate E82 Training ----/6754, starting\n",
      "2023-12-11 11:15:29,556 INFO     pid:22692 util:126:enumerateWithEstimate E82 Training   64/6754, done at 2023-12-11 11:22:29, 0:07:03\n",
      "2023-12-11 11:15:41,649 INFO     pid:22692 util:126:enumerateWithEstimate E82 Training  256/6754, done at 2023-12-11 11:22:30, 0:07:04\n",
      "2023-12-11 11:16:29,832 INFO     pid:22692 util:126:enumerateWithEstimate E82 Training 1024/6754, done at 2023-12-11 11:22:29, 0:07:03\n",
      "2023-12-11 11:19:41,735 INFO     pid:22692 util:126:enumerateWithEstimate E82 Training 4096/6754, done at 2023-12-11 11:22:27, 0:07:01\n",
      "2023-12-11 11:22:29,655 WARNING  pid:22692 util:139:enumerateWithEstimate E82 Training ----/6754, done at 2023-12-11 11:22:29\n",
      "2023-12-11 11:22:29,655 INFO     pid:22692 __main__:409:logMetrics E82 SegmentationTrainingApp\n",
      "2023-12-11 11:22:29,655 INFO     pid:22692 __main__:444:logMetrics E82 trn      0.2539 loss, 0.1579 fnloss, 0.9957 fploss, 0.8248 precision, 0.7788 recall, 0.8011 f1 score\n",
      "2023-12-11 11:22:29,655 INFO     pid:22692 __main__:458:logMetrics E82 trn_all  0.2539 loss,  77.9% tp,  22.1% fn,      16.5% fp\n",
      "2023-12-11 11:22:29,655 INFO     pid:22692 __main__:191:main Epoch 83 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:22:29,671 WARNING  pid:22692 util:109:enumerateWithEstimate E83 Training ----/6754, starting\n",
      "2023-12-11 11:23:12,823 INFO     pid:22692 util:126:enumerateWithEstimate E83 Training   64/6754, done at 2023-12-11 11:30:15, 0:07:05\n",
      "2023-12-11 11:23:24,916 INFO     pid:22692 util:126:enumerateWithEstimate E83 Training  256/6754, done at 2023-12-11 11:30:14, 0:07:04\n",
      "2023-12-11 11:24:13,146 INFO     pid:22692 util:126:enumerateWithEstimate E83 Training 1024/6754, done at 2023-12-11 11:30:13, 0:07:03\n",
      "2023-12-11 11:27:25,111 INFO     pid:22692 util:126:enumerateWithEstimate E83 Training 4096/6754, done at 2023-12-11 11:30:11, 0:07:01\n",
      "2023-12-11 11:30:12,547 WARNING  pid:22692 util:139:enumerateWithEstimate E83 Training ----/6754, done at 2023-12-11 11:30:12\n",
      "2023-12-11 11:30:12,547 INFO     pid:22692 __main__:409:logMetrics E83 SegmentationTrainingApp\n",
      "2023-12-11 11:30:12,547 INFO     pid:22692 __main__:444:logMetrics E83 trn      0.2552 loss, 0.1595 fnloss, 0.9957 fploss, 0.8300 precision, 0.7708 recall, 0.7993 f1 score\n",
      "2023-12-11 11:30:12,547 INFO     pid:22692 __main__:458:logMetrics E83 trn_all  0.2552 loss,  77.1% tp,  22.9% fn,      15.8% fp\n",
      "2023-12-11 11:30:12,547 INFO     pid:22692 __main__:191:main Epoch 84 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:30:12,563 WARNING  pid:22692 util:109:enumerateWithEstimate E84 Training ----/6754, starting\n",
      "2023-12-11 11:30:55,435 INFO     pid:22692 util:126:enumerateWithEstimate E84 Training   64/6754, done at 2023-12-11 11:37:53, 0:07:01\n",
      "2023-12-11 11:31:07,528 INFO     pid:22692 util:126:enumerateWithEstimate E84 Training  256/6754, done at 2023-12-11 11:37:56, 0:07:03\n",
      "2023-12-11 11:31:55,716 INFO     pid:22692 util:126:enumerateWithEstimate E84 Training 1024/6754, done at 2023-12-11 11:37:55, 0:07:02\n",
      "2023-12-11 11:35:07,603 INFO     pid:22692 util:126:enumerateWithEstimate E84 Training 4096/6754, done at 2023-12-11 11:37:53, 0:07:01\n",
      "2023-12-11 11:37:55,274 WARNING  pid:22692 util:139:enumerateWithEstimate E84 Training ----/6754, done at 2023-12-11 11:37:55\n",
      "2023-12-11 11:37:55,274 INFO     pid:22692 __main__:409:logMetrics E84 SegmentationTrainingApp\n",
      "2023-12-11 11:37:55,274 INFO     pid:22692 __main__:444:logMetrics E84 trn      0.2497 loss, 0.1459 fnloss, 0.9958 fploss, 0.8163 precision, 0.7879 recall, 0.8018 f1 score\n",
      "2023-12-11 11:37:55,274 INFO     pid:22692 __main__:458:logMetrics E84 trn_all  0.2497 loss,  78.8% tp,  21.2% fn,      17.7% fp\n",
      "2023-12-11 11:37:55,290 INFO     pid:22692 __main__:191:main Epoch 85 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:37:55,290 WARNING  pid:22692 util:109:enumerateWithEstimate E85 Training ----/6754, starting\n",
      "2023-12-11 11:38:38,459 INFO     pid:22692 util:126:enumerateWithEstimate E85 Training   64/6754, done at 2023-12-11 11:45:40, 0:07:05\n",
      "2023-12-11 11:38:50,536 INFO     pid:22692 util:126:enumerateWithEstimate E85 Training  256/6754, done at 2023-12-11 11:45:39, 0:07:04\n",
      "2023-12-11 11:39:38,968 INFO     pid:22692 util:126:enumerateWithEstimate E85 Training 1024/6754, done at 2023-12-11 11:45:40, 0:07:04\n",
      "2023-12-11 11:42:51,027 INFO     pid:22692 util:126:enumerateWithEstimate E85 Training 4096/6754, done at 2023-12-11 11:45:37, 0:07:02\n",
      "2023-12-11 11:45:38,480 WARNING  pid:22692 util:139:enumerateWithEstimate E85 Training ----/6754, done at 2023-12-11 11:45:38\n",
      "2023-12-11 11:45:38,480 INFO     pid:22692 __main__:409:logMetrics E85 SegmentationTrainingApp\n",
      "2023-12-11 11:45:38,480 INFO     pid:22692 __main__:444:logMetrics E85 trn      0.2574 loss, 0.1575 fnloss, 0.9958 fploss, 0.8197 precision, 0.7751 recall, 0.7968 f1 score\n",
      "2023-12-11 11:45:38,480 INFO     pid:22692 __main__:458:logMetrics E85 trn_all  0.2574 loss,  77.5% tp,  22.5% fn,      17.0% fp\n",
      "2023-12-11 11:45:38,480 WARNING  pid:22692 util:109:enumerateWithEstimate E85 Validation  ----/862, starting\n",
      "2023-12-11 11:45:44,702 INFO     pid:22692 util:126:enumerateWithEstimate E85 Validation    64/862, done at 2023-12-11 11:45:59, 0:00:15\n",
      "2023-12-11 11:45:48,139 INFO     pid:22692 util:126:enumerateWithEstimate E85 Validation   256/862, done at 2023-12-11 11:45:59, 0:00:15\n",
      "2023-12-11 11:46:00,622 WARNING  pid:22692 util:139:enumerateWithEstimate E85 Validation  ----/862, done at 2023-12-11 11:46:00\n",
      "2023-12-11 11:46:00,622 INFO     pid:22692 __main__:409:logMetrics E85 SegmentationTrainingApp\n",
      "2023-12-11 11:46:00,622 INFO     pid:22692 __main__:444:logMetrics E85 val      0.3510 loss, 0.2766 fnloss, 0.9922 fploss, 0.6885 precision, 0.7650 recall, 0.7247 f1 score\n",
      "2023-12-11 11:46:00,622 INFO     pid:22692 __main__:458:logMetrics E85 val_all  0.3510 loss,  76.5% tp,  23.5% fn,      34.6% fp\n",
      "2023-12-11 11:46:00,731 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1148180.state\n",
      "2023-12-11 11:46:00,794 INFO     pid:22692 __main__:523:saveModel SHA1: e014bd07407e5870dab569f490ece79282b2b405\n",
      "2023-12-11 11:46:47,820 INFO     pid:22692 __main__:191:main Epoch 86 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:46:47,820 WARNING  pid:22692 util:109:enumerateWithEstimate E86 Training ----/6754, starting\n",
      "2023-12-11 11:47:30,504 INFO     pid:22692 util:126:enumerateWithEstimate E86 Training   64/6754, done at 2023-12-11 11:54:28, 0:07:01\n",
      "2023-12-11 11:47:42,596 INFO     pid:22692 util:126:enumerateWithEstimate E86 Training  256/6754, done at 2023-12-11 11:54:31, 0:07:03\n",
      "2023-12-11 11:48:31,247 INFO     pid:22692 util:126:enumerateWithEstimate E86 Training 1024/6754, done at 2023-12-11 11:54:33, 0:07:06\n",
      "2023-12-11 11:51:45,022 INFO     pid:22692 util:126:enumerateWithEstimate E86 Training 4096/6754, done at 2023-12-11 11:54:32, 0:07:05\n",
      "2023-12-11 11:54:47,010 WARNING  pid:22692 util:139:enumerateWithEstimate E86 Training ----/6754, done at 2023-12-11 11:54:47\n",
      "2023-12-11 11:54:47,010 INFO     pid:22692 __main__:409:logMetrics E86 SegmentationTrainingApp\n",
      "2023-12-11 11:54:47,010 INFO     pid:22692 __main__:444:logMetrics E86 trn      0.2514 loss, 0.1490 fnloss, 0.9959 fploss, 0.8245 precision, 0.7835 recall, 0.8035 f1 score\n",
      "2023-12-11 11:54:47,010 INFO     pid:22692 __main__:458:logMetrics E86 trn_all  0.2514 loss,  78.4% tp,  21.6% fn,      16.7% fp\n",
      "2023-12-11 11:54:47,020 INFO     pid:22692 __main__:191:main Epoch 87 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 11:54:47,026 WARNING  pid:22692 util:109:enumerateWithEstimate E87 Training ----/6754, starting\n",
      "2023-12-11 11:55:31,736 INFO     pid:22692 util:126:enumerateWithEstimate E87 Training   64/6754, done at 2023-12-11 12:03:26, 0:07:58\n",
      "2023-12-11 11:55:45,308 INFO     pid:22692 util:126:enumerateWithEstimate E87 Training  256/6754, done at 2023-12-11 12:03:24, 0:07:56\n",
      "2023-12-11 11:56:39,135 INFO     pid:22692 util:126:enumerateWithEstimate E87 Training 1024/6754, done at 2023-12-11 12:03:21, 0:07:53\n",
      "2023-12-11 11:59:56,655 INFO     pid:22692 util:126:enumerateWithEstimate E87 Training 4096/6754, done at 2023-12-11 12:02:51, 0:07:23\n",
      "2023-12-11 12:02:46,428 WARNING  pid:22692 util:139:enumerateWithEstimate E87 Training ----/6754, done at 2023-12-11 12:02:46\n",
      "2023-12-11 12:02:46,428 INFO     pid:22692 __main__:409:logMetrics E87 SegmentationTrainingApp\n",
      "2023-12-11 12:02:46,428 INFO     pid:22692 __main__:444:logMetrics E87 trn      0.2515 loss, 0.1548 fnloss, 0.9958 fploss, 0.8253 precision, 0.7752 recall, 0.7995 f1 score\n",
      "2023-12-11 12:02:46,428 INFO     pid:22692 __main__:458:logMetrics E87 trn_all  0.2515 loss,  77.5% tp,  22.5% fn,      16.4% fp\n",
      "2023-12-11 12:02:46,444 INFO     pid:22692 __main__:191:main Epoch 88 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:02:46,444 WARNING  pid:22692 util:109:enumerateWithEstimate E88 Training ----/6754, starting\n",
      "2023-12-11 12:03:29,596 INFO     pid:22692 util:126:enumerateWithEstimate E88 Training   64/6754, done at 2023-12-11 12:10:27, 0:07:01\n",
      "2023-12-11 12:03:41,704 INFO     pid:22692 util:126:enumerateWithEstimate E88 Training  256/6754, done at 2023-12-11 12:10:30, 0:07:04\n",
      "2023-12-11 12:04:29,844 INFO     pid:22692 util:126:enumerateWithEstimate E88 Training 1024/6754, done at 2023-12-11 12:10:29, 0:07:02\n",
      "2023-12-11 12:07:41,751 INFO     pid:22692 util:126:enumerateWithEstimate E88 Training 4096/6754, done at 2023-12-11 12:10:27, 0:07:01\n",
      "2023-12-11 12:10:29,536 WARNING  pid:22692 util:139:enumerateWithEstimate E88 Training ----/6754, done at 2023-12-11 12:10:29\n",
      "2023-12-11 12:10:29,536 INFO     pid:22692 __main__:409:logMetrics E88 SegmentationTrainingApp\n",
      "2023-12-11 12:10:29,536 INFO     pid:22692 __main__:444:logMetrics E88 trn      0.2489 loss, 0.1486 fnloss, 0.9958 fploss, 0.8268 precision, 0.7837 recall, 0.8047 f1 score\n",
      "2023-12-11 12:10:29,536 INFO     pid:22692 __main__:458:logMetrics E88 trn_all  0.2489 loss,  78.4% tp,  21.6% fn,      16.4% fp\n",
      "2023-12-11 12:10:29,552 INFO     pid:22692 __main__:191:main Epoch 89 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:10:29,552 WARNING  pid:22692 util:109:enumerateWithEstimate E89 Training ----/6754, starting\n",
      "2023-12-11 12:11:12,579 INFO     pid:22692 util:126:enumerateWithEstimate E89 Training   64/6754, done at 2023-12-11 12:18:12, 0:07:03\n",
      "2023-12-11 12:11:24,704 INFO     pid:22692 util:126:enumerateWithEstimate E89 Training  256/6754, done at 2023-12-11 12:18:14, 0:07:05\n",
      "2023-12-11 12:12:13,063 INFO     pid:22692 util:126:enumerateWithEstimate E89 Training 1024/6754, done at 2023-12-11 12:18:13, 0:07:04\n",
      "2023-12-11 12:15:24,973 INFO     pid:22692 util:126:enumerateWithEstimate E89 Training 4096/6754, done at 2023-12-11 12:18:11, 0:07:01\n",
      "2023-12-11 12:18:12,604 WARNING  pid:22692 util:139:enumerateWithEstimate E89 Training ----/6754, done at 2023-12-11 12:18:12\n",
      "2023-12-11 12:18:12,604 INFO     pid:22692 __main__:409:logMetrics E89 SegmentationTrainingApp\n",
      "2023-12-11 12:18:12,604 INFO     pid:22692 __main__:444:logMetrics E89 trn      0.2485 loss, 0.1507 fnloss, 0.9958 fploss, 0.8280 precision, 0.7781 recall, 0.8023 f1 score\n",
      "2023-12-11 12:18:12,604 INFO     pid:22692 __main__:458:logMetrics E89 trn_all  0.2485 loss,  77.8% tp,  22.2% fn,      16.2% fp\n",
      "2023-12-11 12:18:12,604 INFO     pid:22692 __main__:191:main Epoch 90 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:18:12,619 WARNING  pid:22692 util:109:enumerateWithEstimate E90 Training ----/6754, starting\n",
      "2023-12-11 12:18:55,631 INFO     pid:22692 util:126:enumerateWithEstimate E90 Training   64/6754, done at 2023-12-11 12:25:55, 0:07:03\n",
      "2023-12-11 12:19:07,739 INFO     pid:22692 util:126:enumerateWithEstimate E90 Training  256/6754, done at 2023-12-11 12:25:57, 0:07:04\n",
      "2023-12-11 12:19:56,017 INFO     pid:22692 util:126:enumerateWithEstimate E90 Training 1024/6754, done at 2023-12-11 12:25:56, 0:07:03\n",
      "2023-12-11 12:23:07,957 INFO     pid:22692 util:126:enumerateWithEstimate E90 Training 4096/6754, done at 2023-12-11 12:25:54, 0:07:01\n",
      "2023-12-11 12:25:55,714 WARNING  pid:22692 util:139:enumerateWithEstimate E90 Training ----/6754, done at 2023-12-11 12:25:55\n",
      "2023-12-11 12:25:55,714 INFO     pid:22692 __main__:409:logMetrics E90 SegmentationTrainingApp\n",
      "2023-12-11 12:25:55,714 INFO     pid:22692 __main__:444:logMetrics E90 trn      0.2498 loss, 0.1490 fnloss, 0.9958 fploss, 0.8222 precision, 0.7892 recall, 0.8054 f1 score\n",
      "2023-12-11 12:25:55,714 INFO     pid:22692 __main__:458:logMetrics E90 trn_all  0.2498 loss,  78.9% tp,  21.1% fn,      17.1% fp\n",
      "2023-12-11 12:25:55,714 WARNING  pid:22692 util:109:enumerateWithEstimate E90 Validation  ----/862, starting\n",
      "2023-12-11 12:26:01,891 INFO     pid:22692 util:126:enumerateWithEstimate E90 Validation    64/862, done at 2023-12-11 12:26:16, 0:00:15\n",
      "2023-12-11 12:26:05,343 INFO     pid:22692 util:126:enumerateWithEstimate E90 Validation   256/862, done at 2023-12-11 12:26:16, 0:00:15\n",
      "2023-12-11 12:26:17,749 WARNING  pid:22692 util:139:enumerateWithEstimate E90 Validation  ----/862, done at 2023-12-11 12:26:17\n",
      "2023-12-11 12:26:17,749 INFO     pid:22692 __main__:409:logMetrics E90 SegmentationTrainingApp\n",
      "2023-12-11 12:26:17,749 INFO     pid:22692 __main__:444:logMetrics E90 val      0.3475 loss, 0.2886 fnloss, 0.9925 fploss, 0.8722 precision, 0.7399 recall, 0.8006 f1 score\n",
      "2023-12-11 12:26:17,749 INFO     pid:22692 __main__:458:logMetrics E90 val_all  0.3475 loss,  74.0% tp,  26.0% fn,      10.8% fp\n",
      "2023-12-11 12:26:17,858 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1215720.state\n",
      "2023-12-11 12:26:17,921 INFO     pid:22692 __main__:523:saveModel SHA1: c3b598b2ebd13ea3ac1ba4af0971381f20f926de\n",
      "2023-12-11 12:27:04,683 INFO     pid:22692 __main__:191:main Epoch 91 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:27:04,683 WARNING  pid:22692 util:109:enumerateWithEstimate E91 Training ----/6754, starting\n",
      "2023-12-11 12:27:47,272 INFO     pid:22692 util:126:enumerateWithEstimate E91 Training   64/6754, done at 2023-12-11 12:34:45, 0:07:01\n",
      "2023-12-11 12:27:59,412 INFO     pid:22692 util:126:enumerateWithEstimate E91 Training  256/6754, done at 2023-12-11 12:34:49, 0:07:05\n",
      "2023-12-11 12:28:47,596 INFO     pid:22692 util:126:enumerateWithEstimate E91 Training 1024/6754, done at 2023-12-11 12:34:47, 0:07:03\n",
      "2023-12-11 12:31:59,739 INFO     pid:22692 util:126:enumerateWithEstimate E91 Training 4096/6754, done at 2023-12-11 12:34:46, 0:07:01\n",
      "2023-12-11 12:34:47,366 WARNING  pid:22692 util:139:enumerateWithEstimate E91 Training ----/6754, done at 2023-12-11 12:34:47\n",
      "2023-12-11 12:34:47,366 INFO     pid:22692 __main__:409:logMetrics E91 SegmentationTrainingApp\n",
      "2023-12-11 12:34:47,366 INFO     pid:22692 __main__:444:logMetrics E91 trn      0.2473 loss, 0.1487 fnloss, 0.9958 fploss, 0.8274 precision, 0.7889 recall, 0.8077 f1 score\n",
      "2023-12-11 12:34:47,366 INFO     pid:22692 __main__:458:logMetrics E91 trn_all  0.2473 loss,  78.9% tp,  21.1% fn,      16.5% fp\n",
      "2023-12-11 12:34:47,381 INFO     pid:22692 __main__:191:main Epoch 92 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:34:47,381 WARNING  pid:22692 util:109:enumerateWithEstimate E92 Training ----/6754, starting\n",
      "2023-12-11 12:35:30,549 INFO     pid:22692 util:126:enumerateWithEstimate E92 Training   64/6754, done at 2023-12-11 12:42:32, 0:07:05\n",
      "2023-12-11 12:35:42,642 INFO     pid:22692 util:126:enumerateWithEstimate E92 Training  256/6754, done at 2023-12-11 12:42:32, 0:07:04\n",
      "2023-12-11 12:36:30,686 INFO     pid:22692 util:126:enumerateWithEstimate E92 Training 1024/6754, done at 2023-12-11 12:42:29, 0:07:02\n",
      "2023-12-11 12:39:42,984 INFO     pid:22692 util:126:enumerateWithEstimate E92 Training 4096/6754, done at 2023-12-11 12:42:29, 0:07:01\n",
      "2023-12-11 12:42:30,694 WARNING  pid:22692 util:139:enumerateWithEstimate E92 Training ----/6754, done at 2023-12-11 12:42:30\n",
      "2023-12-11 12:42:30,694 INFO     pid:22692 __main__:409:logMetrics E92 SegmentationTrainingApp\n",
      "2023-12-11 12:42:30,694 INFO     pid:22692 __main__:444:logMetrics E92 trn      0.2462 loss, 0.1478 fnloss, 0.9958 fploss, 0.8245 precision, 0.7851 recall, 0.8043 f1 score\n",
      "2023-12-11 12:42:30,694 INFO     pid:22692 __main__:458:logMetrics E92 trn_all  0.2462 loss,  78.5% tp,  21.5% fn,      16.7% fp\n",
      "2023-12-11 12:42:30,710 INFO     pid:22692 __main__:191:main Epoch 93 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:42:30,710 WARNING  pid:22692 util:109:enumerateWithEstimate E93 Training ----/6754, starting\n",
      "2023-12-11 12:43:13,799 INFO     pid:22692 util:126:enumerateWithEstimate E93 Training   64/6754, done at 2023-12-11 12:50:18, 0:07:07\n",
      "2023-12-11 12:43:25,908 INFO     pid:22692 util:126:enumerateWithEstimate E93 Training  256/6754, done at 2023-12-11 12:50:16, 0:07:05\n",
      "2023-12-11 12:44:14,328 INFO     pid:22692 util:126:enumerateWithEstimate E93 Training 1024/6754, done at 2023-12-11 12:50:15, 0:07:04\n",
      "2023-12-11 12:47:26,485 INFO     pid:22692 util:126:enumerateWithEstimate E93 Training 4096/6754, done at 2023-12-11 12:50:13, 0:07:02\n",
      "2023-12-11 12:50:14,309 WARNING  pid:22692 util:139:enumerateWithEstimate E93 Training ----/6754, done at 2023-12-11 12:50:14\n",
      "2023-12-11 12:50:14,309 INFO     pid:22692 __main__:409:logMetrics E93 SegmentationTrainingApp\n",
      "2023-12-11 12:50:14,309 INFO     pid:22692 __main__:444:logMetrics E93 trn      0.2473 loss, 0.1411 fnloss, 0.9959 fploss, 0.8189 precision, 0.7923 recall, 0.8054 f1 score\n",
      "2023-12-11 12:50:14,309 INFO     pid:22692 __main__:458:logMetrics E93 trn_all  0.2473 loss,  79.2% tp,  20.8% fn,      17.5% fp\n",
      "2023-12-11 12:50:14,325 INFO     pid:22692 __main__:191:main Epoch 94 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:50:14,325 WARNING  pid:22692 util:109:enumerateWithEstimate E94 Training ----/6754, starting\n",
      "2023-12-11 12:50:57,195 INFO     pid:22692 util:126:enumerateWithEstimate E94 Training   64/6754, done at 2023-12-11 12:57:59, 0:07:05\n",
      "2023-12-11 12:51:09,321 INFO     pid:22692 util:126:enumerateWithEstimate E94 Training  256/6754, done at 2023-12-11 12:57:59, 0:07:05\n",
      "2023-12-11 12:51:57,619 INFO     pid:22692 util:126:enumerateWithEstimate E94 Training 1024/6754, done at 2023-12-11 12:57:58, 0:07:04\n",
      "2023-12-11 12:55:09,702 INFO     pid:22692 util:126:enumerateWithEstimate E94 Training 4096/6754, done at 2023-12-11 12:57:56, 0:07:02\n",
      "2023-12-11 12:57:57,429 WARNING  pid:22692 util:139:enumerateWithEstimate E94 Training ----/6754, done at 2023-12-11 12:57:57\n",
      "2023-12-11 12:57:57,429 INFO     pid:22692 __main__:409:logMetrics E94 SegmentationTrainingApp\n",
      "2023-12-11 12:57:57,429 INFO     pid:22692 __main__:444:logMetrics E94 trn      0.2456 loss, 0.1461 fnloss, 0.9958 fploss, 0.8271 precision, 0.7851 recall, 0.8056 f1 score\n",
      "2023-12-11 12:57:57,429 INFO     pid:22692 __main__:458:logMetrics E94 trn_all  0.2456 loss,  78.5% tp,  21.5% fn,      16.4% fp\n",
      "2023-12-11 12:57:57,444 INFO     pid:22692 __main__:191:main Epoch 95 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 12:57:57,444 WARNING  pid:22692 util:109:enumerateWithEstimate E95 Training ----/6754, starting\n",
      "2023-12-11 12:58:40,658 INFO     pid:22692 util:126:enumerateWithEstimate E95 Training   64/6754, done at 2023-12-11 13:05:42, 0:07:05\n",
      "2023-12-11 12:58:52,767 INFO     pid:22692 util:126:enumerateWithEstimate E95 Training  256/6754, done at 2023-12-11 13:05:42, 0:07:05\n",
      "2023-12-11 12:59:41,013 INFO     pid:22692 util:126:enumerateWithEstimate E95 Training 1024/6754, done at 2023-12-11 13:05:41, 0:07:03\n",
      "2023-12-11 13:02:52,923 INFO     pid:22692 util:126:enumerateWithEstimate E95 Training 4096/6754, done at 2023-12-11 13:05:39, 0:07:01\n",
      "2023-12-11 13:05:43,022 WARNING  pid:22692 util:139:enumerateWithEstimate E95 Training ----/6754, done at 2023-12-11 13:05:43\n",
      "2023-12-11 13:05:43,022 INFO     pid:22692 __main__:409:logMetrics E95 SegmentationTrainingApp\n",
      "2023-12-11 13:05:43,022 INFO     pid:22692 __main__:444:logMetrics E95 trn      0.2472 loss, 0.1539 fnloss, 0.9957 fploss, 0.8303 precision, 0.7883 recall, 0.8088 f1 score\n",
      "2023-12-11 13:05:43,022 INFO     pid:22692 __main__:458:logMetrics E95 trn_all  0.2472 loss,  78.8% tp,  21.2% fn,      16.1% fp\n",
      "2023-12-11 13:05:43,038 WARNING  pid:22692 util:109:enumerateWithEstimate E95 Validation  ----/862, starting\n",
      "2023-12-11 13:05:49,802 INFO     pid:22692 util:126:enumerateWithEstimate E95 Validation    64/862, done at 2023-12-11 13:06:04, 0:00:15\n",
      "2023-12-11 13:05:53,239 INFO     pid:22692 util:126:enumerateWithEstimate E95 Validation   256/862, done at 2023-12-11 13:06:04, 0:00:15\n",
      "2023-12-11 13:06:05,566 WARNING  pid:22692 util:139:enumerateWithEstimate E95 Validation  ----/862, done at 2023-12-11 13:06:05\n",
      "2023-12-11 13:06:05,566 INFO     pid:22692 __main__:409:logMetrics E95 SegmentationTrainingApp\n",
      "2023-12-11 13:06:05,566 INFO     pid:22692 __main__:444:logMetrics E95 val      0.4123 loss, 0.3690 fnloss, 0.9916 fploss, 0.8865 precision, 0.7187 recall, 0.7938 f1 score\n",
      "2023-12-11 13:06:05,566 INFO     pid:22692 __main__:458:logMetrics E95 val_all  0.4123 loss,  71.9% tp,  28.1% fn,       9.2% fp\n",
      "2023-12-11 13:06:05,691 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1283260.state\n",
      "2023-12-11 13:06:05,753 INFO     pid:22692 __main__:523:saveModel SHA1: 2cac41094e1203157f9b163410613f4a9fd5caa5\n",
      "2023-12-11 13:06:52,014 INFO     pid:22692 __main__:191:main Epoch 96 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:06:52,030 WARNING  pid:22692 util:109:enumerateWithEstimate E96 Training ----/6754, starting\n",
      "2023-12-11 13:07:34,495 INFO     pid:22692 util:126:enumerateWithEstimate E96 Training   64/6754, done at 2023-12-11 13:14:38, 0:07:07\n",
      "2023-12-11 13:07:46,604 INFO     pid:22692 util:126:enumerateWithEstimate E96 Training  256/6754, done at 2023-12-11 13:14:36, 0:07:05\n",
      "2023-12-11 13:08:35,179 INFO     pid:22692 util:126:enumerateWithEstimate E96 Training 1024/6754, done at 2023-12-11 13:14:37, 0:07:06\n",
      "2023-12-11 13:11:47,477 INFO     pid:22692 util:126:enumerateWithEstimate E96 Training 4096/6754, done at 2023-12-11 13:14:34, 0:07:02\n",
      "2023-12-11 13:14:35,230 WARNING  pid:22692 util:139:enumerateWithEstimate E96 Training ----/6754, done at 2023-12-11 13:14:35\n",
      "2023-12-11 13:14:35,230 INFO     pid:22692 __main__:409:logMetrics E96 SegmentationTrainingApp\n",
      "2023-12-11 13:14:35,230 INFO     pid:22692 __main__:444:logMetrics E96 trn      0.2494 loss, 0.1463 fnloss, 0.9959 fploss, 0.8239 precision, 0.7869 recall, 0.8050 f1 score\n",
      "2023-12-11 13:14:35,230 INFO     pid:22692 __main__:458:logMetrics E96 trn_all  0.2494 loss,  78.7% tp,  21.3% fn,      16.8% fp\n",
      "2023-12-11 13:14:35,245 INFO     pid:22692 __main__:191:main Epoch 97 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:14:35,245 WARNING  pid:22692 util:109:enumerateWithEstimate E97 Training ----/6754, starting\n",
      "2023-12-11 13:15:18,367 INFO     pid:22692 util:126:enumerateWithEstimate E97 Training   64/6754, done at 2023-12-11 13:22:20, 0:07:05\n",
      "2023-12-11 13:15:30,475 INFO     pid:22692 util:126:enumerateWithEstimate E97 Training  256/6754, done at 2023-12-11 13:22:20, 0:07:05\n",
      "2023-12-11 13:16:18,912 INFO     pid:22692 util:126:enumerateWithEstimate E97 Training 1024/6754, done at 2023-12-11 13:22:20, 0:07:04\n",
      "2023-12-11 13:19:31,459 INFO     pid:22692 util:126:enumerateWithEstimate E97 Training 4096/6754, done at 2023-12-11 13:22:18, 0:07:02\n",
      "2023-12-11 13:22:19,087 WARNING  pid:22692 util:139:enumerateWithEstimate E97 Training ----/6754, done at 2023-12-11 13:22:19\n",
      "2023-12-11 13:22:19,103 INFO     pid:22692 __main__:409:logMetrics E97 SegmentationTrainingApp\n",
      "2023-12-11 13:22:19,103 INFO     pid:22692 __main__:444:logMetrics E97 trn      0.2471 loss, 0.1470 fnloss, 0.9958 fploss, 0.8277 precision, 0.7871 recall, 0.8069 f1 score\n",
      "2023-12-11 13:22:19,103 INFO     pid:22692 __main__:458:logMetrics E97 trn_all  0.2471 loss,  78.7% tp,  21.3% fn,      16.4% fp\n",
      "2023-12-11 13:22:19,103 INFO     pid:22692 __main__:191:main Epoch 98 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:22:19,103 WARNING  pid:22692 util:109:enumerateWithEstimate E98 Training ----/6754, starting\n",
      "2023-12-11 13:23:02,177 INFO     pid:22692 util:126:enumerateWithEstimate E98 Training   64/6754, done at 2023-12-11 13:30:04, 0:07:05\n",
      "2023-12-11 13:23:14,286 INFO     pid:22692 util:126:enumerateWithEstimate E98 Training  256/6754, done at 2023-12-11 13:30:04, 0:07:05\n",
      "2023-12-11 13:24:02,754 INFO     pid:22692 util:126:enumerateWithEstimate E98 Training 1024/6754, done at 2023-12-11 13:30:04, 0:07:05\n",
      "2023-12-11 13:27:14,815 INFO     pid:22692 util:126:enumerateWithEstimate E98 Training 4096/6754, done at 2023-12-11 13:30:01, 0:07:02\n",
      "2023-12-11 13:30:02,492 WARNING  pid:22692 util:139:enumerateWithEstimate E98 Training ----/6754, done at 2023-12-11 13:30:02\n",
      "2023-12-11 13:30:02,492 INFO     pid:22692 __main__:409:logMetrics E98 SegmentationTrainingApp\n",
      "2023-12-11 13:30:02,508 INFO     pid:22692 __main__:444:logMetrics E98 trn      0.2465 loss, 0.1475 fnloss, 0.9958 fploss, 0.8255 precision, 0.7867 recall, 0.8057 f1 score\n",
      "2023-12-11 13:30:02,508 INFO     pid:22692 __main__:458:logMetrics E98 trn_all  0.2465 loss,  78.7% tp,  21.3% fn,      16.6% fp\n",
      "2023-12-11 13:30:02,508 INFO     pid:22692 __main__:191:main Epoch 99 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:30:02,508 WARNING  pid:22692 util:109:enumerateWithEstimate E99 Training ----/6754, starting\n",
      "2023-12-11 13:30:45,628 INFO     pid:22692 util:126:enumerateWithEstimate E99 Training   64/6754, done at 2023-12-11 13:37:45, 0:07:03\n",
      "2023-12-11 13:30:57,737 INFO     pid:22692 util:126:enumerateWithEstimate E99 Training  256/6754, done at 2023-12-11 13:37:47, 0:07:04\n",
      "2023-12-11 13:31:46,063 INFO     pid:22692 util:126:enumerateWithEstimate E99 Training 1024/6754, done at 2023-12-11 13:37:46, 0:07:04\n",
      "2023-12-11 13:34:58,017 INFO     pid:22692 util:126:enumerateWithEstimate E99 Training 4096/6754, done at 2023-12-11 13:37:44, 0:07:01\n",
      "2023-12-11 13:37:45,661 WARNING  pid:22692 util:139:enumerateWithEstimate E99 Training ----/6754, done at 2023-12-11 13:37:45\n",
      "2023-12-11 13:37:45,677 INFO     pid:22692 __main__:409:logMetrics E99 SegmentationTrainingApp\n",
      "2023-12-11 13:37:45,677 INFO     pid:22692 __main__:444:logMetrics E99 trn      0.2459 loss, 0.1499 fnloss, 0.9957 fploss, 0.8276 precision, 0.7885 recall, 0.8076 f1 score\n",
      "2023-12-11 13:37:45,677 INFO     pid:22692 __main__:458:logMetrics E99 trn_all  0.2459 loss,  78.8% tp,  21.2% fn,      16.4% fp\n",
      "2023-12-11 13:37:45,677 INFO     pid:22692 __main__:191:main Epoch 100 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:37:45,677 WARNING  pid:22692 util:109:enumerateWithEstimate E100 Training ----/6754, starting\n",
      "2023-12-11 13:38:28,485 INFO     pid:22692 util:126:enumerateWithEstimate E100 Training   64/6754, done at 2023-12-11 13:45:30, 0:07:05\n",
      "2023-12-11 13:38:40,609 INFO     pid:22692 util:126:enumerateWithEstimate E100 Training  256/6754, done at 2023-12-11 13:45:30, 0:07:05\n",
      "2023-12-11 13:39:28,824 INFO     pid:22692 util:126:enumerateWithEstimate E100 Training 1024/6754, done at 2023-12-11 13:45:28, 0:07:03\n",
      "2023-12-11 13:42:41,045 INFO     pid:22692 util:126:enumerateWithEstimate E100 Training 4096/6754, done at 2023-12-11 13:45:27, 0:07:02\n",
      "2023-12-11 13:45:28,973 WARNING  pid:22692 util:139:enumerateWithEstimate E100 Training ----/6754, done at 2023-12-11 13:45:28\n",
      "2023-12-11 13:45:28,973 INFO     pid:22692 __main__:409:logMetrics E100 SegmentationTrainingApp\n",
      "2023-12-11 13:45:28,973 INFO     pid:22692 __main__:444:logMetrics E100 trn      0.2424 loss, 0.1482 fnloss, 0.9957 fploss, 0.8341 precision, 0.7864 recall, 0.8096 f1 score\n",
      "2023-12-11 13:45:28,973 INFO     pid:22692 __main__:458:logMetrics E100 trn_all  0.2424 loss,  78.6% tp,  21.4% fn,      15.6% fp\n",
      "2023-12-11 13:45:28,973 WARNING  pid:22692 util:109:enumerateWithEstimate E100 Validation  ----/862, starting\n",
      "2023-12-11 13:45:35,757 INFO     pid:22692 util:126:enumerateWithEstimate E100 Validation    64/862, done at 2023-12-11 13:45:50, 0:00:15\n",
      "2023-12-11 13:45:39,209 INFO     pid:22692 util:126:enumerateWithEstimate E100 Validation   256/862, done at 2023-12-11 13:45:50, 0:00:15\n",
      "2023-12-11 13:45:51,567 WARNING  pid:22692 util:139:enumerateWithEstimate E100 Validation  ----/862, done at 2023-12-11 13:45:51\n",
      "2023-12-11 13:45:51,567 INFO     pid:22692 __main__:409:logMetrics E100 SegmentationTrainingApp\n",
      "2023-12-11 13:45:51,567 INFO     pid:22692 __main__:444:logMetrics E100 val      0.3296 loss, 0.2565 fnloss, 0.9927 fploss, 0.8484 precision, 0.7791 recall, 0.8123 f1 score\n",
      "2023-12-11 13:45:51,567 INFO     pid:22692 __main__:458:logMetrics E100 val_all  0.3296 loss,  77.9% tp,  22.1% fn,      13.9% fp\n",
      "2023-12-11 13:45:51,692 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1350800.state\n",
      "2023-12-11 13:45:51,739 INFO     pid:22692 __main__:523:saveModel SHA1: 28929631d5876148de1767c43ba7517bd1fa575f\n",
      "2023-12-11 13:46:39,782 INFO     pid:22692 __main__:191:main Epoch 101 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:46:39,782 WARNING  pid:22692 util:109:enumerateWithEstimate E101 Training ----/6754, starting\n",
      "2023-12-11 13:47:22,496 INFO     pid:22692 util:126:enumerateWithEstimate E101 Training   64/6754, done at 2023-12-11 13:54:24, 0:07:05\n",
      "2023-12-11 13:47:34,714 INFO     pid:22692 util:126:enumerateWithEstimate E101 Training  256/6754, done at 2023-12-11 13:54:27, 0:07:08\n",
      "2023-12-11 13:48:23,433 INFO     pid:22692 util:126:enumerateWithEstimate E101 Training 1024/6754, done at 2023-12-11 13:54:26, 0:07:07\n",
      "2023-12-11 13:51:35,901 INFO     pid:22692 util:126:enumerateWithEstimate E101 Training 4096/6754, done at 2023-12-11 13:54:22, 0:07:03\n",
      "2023-12-11 13:54:23,883 WARNING  pid:22692 util:139:enumerateWithEstimate E101 Training ----/6754, done at 2023-12-11 13:54:23\n",
      "2023-12-11 13:54:23,883 INFO     pid:22692 __main__:409:logMetrics E101 SegmentationTrainingApp\n",
      "2023-12-11 13:54:23,883 INFO     pid:22692 __main__:444:logMetrics E101 trn      0.2447 loss, 0.1504 fnloss, 0.9958 fploss, 0.8323 precision, 0.7911 recall, 0.8111 f1 score\n",
      "2023-12-11 13:54:23,883 INFO     pid:22692 __main__:458:logMetrics E101 trn_all  0.2447 loss,  79.1% tp,  20.9% fn,      15.9% fp\n",
      "2023-12-11 13:54:23,883 INFO     pid:22692 __main__:191:main Epoch 102 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 13:54:23,883 WARNING  pid:22692 util:109:enumerateWithEstimate E102 Training ----/6754, starting\n",
      "2023-12-11 13:55:06,929 INFO     pid:22692 util:126:enumerateWithEstimate E102 Training   64/6754, done at 2023-12-11 14:02:07, 0:07:03\n",
      "2023-12-11 13:55:19,053 INFO     pid:22692 util:126:enumerateWithEstimate E102 Training  256/6754, done at 2023-12-11 14:02:08, 0:07:05\n",
      "2023-12-11 13:56:07,236 INFO     pid:22692 util:126:enumerateWithEstimate E102 Training 1024/6754, done at 2023-12-11 14:02:07, 0:07:03\n",
      "2023-12-11 13:59:19,186 INFO     pid:22692 util:126:enumerateWithEstimate E102 Training 4096/6754, done at 2023-12-11 14:02:05, 0:07:01\n",
      "2023-12-11 14:02:06,794 WARNING  pid:22692 util:139:enumerateWithEstimate E102 Training ----/6754, done at 2023-12-11 14:02:06\n",
      "2023-12-11 14:02:06,794 INFO     pid:22692 __main__:409:logMetrics E102 SegmentationTrainingApp\n",
      "2023-12-11 14:02:06,810 INFO     pid:22692 __main__:444:logMetrics E102 trn      0.2417 loss, 0.1387 fnloss, 0.9958 fploss, 0.8216 precision, 0.8002 recall, 0.8108 f1 score\n",
      "2023-12-11 14:02:06,810 INFO     pid:22692 __main__:458:logMetrics E102 trn_all  0.2417 loss,  80.0% tp,  20.0% fn,      17.4% fp\n",
      "2023-12-11 14:02:06,810 INFO     pid:22692 __main__:191:main Epoch 103 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:02:06,810 WARNING  pid:22692 util:109:enumerateWithEstimate E103 Training ----/6754, starting\n",
      "2023-12-11 14:02:49,792 INFO     pid:22692 util:126:enumerateWithEstimate E103 Training   64/6754, done at 2023-12-11 14:09:54, 0:07:07\n",
      "2023-12-11 14:03:01,932 INFO     pid:22692 util:126:enumerateWithEstimate E103 Training  256/6754, done at 2023-12-11 14:09:53, 0:07:06\n",
      "2023-12-11 14:03:50,489 INFO     pid:22692 util:126:enumerateWithEstimate E103 Training 1024/6754, done at 2023-12-11 14:09:52, 0:07:06\n",
      "2023-12-11 14:07:02,485 INFO     pid:22692 util:126:enumerateWithEstimate E103 Training 4096/6754, done at 2023-12-11 14:09:49, 0:07:02\n",
      "2023-12-11 14:09:50,266 WARNING  pid:22692 util:139:enumerateWithEstimate E103 Training ----/6754, done at 2023-12-11 14:09:50\n",
      "2023-12-11 14:09:50,266 INFO     pid:22692 __main__:409:logMetrics E103 SegmentationTrainingApp\n",
      "2023-12-11 14:09:50,266 INFO     pid:22692 __main__:444:logMetrics E103 trn      0.2444 loss, 0.1410 fnloss, 0.9959 fploss, 0.8233 precision, 0.7962 recall, 0.8095 f1 score\n",
      "2023-12-11 14:09:50,266 INFO     pid:22692 __main__:458:logMetrics E103 trn_all  0.2444 loss,  79.6% tp,  20.4% fn,      17.1% fp\n",
      "2023-12-11 14:09:50,281 INFO     pid:22692 __main__:191:main Epoch 104 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:09:50,281 WARNING  pid:22692 util:109:enumerateWithEstimate E104 Training ----/6754, starting\n",
      "2023-12-11 14:10:33,558 INFO     pid:22692 util:126:enumerateWithEstimate E104 Training   64/6754, done at 2023-12-11 14:17:35, 0:07:05\n",
      "2023-12-11 14:10:45,697 INFO     pid:22692 util:126:enumerateWithEstimate E104 Training  256/6754, done at 2023-12-11 14:17:36, 0:07:05\n",
      "2023-12-11 14:11:34,254 INFO     pid:22692 util:126:enumerateWithEstimate E104 Training 1024/6754, done at 2023-12-11 14:17:36, 0:07:05\n",
      "2023-12-11 14:14:46,407 INFO     pid:22692 util:126:enumerateWithEstimate E104 Training 4096/6754, done at 2023-12-11 14:17:33, 0:07:02\n",
      "2023-12-11 14:17:33,954 WARNING  pid:22692 util:139:enumerateWithEstimate E104 Training ----/6754, done at 2023-12-11 14:17:33\n",
      "2023-12-11 14:17:33,954 INFO     pid:22692 __main__:409:logMetrics E104 SegmentationTrainingApp\n",
      "2023-12-11 14:17:33,954 INFO     pid:22692 __main__:444:logMetrics E104 trn      0.2401 loss, 0.1395 fnloss, 0.9958 fploss, 0.8273 precision, 0.8006 recall, 0.8137 f1 score\n",
      "2023-12-11 14:17:33,954 INFO     pid:22692 __main__:458:logMetrics E104 trn_all  0.2401 loss,  80.1% tp,  19.9% fn,      16.7% fp\n",
      "2023-12-11 14:17:33,969 INFO     pid:22692 __main__:191:main Epoch 105 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:17:33,969 WARNING  pid:22692 util:109:enumerateWithEstimate E105 Training ----/6754, starting\n",
      "2023-12-11 14:18:16,857 INFO     pid:22692 util:126:enumerateWithEstimate E105 Training   64/6754, done at 2023-12-11 14:25:17, 0:07:03\n",
      "2023-12-11 14:18:28,980 INFO     pid:22692 util:126:enumerateWithEstimate E105 Training  256/6754, done at 2023-12-11 14:25:18, 0:07:05\n",
      "2023-12-11 14:19:17,507 INFO     pid:22692 util:126:enumerateWithEstimate E105 Training 1024/6754, done at 2023-12-11 14:25:19, 0:07:05\n",
      "2023-12-11 14:22:29,596 INFO     pid:22692 util:126:enumerateWithEstimate E105 Training 4096/6754, done at 2023-12-11 14:25:16, 0:07:02\n",
      "2023-12-11 14:25:17,126 WARNING  pid:22692 util:139:enumerateWithEstimate E105 Training ----/6754, done at 2023-12-11 14:25:17\n",
      "2023-12-11 14:25:17,126 INFO     pid:22692 __main__:409:logMetrics E105 SegmentationTrainingApp\n",
      "2023-12-11 14:25:17,142 INFO     pid:22692 __main__:444:logMetrics E105 trn      0.2414 loss, 0.1479 fnloss, 0.9957 fploss, 0.8312 precision, 0.7888 recall, 0.8094 f1 score\n",
      "2023-12-11 14:25:17,142 INFO     pid:22692 __main__:458:logMetrics E105 trn_all  0.2414 loss,  78.9% tp,  21.1% fn,      16.0% fp\n",
      "2023-12-11 14:25:17,142 WARNING  pid:22692 util:109:enumerateWithEstimate E105 Validation  ----/862, starting\n",
      "2023-12-11 14:25:23,395 INFO     pid:22692 util:126:enumerateWithEstimate E105 Validation    64/862, done at 2023-12-11 14:25:37, 0:00:15\n",
      "2023-12-11 14:25:26,848 INFO     pid:22692 util:126:enumerateWithEstimate E105 Validation   256/862, done at 2023-12-11 14:25:37, 0:00:15\n",
      "2023-12-11 14:25:39,237 WARNING  pid:22692 util:139:enumerateWithEstimate E105 Validation  ----/862, done at 2023-12-11 14:25:39\n",
      "2023-12-11 14:25:39,237 INFO     pid:22692 __main__:409:logMetrics E105 SegmentationTrainingApp\n",
      "2023-12-11 14:25:39,237 INFO     pid:22692 __main__:444:logMetrics E105 val      0.4011 loss, 0.3534 fnloss, 0.9917 fploss, 0.8450 precision, 0.7318 recall, 0.7843 f1 score\n",
      "2023-12-11 14:25:39,237 INFO     pid:22692 __main__:458:logMetrics E105 val_all  0.4011 loss,  73.2% tp,  26.8% fn,      13.4% fp\n",
      "2023-12-11 14:25:39,347 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1418340.state\n",
      "2023-12-11 14:25:39,409 INFO     pid:22692 __main__:523:saveModel SHA1: f01ec2f2f52189da7169d155de3b2ceb10926c02\n",
      "2023-12-11 14:26:26,221 INFO     pid:22692 __main__:191:main Epoch 106 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:26:26,221 WARNING  pid:22692 util:109:enumerateWithEstimate E106 Training ----/6754, starting\n",
      "2023-12-11 14:27:08,811 INFO     pid:22692 util:126:enumerateWithEstimate E106 Training   64/6754, done at 2023-12-11 14:34:06, 0:07:01\n",
      "2023-12-11 14:27:20,778 INFO     pid:22692 util:126:enumerateWithEstimate E106 Training  256/6754, done at 2023-12-11 14:34:05, 0:07:00\n",
      "2023-12-11 14:28:09,273 INFO     pid:22692 util:126:enumerateWithEstimate E106 Training 1024/6754, done at 2023-12-11 14:34:09, 0:07:04\n",
      "2023-12-11 14:31:21,503 INFO     pid:22692 util:126:enumerateWithEstimate E106 Training 4096/6754, done at 2023-12-11 14:34:08, 0:07:02\n",
      "2023-12-11 14:34:09,300 WARNING  pid:22692 util:139:enumerateWithEstimate E106 Training ----/6754, done at 2023-12-11 14:34:09\n",
      "2023-12-11 14:34:09,300 INFO     pid:22692 __main__:409:logMetrics E106 SegmentationTrainingApp\n",
      "2023-12-11 14:34:09,300 INFO     pid:22692 __main__:444:logMetrics E106 trn      0.2359 loss, 0.1418 fnloss, 0.9958 fploss, 0.8392 precision, 0.7894 recall, 0.8136 f1 score\n",
      "2023-12-11 14:34:09,315 INFO     pid:22692 __main__:458:logMetrics E106 trn_all  0.2359 loss,  78.9% tp,  21.1% fn,      15.1% fp\n",
      "2023-12-11 14:34:09,315 INFO     pid:22692 __main__:191:main Epoch 107 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:34:09,315 WARNING  pid:22692 util:109:enumerateWithEstimate E107 Training ----/6754, starting\n",
      "2023-12-11 14:34:52,514 INFO     pid:22692 util:126:enumerateWithEstimate E107 Training   64/6754, done at 2023-12-11 14:41:52, 0:07:03\n",
      "2023-12-11 14:35:04,623 INFO     pid:22692 util:126:enumerateWithEstimate E107 Training  256/6754, done at 2023-12-11 14:41:54, 0:07:04\n",
      "2023-12-11 14:35:52,790 INFO     pid:22692 util:126:enumerateWithEstimate E107 Training 1024/6754, done at 2023-12-11 14:41:52, 0:07:03\n",
      "2023-12-11 14:39:04,927 INFO     pid:22692 util:126:enumerateWithEstimate E107 Training 4096/6754, done at 2023-12-11 14:41:51, 0:07:01\n",
      "2023-12-11 14:41:52,614 WARNING  pid:22692 util:139:enumerateWithEstimate E107 Training ----/6754, done at 2023-12-11 14:41:52\n",
      "2023-12-11 14:41:52,614 INFO     pid:22692 __main__:409:logMetrics E107 SegmentationTrainingApp\n",
      "2023-12-11 14:41:52,614 INFO     pid:22692 __main__:444:logMetrics E107 trn      0.2413 loss, 0.1433 fnloss, 0.9957 fploss, 0.8265 precision, 0.7985 recall, 0.8122 f1 score\n",
      "2023-12-11 14:41:52,614 INFO     pid:22692 __main__:458:logMetrics E107 trn_all  0.2413 loss,  79.8% tp,  20.2% fn,      16.8% fp\n",
      "2023-12-11 14:41:52,614 INFO     pid:22692 __main__:191:main Epoch 108 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:41:52,629 WARNING  pid:22692 util:109:enumerateWithEstimate E108 Training ----/6754, starting\n",
      "2023-12-11 14:42:35,750 INFO     pid:22692 util:126:enumerateWithEstimate E108 Training   64/6754, done at 2023-12-11 14:49:40, 0:07:07\n",
      "2023-12-11 14:42:47,842 INFO     pid:22692 util:126:enumerateWithEstimate E108 Training  256/6754, done at 2023-12-11 14:49:37, 0:07:05\n",
      "2023-12-11 14:43:35,978 INFO     pid:22692 util:126:enumerateWithEstimate E108 Training 1024/6754, done at 2023-12-11 14:49:35, 0:07:02\n",
      "2023-12-11 14:46:48,240 INFO     pid:22692 util:126:enumerateWithEstimate E108 Training 4096/6754, done at 2023-12-11 14:49:34, 0:07:02\n",
      "2023-12-11 14:49:36,011 WARNING  pid:22692 util:139:enumerateWithEstimate E108 Training ----/6754, done at 2023-12-11 14:49:36\n",
      "2023-12-11 14:49:36,027 INFO     pid:22692 __main__:409:logMetrics E108 SegmentationTrainingApp\n",
      "2023-12-11 14:49:36,027 INFO     pid:22692 __main__:444:logMetrics E108 trn      0.2435 loss, 0.1457 fnloss, 0.9957 fploss, 0.8217 precision, 0.7982 recall, 0.8098 f1 score\n",
      "2023-12-11 14:49:36,027 INFO     pid:22692 __main__:458:logMetrics E108 trn_all  0.2435 loss,  79.8% tp,  20.2% fn,      17.3% fp\n",
      "2023-12-11 14:49:36,027 INFO     pid:22692 __main__:191:main Epoch 109 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:49:36,027 WARNING  pid:22692 util:109:enumerateWithEstimate E109 Training ----/6754, starting\n",
      "2023-12-11 14:50:18,819 INFO     pid:22692 util:126:enumerateWithEstimate E109 Training   64/6754, done at 2023-12-11 14:57:18, 0:07:03\n",
      "2023-12-11 14:50:30,974 INFO     pid:22692 util:126:enumerateWithEstimate E109 Training  256/6754, done at 2023-12-11 14:57:21, 0:07:05\n",
      "2023-12-11 14:51:19,470 INFO     pid:22692 util:126:enumerateWithEstimate E109 Training 1024/6754, done at 2023-12-11 14:57:21, 0:07:05\n",
      "2023-12-11 14:54:31,715 INFO     pid:22692 util:126:enumerateWithEstimate E109 Training 4096/6754, done at 2023-12-11 14:57:18, 0:07:02\n",
      "2023-12-11 14:57:19,528 WARNING  pid:22692 util:139:enumerateWithEstimate E109 Training ----/6754, done at 2023-12-11 14:57:19\n",
      "2023-12-11 14:57:19,528 INFO     pid:22692 __main__:409:logMetrics E109 SegmentationTrainingApp\n",
      "2023-12-11 14:57:19,528 INFO     pid:22692 __main__:444:logMetrics E109 trn      0.2395 loss, 0.1401 fnloss, 0.9957 fploss, 0.8274 precision, 0.7970 recall, 0.8119 f1 score\n",
      "2023-12-11 14:57:19,528 INFO     pid:22692 __main__:458:logMetrics E109 trn_all  0.2395 loss,  79.7% tp,  20.3% fn,      16.6% fp\n",
      "2023-12-11 14:57:19,544 INFO     pid:22692 __main__:191:main Epoch 110 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 14:57:19,544 WARNING  pid:22692 util:109:enumerateWithEstimate E110 Training ----/6754, starting\n",
      "2023-12-11 14:58:02,634 INFO     pid:22692 util:126:enumerateWithEstimate E110 Training   64/6754, done at 2023-12-11 15:05:07, 0:07:07\n",
      "2023-12-11 14:58:14,742 INFO     pid:22692 util:126:enumerateWithEstimate E110 Training  256/6754, done at 2023-12-11 15:05:04, 0:07:05\n",
      "2023-12-11 14:59:03,159 INFO     pid:22692 util:126:enumerateWithEstimate E110 Training 1024/6754, done at 2023-12-11 15:05:04, 0:07:04\n",
      "2023-12-11 15:02:15,264 INFO     pid:22692 util:126:enumerateWithEstimate E110 Training 4096/6754, done at 2023-12-11 15:05:01, 0:07:02\n",
      "2023-12-11 15:05:02,919 WARNING  pid:22692 util:139:enumerateWithEstimate E110 Training ----/6754, done at 2023-12-11 15:05:02\n",
      "2023-12-11 15:05:02,919 INFO     pid:22692 __main__:409:logMetrics E110 SegmentationTrainingApp\n",
      "2023-12-11 15:05:02,919 INFO     pid:22692 __main__:444:logMetrics E110 trn      0.2421 loss, 0.1399 fnloss, 0.9958 fploss, 0.8243 precision, 0.7963 recall, 0.8100 f1 score\n",
      "2023-12-11 15:05:02,919 INFO     pid:22692 __main__:458:logMetrics E110 trn_all  0.2421 loss,  79.6% tp,  20.4% fn,      17.0% fp\n",
      "2023-12-11 15:05:02,919 WARNING  pid:22692 util:109:enumerateWithEstimate E110 Validation  ----/862, starting\n",
      "2023-12-11 15:05:09,330 INFO     pid:22692 util:126:enumerateWithEstimate E110 Validation    64/862, done at 2023-12-11 15:05:23, 0:00:15\n",
      "2023-12-11 15:05:12,783 INFO     pid:22692 util:126:enumerateWithEstimate E110 Validation   256/862, done at 2023-12-11 15:05:23, 0:00:15\n",
      "2023-12-11 15:05:25,063 WARNING  pid:22692 util:139:enumerateWithEstimate E110 Validation  ----/862, done at 2023-12-11 15:05:25\n",
      "2023-12-11 15:05:25,079 INFO     pid:22692 __main__:409:logMetrics E110 SegmentationTrainingApp\n",
      "2023-12-11 15:05:25,079 INFO     pid:22692 __main__:444:logMetrics E110 val      0.3397 loss, 0.2632 fnloss, 0.9923 fploss, 0.8293 precision, 0.7507 recall, 0.7880 f1 score\n",
      "2023-12-11 15:05:25,079 INFO     pid:22692 __main__:458:logMetrics E110 val_all  0.3397 loss,  75.1% tp,  24.9% fn,      15.5% fp\n",
      "2023-12-11 15:05:25,172 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1485880.state\n",
      "2023-12-11 15:05:25,235 INFO     pid:22692 __main__:523:saveModel SHA1: 0748a3ba6e38f2358176cce72e65044668a18d6c\n",
      "2023-12-11 15:06:11,420 INFO     pid:22692 __main__:191:main Epoch 111 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:06:11,420 WARNING  pid:22692 util:109:enumerateWithEstimate E111 Training ----/6754, starting\n",
      "2023-12-11 15:06:53,853 INFO     pid:22692 util:126:enumerateWithEstimate E111 Training   64/6754, done at 2023-12-11 15:13:49, 0:06:58\n",
      "2023-12-11 15:07:05,946 INFO     pid:22692 util:126:enumerateWithEstimate E111 Training  256/6754, done at 2023-12-11 15:13:54, 0:07:03\n",
      "2023-12-11 15:07:54,191 INFO     pid:22692 util:126:enumerateWithEstimate E111 Training 1024/6754, done at 2023-12-11 15:13:54, 0:07:03\n",
      "2023-12-11 15:11:06,287 INFO     pid:22692 util:126:enumerateWithEstimate E111 Training 4096/6754, done at 2023-12-11 15:13:52, 0:07:01\n",
      "2023-12-11 15:13:53,963 WARNING  pid:22692 util:139:enumerateWithEstimate E111 Training ----/6754, done at 2023-12-11 15:13:53\n",
      "2023-12-11 15:13:53,963 INFO     pid:22692 __main__:409:logMetrics E111 SegmentationTrainingApp\n",
      "2023-12-11 15:13:53,963 INFO     pid:22692 __main__:444:logMetrics E111 trn      0.2426 loss, 0.1479 fnloss, 0.9957 fploss, 0.8310 precision, 0.7961 recall, 0.8132 f1 score\n",
      "2023-12-11 15:13:53,963 INFO     pid:22692 __main__:458:logMetrics E111 trn_all  0.2426 loss,  79.6% tp,  20.4% fn,      16.2% fp\n",
      "2023-12-11 15:13:53,963 INFO     pid:22692 __main__:191:main Epoch 112 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:13:53,978 WARNING  pid:22692 util:109:enumerateWithEstimate E112 Training ----/6754, starting\n",
      "2023-12-11 15:14:37,084 INFO     pid:22692 util:126:enumerateWithEstimate E112 Training   64/6754, done at 2023-12-11 15:21:35, 0:07:01\n",
      "2023-12-11 15:14:49,161 INFO     pid:22692 util:126:enumerateWithEstimate E112 Training  256/6754, done at 2023-12-11 15:21:37, 0:07:03\n",
      "2023-12-11 15:15:37,362 INFO     pid:22692 util:126:enumerateWithEstimate E112 Training 1024/6754, done at 2023-12-11 15:21:37, 0:07:02\n",
      "2023-12-11 15:18:49,706 INFO     pid:22692 util:126:enumerateWithEstimate E112 Training 4096/6754, done at 2023-12-11 15:21:36, 0:07:02\n",
      "2023-12-11 15:21:37,443 WARNING  pid:22692 util:139:enumerateWithEstimate E112 Training ----/6754, done at 2023-12-11 15:21:37\n",
      "2023-12-11 15:21:37,443 INFO     pid:22692 __main__:409:logMetrics E112 SegmentationTrainingApp\n",
      "2023-12-11 15:21:37,443 INFO     pid:22692 __main__:444:logMetrics E112 trn      0.2383 loss, 0.1372 fnloss, 0.9958 fploss, 0.8315 precision, 0.7977 recall, 0.8143 f1 score\n",
      "2023-12-11 15:21:37,443 INFO     pid:22692 __main__:458:logMetrics E112 trn_all  0.2383 loss,  79.8% tp,  20.2% fn,      16.2% fp\n",
      "2023-12-11 15:21:37,443 INFO     pid:22692 __main__:191:main Epoch 113 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:21:37,443 WARNING  pid:22692 util:109:enumerateWithEstimate E113 Training ----/6754, starting\n",
      "2023-12-11 15:22:21,362 INFO     pid:22692 util:126:enumerateWithEstimate E113 Training   64/6754, done at 2023-12-11 15:29:21, 0:07:03\n",
      "2023-12-11 15:22:33,534 INFO     pid:22692 util:126:enumerateWithEstimate E113 Training  256/6754, done at 2023-12-11 15:29:24, 0:07:06\n",
      "2023-12-11 15:23:22,815 INFO     pid:22692 util:126:enumerateWithEstimate E113 Training 1024/6754, done at 2023-12-11 15:29:29, 0:07:10\n",
      "2023-12-11 15:26:37,660 INFO     pid:22692 util:126:enumerateWithEstimate E113 Training 4096/6754, done at 2023-12-11 15:29:26, 0:07:08\n",
      "2023-12-11 15:29:28,139 WARNING  pid:22692 util:139:enumerateWithEstimate E113 Training ----/6754, done at 2023-12-11 15:29:28\n",
      "2023-12-11 15:29:28,139 INFO     pid:22692 __main__:409:logMetrics E113 SegmentationTrainingApp\n",
      "2023-12-11 15:29:28,139 INFO     pid:22692 __main__:444:logMetrics E113 trn      0.2360 loss, 0.1385 fnloss, 0.9958 fploss, 0.8328 precision, 0.7993 recall, 0.8157 f1 score\n",
      "2023-12-11 15:29:28,139 INFO     pid:22692 __main__:458:logMetrics E113 trn_all  0.2360 loss,  79.9% tp,  20.1% fn,      16.0% fp\n",
      "2023-12-11 15:29:28,795 INFO     pid:22692 __main__:191:main Epoch 114 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:29:28,795 WARNING  pid:22692 util:109:enumerateWithEstimate E114 Training ----/6754, starting\n",
      "2023-12-11 15:30:17,762 INFO     pid:22692 util:126:enumerateWithEstimate E114 Training   64/6754, done at 2023-12-11 15:37:21, 0:07:06\n",
      "2023-12-11 15:30:30,063 INFO     pid:22692 util:126:enumerateWithEstimate E114 Training  256/6754, done at 2023-12-11 15:37:25, 0:07:10\n",
      "2023-12-11 15:31:18,764 INFO     pid:22692 util:126:enumerateWithEstimate E114 Training 1024/6754, done at 2023-12-11 15:37:22, 0:07:08\n",
      "2023-12-11 15:34:31,888 INFO     pid:22692 util:126:enumerateWithEstimate E114 Training 4096/6754, done at 2023-12-11 15:37:19, 0:07:04\n",
      "2023-12-11 15:37:20,730 WARNING  pid:22692 util:139:enumerateWithEstimate E114 Training ----/6754, done at 2023-12-11 15:37:20\n",
      "2023-12-11 15:37:20,730 INFO     pid:22692 __main__:409:logMetrics E114 SegmentationTrainingApp\n",
      "2023-12-11 15:37:20,730 INFO     pid:22692 __main__:444:logMetrics E114 trn      0.2389 loss, 0.1435 fnloss, 0.9957 fploss, 0.8341 precision, 0.7954 recall, 0.8143 f1 score\n",
      "2023-12-11 15:37:20,730 INFO     pid:22692 __main__:458:logMetrics E114 trn_all  0.2389 loss,  79.5% tp,  20.5% fn,      15.8% fp\n",
      "2023-12-11 15:37:21,476 INFO     pid:22692 __main__:191:main Epoch 115 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:37:21,491 WARNING  pid:22692 util:109:enumerateWithEstimate E115 Training ----/6754, starting\n",
      "2023-12-11 15:38:09,241 INFO     pid:22692 util:126:enumerateWithEstimate E115 Training   64/6754, done at 2023-12-11 15:45:13, 0:07:07\n",
      "2023-12-11 15:38:21,460 INFO     pid:22692 util:126:enumerateWithEstimate E115 Training  256/6754, done at 2023-12-11 15:45:14, 0:07:08\n",
      "2023-12-11 15:39:10,202 INFO     pid:22692 util:126:enumerateWithEstimate E115 Training 1024/6754, done at 2023-12-11 15:45:13, 0:07:07\n",
      "2023-12-11 15:42:24,082 INFO     pid:22692 util:126:enumerateWithEstimate E115 Training 4096/6754, done at 2023-12-11 15:45:12, 0:07:05\n",
      "2023-12-11 15:45:13,209 WARNING  pid:22692 util:139:enumerateWithEstimate E115 Training ----/6754, done at 2023-12-11 15:45:13\n",
      "2023-12-11 15:45:13,209 INFO     pid:22692 __main__:409:logMetrics E115 SegmentationTrainingApp\n",
      "2023-12-11 15:45:13,209 INFO     pid:22692 __main__:444:logMetrics E115 trn      0.2413 loss, 0.1441 fnloss, 0.9957 fploss, 0.8299 precision, 0.7976 recall, 0.8134 f1 score\n",
      "2023-12-11 15:45:13,209 INFO     pid:22692 __main__:458:logMetrics E115 trn_all  0.2413 loss,  79.8% tp,  20.2% fn,      16.3% fp\n",
      "2023-12-11 15:45:13,209 WARNING  pid:22692 util:109:enumerateWithEstimate E115 Validation  ----/862, starting\n",
      "2023-12-11 15:45:20,551 INFO     pid:22692 util:126:enumerateWithEstimate E115 Validation    64/862, done at 2023-12-11 15:45:43, 0:00:24\n",
      "2023-12-11 15:45:24,283 INFO     pid:22692 util:126:enumerateWithEstimate E115 Validation   256/862, done at 2023-12-11 15:45:37, 0:00:18\n",
      "2023-12-11 15:45:37,384 WARNING  pid:22692 util:139:enumerateWithEstimate E115 Validation  ----/862, done at 2023-12-11 15:45:37\n",
      "2023-12-11 15:45:37,384 INFO     pid:22692 __main__:409:logMetrics E115 SegmentationTrainingApp\n",
      "2023-12-11 15:45:37,384 INFO     pid:22692 __main__:444:logMetrics E115 val      0.3715 loss, 0.3232 fnloss, 0.9921 fploss, 0.8821 precision, 0.7436 recall, 0.8069 f1 score\n",
      "2023-12-11 15:45:37,384 INFO     pid:22692 __main__:458:logMetrics E115 val_all  0.3715 loss,  74.4% tp,  25.6% fn,       9.9% fp\n",
      "2023-12-11 15:45:37,511 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1553420.state\n",
      "2023-12-11 15:45:37,592 INFO     pid:22692 __main__:523:saveModel SHA1: a382329295634c1580e546b35fab19bd5422f2fd\n",
      "2023-12-11 15:46:33,840 INFO     pid:22692 __main__:191:main Epoch 116 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:46:33,840 WARNING  pid:22692 util:109:enumerateWithEstimate E116 Training ----/6754, starting\n",
      "2023-12-11 15:47:27,653 INFO     pid:22692 util:126:enumerateWithEstimate E116 Training   64/6754, done at 2023-12-11 15:54:26, 0:07:01\n",
      "2023-12-11 15:47:39,889 INFO     pid:22692 util:126:enumerateWithEstimate E116 Training  256/6754, done at 2023-12-11 15:54:32, 0:07:07\n",
      "2023-12-11 15:48:28,947 INFO     pid:22692 util:126:enumerateWithEstimate E116 Training 1024/6754, done at 2023-12-11 15:54:34, 0:07:09\n",
      "2023-12-11 15:51:45,182 INFO     pid:22692 util:126:enumerateWithEstimate E116 Training 4096/6754, done at 2023-12-11 15:54:34, 0:07:10\n",
      "2023-12-11 15:54:36,665 WARNING  pid:22692 util:139:enumerateWithEstimate E116 Training ----/6754, done at 2023-12-11 15:54:36\n",
      "2023-12-11 15:54:36,665 INFO     pid:22692 __main__:409:logMetrics E116 SegmentationTrainingApp\n",
      "2023-12-11 15:54:36,665 INFO     pid:22692 __main__:444:logMetrics E116 trn      0.2392 loss, 0.1382 fnloss, 0.9958 fploss, 0.8282 precision, 0.8003 recall, 0.8140 f1 score\n",
      "2023-12-11 15:54:36,665 INFO     pid:22692 __main__:458:logMetrics E116 trn_all  0.2392 loss,  80.0% tp,  20.0% fn,      16.6% fp\n",
      "2023-12-11 15:54:36,665 INFO     pid:22692 __main__:191:main Epoch 117 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 15:54:36,681 WARNING  pid:22692 util:109:enumerateWithEstimate E117 Training ----/6754, starting\n",
      "2023-12-11 15:55:21,901 INFO     pid:22692 util:126:enumerateWithEstimate E117 Training   64/6754, done at 2023-12-11 16:02:24, 0:07:05\n",
      "2023-12-11 15:55:34,056 INFO     pid:22692 util:126:enumerateWithEstimate E117 Training  256/6754, done at 2023-12-11 16:02:25, 0:07:06\n",
      "2023-12-11 15:56:22,191 INFO     pid:22692 util:126:enumerateWithEstimate E117 Training 1024/6754, done at 2023-12-11 16:02:22, 0:07:03\n",
      "2023-12-11 15:59:35,202 INFO     pid:22692 util:126:enumerateWithEstimate E117 Training 4096/6754, done at 2023-12-11 16:02:22, 0:07:03\n",
      "2023-12-11 16:02:23,372 WARNING  pid:22692 util:139:enumerateWithEstimate E117 Training ----/6754, done at 2023-12-11 16:02:23\n",
      "2023-12-11 16:02:23,372 INFO     pid:22692 __main__:409:logMetrics E117 SegmentationTrainingApp\n",
      "2023-12-11 16:02:23,372 INFO     pid:22692 __main__:444:logMetrics E117 trn      0.2359 loss, 0.1385 fnloss, 0.9958 fploss, 0.8372 precision, 0.7976 recall, 0.8169 f1 score\n",
      "2023-12-11 16:02:23,372 INFO     pid:22692 __main__:458:logMetrics E117 trn_all  0.2359 loss,  79.8% tp,  20.2% fn,      15.5% fp\n",
      "2023-12-11 16:02:23,372 INFO     pid:22692 __main__:191:main Epoch 118 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:02:23,388 WARNING  pid:22692 util:109:enumerateWithEstimate E118 Training ----/6754, starting\n",
      "2023-12-11 16:03:06,328 INFO     pid:22692 util:126:enumerateWithEstimate E118 Training   64/6754, done at 2023-12-11 16:10:08, 0:07:05\n",
      "2023-12-11 16:03:18,499 INFO     pid:22692 util:126:enumerateWithEstimate E118 Training  256/6754, done at 2023-12-11 16:10:09, 0:07:06\n",
      "2023-12-11 16:04:06,743 INFO     pid:22692 util:126:enumerateWithEstimate E118 Training 1024/6754, done at 2023-12-11 16:10:07, 0:07:04\n",
      "2023-12-11 16:07:19,548 INFO     pid:22692 util:126:enumerateWithEstimate E118 Training 4096/6754, done at 2023-12-11 16:10:06, 0:07:03\n",
      "2023-12-11 16:10:07,797 WARNING  pid:22692 util:139:enumerateWithEstimate E118 Training ----/6754, done at 2023-12-11 16:10:07\n",
      "2023-12-11 16:10:07,812 INFO     pid:22692 __main__:409:logMetrics E118 SegmentationTrainingApp\n",
      "2023-12-11 16:10:07,812 INFO     pid:22692 __main__:444:logMetrics E118 trn      0.2383 loss, 0.1463 fnloss, 0.9957 fploss, 0.8414 precision, 0.7954 recall, 0.8178 f1 score\n",
      "2023-12-11 16:10:07,812 INFO     pid:22692 __main__:458:logMetrics E118 trn_all  0.2383 loss,  79.5% tp,  20.5% fn,      15.0% fp\n",
      "2023-12-11 16:10:07,812 INFO     pid:22692 __main__:191:main Epoch 119 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:10:07,812 WARNING  pid:22692 util:109:enumerateWithEstimate E119 Training ----/6754, starting\n",
      "2023-12-11 16:10:51,183 INFO     pid:22692 util:126:enumerateWithEstimate E119 Training   64/6754, done at 2023-12-11 16:17:53, 0:07:05\n",
      "2023-12-11 16:11:03,291 INFO     pid:22692 util:126:enumerateWithEstimate E119 Training  256/6754, done at 2023-12-11 16:17:53, 0:07:05\n",
      "2023-12-11 16:11:51,630 INFO     pid:22692 util:126:enumerateWithEstimate E119 Training 1024/6754, done at 2023-12-11 16:17:52, 0:07:04\n",
      "2023-12-11 16:15:04,284 INFO     pid:22692 util:126:enumerateWithEstimate E119 Training 4096/6754, done at 2023-12-11 16:17:51, 0:07:02\n",
      "2023-12-11 16:17:52,782 WARNING  pid:22692 util:139:enumerateWithEstimate E119 Training ----/6754, done at 2023-12-11 16:17:52\n",
      "2023-12-11 16:17:52,782 INFO     pid:22692 __main__:409:logMetrics E119 SegmentationTrainingApp\n",
      "2023-12-11 16:17:52,782 INFO     pid:22692 __main__:444:logMetrics E119 trn      0.2367 loss, 0.1394 fnloss, 0.9958 fploss, 0.8328 precision, 0.8004 recall, 0.8163 f1 score\n",
      "2023-12-11 16:17:52,782 INFO     pid:22692 __main__:458:logMetrics E119 trn_all  0.2367 loss,  80.0% tp,  20.0% fn,      16.1% fp\n",
      "2023-12-11 16:17:52,798 INFO     pid:22692 __main__:191:main Epoch 120 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:17:52,798 WARNING  pid:22692 util:109:enumerateWithEstimate E120 Training ----/6754, starting\n",
      "2023-12-11 16:18:35,808 INFO     pid:22692 util:126:enumerateWithEstimate E120 Training   64/6754, done at 2023-12-11 16:25:38, 0:07:05\n",
      "2023-12-11 16:18:47,979 INFO     pid:22692 util:126:enumerateWithEstimate E120 Training  256/6754, done at 2023-12-11 16:25:39, 0:07:06\n",
      "2023-12-11 16:19:36,209 INFO     pid:22692 util:126:enumerateWithEstimate E120 Training 1024/6754, done at 2023-12-11 16:25:36, 0:07:04\n",
      "2023-12-11 16:22:49,245 INFO     pid:22692 util:126:enumerateWithEstimate E120 Training 4096/6754, done at 2023-12-11 16:25:36, 0:07:03\n",
      "2023-12-11 16:25:37,582 WARNING  pid:22692 util:139:enumerateWithEstimate E120 Training ----/6754, done at 2023-12-11 16:25:37\n",
      "2023-12-11 16:25:37,582 INFO     pid:22692 __main__:409:logMetrics E120 SegmentationTrainingApp\n",
      "2023-12-11 16:25:37,582 INFO     pid:22692 __main__:444:logMetrics E120 trn      0.2352 loss, 0.1387 fnloss, 0.9958 fploss, 0.8380 precision, 0.7940 recall, 0.8154 f1 score\n",
      "2023-12-11 16:25:37,582 INFO     pid:22692 __main__:458:logMetrics E120 trn_all  0.2352 loss,  79.4% tp,  20.6% fn,      15.3% fp\n",
      "2023-12-11 16:25:37,582 WARNING  pid:22692 util:109:enumerateWithEstimate E120 Validation  ----/862, starting\n",
      "2023-12-11 16:25:43,896 INFO     pid:22692 util:126:enumerateWithEstimate E120 Validation    64/862, done at 2023-12-11 16:25:58, 0:00:15\n",
      "2023-12-11 16:25:47,348 INFO     pid:22692 util:126:enumerateWithEstimate E120 Validation   256/862, done at 2023-12-11 16:25:58, 0:00:15\n",
      "2023-12-11 16:25:59,925 WARNING  pid:22692 util:139:enumerateWithEstimate E120 Validation  ----/862, done at 2023-12-11 16:25:59\n",
      "2023-12-11 16:25:59,925 INFO     pid:22692 __main__:409:logMetrics E120 SegmentationTrainingApp\n",
      "2023-12-11 16:25:59,925 INFO     pid:22692 __main__:444:logMetrics E120 val      0.3831 loss, 0.3215 fnloss, 0.9921 fploss, 0.8509 precision, 0.7555 recall, 0.8003 f1 score\n",
      "2023-12-11 16:25:59,925 INFO     pid:22692 __main__:458:logMetrics E120 val_all  0.3831 loss,  75.5% tp,  24.5% fn,      13.2% fp\n",
      "2023-12-11 16:26:00,050 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1620960.state\n",
      "2023-12-11 16:26:00,113 INFO     pid:22692 __main__:523:saveModel SHA1: e42e48b9ca229d2eb115a6cb357e0bd0ddcf79f3\n",
      "2023-12-11 16:26:47,076 INFO     pid:22692 __main__:191:main Epoch 121 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:26:47,092 WARNING  pid:22692 util:109:enumerateWithEstimate E121 Training ----/6754, starting\n",
      "2023-12-11 16:27:29,588 INFO     pid:22692 util:126:enumerateWithEstimate E121 Training   64/6754, done at 2023-12-11 16:34:27, 0:07:01\n",
      "2023-12-11 16:27:41,665 INFO     pid:22692 util:126:enumerateWithEstimate E121 Training  256/6754, done at 2023-12-11 16:34:29, 0:07:03\n",
      "2023-12-11 16:28:30,351 INFO     pid:22692 util:126:enumerateWithEstimate E121 Training 1024/6754, done at 2023-12-11 16:34:32, 0:07:06\n",
      "2023-12-11 16:31:43,196 INFO     pid:22692 util:126:enumerateWithEstimate E121 Training 4096/6754, done at 2023-12-11 16:34:30, 0:07:03\n",
      "2023-12-11 16:34:31,748 WARNING  pid:22692 util:139:enumerateWithEstimate E121 Training ----/6754, done at 2023-12-11 16:34:31\n",
      "2023-12-11 16:34:31,748 INFO     pid:22692 __main__:409:logMetrics E121 SegmentationTrainingApp\n",
      "2023-12-11 16:34:31,748 INFO     pid:22692 __main__:444:logMetrics E121 trn      0.2301 loss, 0.1328 fnloss, 0.9957 fploss, 0.8338 precision, 0.8047 recall, 0.8190 f1 score\n",
      "2023-12-11 16:34:31,748 INFO     pid:22692 __main__:458:logMetrics E121 trn_all  0.2301 loss,  80.5% tp,  19.5% fn,      16.0% fp\n",
      "2023-12-11 16:34:31,763 INFO     pid:22692 __main__:191:main Epoch 122 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:34:31,763 WARNING  pid:22692 util:109:enumerateWithEstimate E122 Training ----/6754, starting\n",
      "2023-12-11 16:35:14,838 INFO     pid:22692 util:126:enumerateWithEstimate E122 Training   64/6754, done at 2023-12-11 16:42:17, 0:07:05\n",
      "2023-12-11 16:35:26,978 INFO     pid:22692 util:126:enumerateWithEstimate E122 Training  256/6754, done at 2023-12-11 16:42:17, 0:07:05\n",
      "2023-12-11 16:36:15,505 INFO     pid:22692 util:126:enumerateWithEstimate E122 Training 1024/6754, done at 2023-12-11 16:42:17, 0:07:05\n",
      "2023-12-11 16:39:28,690 INFO     pid:22692 util:126:enumerateWithEstimate E122 Training 4096/6754, done at 2023-12-11 16:42:15, 0:07:04\n",
      "2023-12-11 16:42:20,893 WARNING  pid:22692 util:139:enumerateWithEstimate E122 Training ----/6754, done at 2023-12-11 16:42:20\n",
      "2023-12-11 16:42:20,893 INFO     pid:22692 __main__:409:logMetrics E122 SegmentationTrainingApp\n",
      "2023-12-11 16:42:20,893 INFO     pid:22692 __main__:444:logMetrics E122 trn      0.2342 loss, 0.1374 fnloss, 0.9958 fploss, 0.8337 precision, 0.8022 recall, 0.8177 f1 score\n",
      "2023-12-11 16:42:20,893 INFO     pid:22692 __main__:458:logMetrics E122 trn_all  0.2342 loss,  80.2% tp,  19.8% fn,      16.0% fp\n",
      "2023-12-11 16:42:20,893 INFO     pid:22692 __main__:191:main Epoch 123 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:42:20,893 WARNING  pid:22692 util:109:enumerateWithEstimate E123 Training ----/6754, starting\n",
      "2023-12-11 16:43:06,222 INFO     pid:22692 util:126:enumerateWithEstimate E123 Training   64/6754, done at 2023-12-11 16:50:21, 0:07:18\n",
      "2023-12-11 16:43:18,503 INFO     pid:22692 util:126:enumerateWithEstimate E123 Training  256/6754, done at 2023-12-11 16:50:15, 0:07:12\n",
      "2023-12-11 16:44:08,942 INFO     pid:22692 util:126:enumerateWithEstimate E123 Training 1024/6754, done at 2023-12-11 16:50:23, 0:07:20\n",
      "2023-12-11 16:47:29,893 INFO     pid:22692 util:126:enumerateWithEstimate E123 Training 4096/6754, done at 2023-12-11 16:50:23, 0:07:20\n",
      "2023-12-11 16:50:25,212 WARNING  pid:22692 util:139:enumerateWithEstimate E123 Training ----/6754, done at 2023-12-11 16:50:25\n",
      "2023-12-11 16:50:25,213 INFO     pid:22692 __main__:409:logMetrics E123 SegmentationTrainingApp\n",
      "2023-12-11 16:50:25,214 INFO     pid:22692 __main__:444:logMetrics E123 trn      0.2308 loss, 0.1349 fnloss, 0.9957 fploss, 0.8362 precision, 0.8071 recall, 0.8214 f1 score\n",
      "2023-12-11 16:50:25,214 INFO     pid:22692 __main__:458:logMetrics E123 trn_all  0.2308 loss,  80.7% tp,  19.3% fn,      15.8% fp\n",
      "2023-12-11 16:50:25,217 INFO     pid:22692 __main__:191:main Epoch 124 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:50:25,223 WARNING  pid:22692 util:109:enumerateWithEstimate E124 Training ----/6754, starting\n",
      "2023-12-11 16:51:14,894 INFO     pid:22692 util:126:enumerateWithEstimate E124 Training   64/6754, done at 2023-12-11 16:58:34, 0:07:22\n",
      "2023-12-11 16:51:27,544 INFO     pid:22692 util:126:enumerateWithEstimate E124 Training  256/6754, done at 2023-12-11 16:58:35, 0:07:23\n",
      "2023-12-11 16:52:17,497 INFO     pid:22692 util:126:enumerateWithEstimate E124 Training 1024/6754, done at 2023-12-11 16:58:31, 0:07:19\n",
      "2023-12-11 16:55:34,862 INFO     pid:22692 util:126:enumerateWithEstimate E124 Training 4096/6754, done at 2023-12-11 16:58:26, 0:07:14\n",
      "2023-12-11 16:58:25,449 WARNING  pid:22692 util:139:enumerateWithEstimate E124 Training ----/6754, done at 2023-12-11 16:58:25\n",
      "2023-12-11 16:58:25,451 INFO     pid:22692 __main__:409:logMetrics E124 SegmentationTrainingApp\n",
      "2023-12-11 16:58:25,451 INFO     pid:22692 __main__:444:logMetrics E124 trn      0.2327 loss, 0.1406 fnloss, 0.9958 fploss, 0.8396 precision, 0.8015 recall, 0.8201 f1 score\n",
      "2023-12-11 16:58:25,452 INFO     pid:22692 __main__:458:logMetrics E124 trn_all  0.2327 loss,  80.2% tp,  19.8% fn,      15.3% fp\n",
      "2023-12-11 16:58:25,455 INFO     pid:22692 __main__:191:main Epoch 125 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 16:58:25,462 WARNING  pid:22692 util:109:enumerateWithEstimate E125 Training ----/6754, starting\n",
      "2023-12-11 16:59:11,429 INFO     pid:22692 util:126:enumerateWithEstimate E125 Training   64/6754, done at 2023-12-11 17:06:15, 0:07:07\n",
      "2023-12-11 16:59:23,835 INFO     pid:22692 util:126:enumerateWithEstimate E125 Training  256/6754, done at 2023-12-11 17:06:22, 0:07:13\n",
      "2023-12-11 17:00:12,814 INFO     pid:22692 util:126:enumerateWithEstimate E125 Training 1024/6754, done at 2023-12-11 17:06:19, 0:07:10\n",
      "2023-12-11 17:03:26,307 INFO     pid:22692 util:126:enumerateWithEstimate E125 Training 4096/6754, done at 2023-12-11 17:06:14, 0:07:05\n",
      "2023-12-11 17:06:15,043 WARNING  pid:22692 util:139:enumerateWithEstimate E125 Training ----/6754, done at 2023-12-11 17:06:15\n",
      "2023-12-11 17:06:15,043 INFO     pid:22692 __main__:409:logMetrics E125 SegmentationTrainingApp\n",
      "2023-12-11 17:06:15,059 INFO     pid:22692 __main__:444:logMetrics E125 trn      0.2349 loss, 0.1375 fnloss, 0.9957 fploss, 0.8307 precision, 0.8022 recall, 0.8162 f1 score\n",
      "2023-12-11 17:06:15,059 INFO     pid:22692 __main__:458:logMetrics E125 trn_all  0.2349 loss,  80.2% tp,  19.8% fn,      16.3% fp\n",
      "2023-12-11 17:06:15,059 WARNING  pid:22692 util:109:enumerateWithEstimate E125 Validation  ----/862, starting\n",
      "2023-12-11 17:06:21,296 INFO     pid:22692 util:126:enumerateWithEstimate E125 Validation    64/862, done at 2023-12-11 17:06:35, 0:00:15\n",
      "2023-12-11 17:06:24,780 INFO     pid:22692 util:126:enumerateWithEstimate E125 Validation   256/862, done at 2023-12-11 17:06:35, 0:00:15\n",
      "2023-12-11 17:06:37,108 WARNING  pid:22692 util:139:enumerateWithEstimate E125 Validation  ----/862, done at 2023-12-11 17:06:37\n",
      "2023-12-11 17:06:37,108 INFO     pid:22692 __main__:409:logMetrics E125 SegmentationTrainingApp\n",
      "2023-12-11 17:06:37,108 INFO     pid:22692 __main__:444:logMetrics E125 val      0.3412 loss, 0.2678 fnloss, 0.9923 fploss, 0.8321 precision, 0.7421 recall, 0.7846 f1 score\n",
      "2023-12-11 17:06:37,108 INFO     pid:22692 __main__:458:logMetrics E125 val_all  0.3412 loss,  74.2% tp,  25.8% fn,      15.0% fp\n",
      "2023-12-11 17:06:37,233 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1688500.state\n",
      "2023-12-11 17:06:37,295 INFO     pid:22692 __main__:523:saveModel SHA1: 368550da8494faed4893d7448a514464de7b5ad9\n",
      "2023-12-11 17:07:25,138 INFO     pid:22692 __main__:191:main Epoch 126 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:07:25,138 WARNING  pid:22692 util:109:enumerateWithEstimate E126 Training ----/6754, starting\n",
      "2023-12-11 17:08:07,622 INFO     pid:22692 util:126:enumerateWithEstimate E126 Training   64/6754, done at 2023-12-11 17:15:05, 0:07:01\n",
      "2023-12-11 17:08:19,778 INFO     pid:22692 util:126:enumerateWithEstimate E126 Training  256/6754, done at 2023-12-11 17:15:10, 0:07:05\n",
      "2023-12-11 17:09:08,585 INFO     pid:22692 util:126:enumerateWithEstimate E126 Training 1024/6754, done at 2023-12-11 17:15:12, 0:07:07\n",
      "2023-12-11 17:12:22,970 INFO     pid:22692 util:126:enumerateWithEstimate E126 Training 4096/6754, done at 2023-12-11 17:15:11, 0:07:06\n",
      "2023-12-11 17:15:12,518 WARNING  pid:22692 util:139:enumerateWithEstimate E126 Training ----/6754, done at 2023-12-11 17:15:12\n",
      "2023-12-11 17:15:12,518 INFO     pid:22692 __main__:409:logMetrics E126 SegmentationTrainingApp\n",
      "2023-12-11 17:15:12,518 INFO     pid:22692 __main__:444:logMetrics E126 trn      0.2305 loss, 0.1346 fnloss, 0.9958 fploss, 0.8384 precision, 0.8019 recall, 0.8198 f1 score\n",
      "2023-12-11 17:15:12,518 INFO     pid:22692 __main__:458:logMetrics E126 trn_all  0.2305 loss,  80.2% tp,  19.8% fn,      15.5% fp\n",
      "2023-12-11 17:15:12,533 INFO     pid:22692 __main__:191:main Epoch 127 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:15:12,533 WARNING  pid:22692 util:109:enumerateWithEstimate E127 Training ----/6754, starting\n",
      "2023-12-11 17:15:55,669 INFO     pid:22692 util:126:enumerateWithEstimate E127 Training   64/6754, done at 2023-12-11 17:23:02, 0:07:09\n",
      "2023-12-11 17:16:07,887 INFO     pid:22692 util:126:enumerateWithEstimate E127 Training  256/6754, done at 2023-12-11 17:23:01, 0:07:08\n",
      "2023-12-11 17:16:56,694 INFO     pid:22692 util:126:enumerateWithEstimate E127 Training 1024/6754, done at 2023-12-11 17:23:00, 0:07:08\n",
      "2023-12-11 17:20:10,894 INFO     pid:22692 util:126:enumerateWithEstimate E127 Training 4096/6754, done at 2023-12-11 17:22:59, 0:07:06\n",
      "2023-12-11 17:23:00,579 WARNING  pid:22692 util:139:enumerateWithEstimate E127 Training ----/6754, done at 2023-12-11 17:23:00\n",
      "2023-12-11 17:23:00,579 INFO     pid:22692 __main__:409:logMetrics E127 SegmentationTrainingApp\n",
      "2023-12-11 17:23:00,579 INFO     pid:22692 __main__:444:logMetrics E127 trn      0.2318 loss, 0.1381 fnloss, 0.9957 fploss, 0.8380 precision, 0.8048 recall, 0.8211 f1 score\n",
      "2023-12-11 17:23:00,579 INFO     pid:22692 __main__:458:logMetrics E127 trn_all  0.2318 loss,  80.5% tp,  19.5% fn,      15.6% fp\n",
      "2023-12-11 17:23:00,579 INFO     pid:22692 __main__:191:main Epoch 128 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:23:00,595 WARNING  pid:22692 util:109:enumerateWithEstimate E128 Training ----/6754, starting\n",
      "2023-12-11 17:23:43,715 INFO     pid:22692 util:126:enumerateWithEstimate E128 Training   64/6754, done at 2023-12-11 17:30:48, 0:07:07\n",
      "2023-12-11 17:23:55,933 INFO     pid:22692 util:126:enumerateWithEstimate E128 Training  256/6754, done at 2023-12-11 17:30:49, 0:07:08\n",
      "2023-12-11 17:24:44,882 INFO     pid:22692 util:126:enumerateWithEstimate E128 Training 1024/6754, done at 2023-12-11 17:30:49, 0:07:09\n",
      "2023-12-11 17:27:59,035 INFO     pid:22692 util:126:enumerateWithEstimate E128 Training 4096/6754, done at 2023-12-11 17:30:47, 0:07:06\n",
      "2023-12-11 17:30:48,565 WARNING  pid:22692 util:139:enumerateWithEstimate E128 Training ----/6754, done at 2023-12-11 17:30:48\n",
      "2023-12-11 17:30:48,565 INFO     pid:22692 __main__:409:logMetrics E128 SegmentationTrainingApp\n",
      "2023-12-11 17:30:48,565 INFO     pid:22692 __main__:444:logMetrics E128 trn      0.2343 loss, 0.1401 fnloss, 0.9957 fploss, 0.8330 precision, 0.8039 recall, 0.8182 f1 score\n",
      "2023-12-11 17:30:48,565 INFO     pid:22692 __main__:458:logMetrics E128 trn_all  0.2343 loss,  80.4% tp,  19.6% fn,      16.1% fp\n",
      "2023-12-11 17:30:48,581 INFO     pid:22692 __main__:191:main Epoch 129 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:30:48,581 WARNING  pid:22692 util:109:enumerateWithEstimate E129 Training ----/6754, starting\n",
      "2023-12-11 17:31:31,563 INFO     pid:22692 util:126:enumerateWithEstimate E129 Training   64/6754, done at 2023-12-11 17:38:35, 0:07:07\n",
      "2023-12-11 17:31:43,765 INFO     pid:22692 util:126:enumerateWithEstimate E129 Training  256/6754, done at 2023-12-11 17:38:36, 0:07:08\n",
      "2023-12-11 17:32:32,667 INFO     pid:22692 util:126:enumerateWithEstimate E129 Training 1024/6754, done at 2023-12-11 17:38:37, 0:07:08\n",
      "2023-12-11 17:35:46,757 INFO     pid:22692 util:126:enumerateWithEstimate E129 Training 4096/6754, done at 2023-12-11 17:38:34, 0:07:06\n",
      "2023-12-11 17:38:35,961 WARNING  pid:22692 util:139:enumerateWithEstimate E129 Training ----/6754, done at 2023-12-11 17:38:35\n",
      "2023-12-11 17:38:35,961 INFO     pid:22692 __main__:409:logMetrics E129 SegmentationTrainingApp\n",
      "2023-12-11 17:38:35,961 INFO     pid:22692 __main__:444:logMetrics E129 trn      0.2351 loss, 0.1370 fnloss, 0.9957 fploss, 0.8310 precision, 0.8066 recall, 0.8186 f1 score\n",
      "2023-12-11 17:38:35,961 INFO     pid:22692 __main__:458:logMetrics E129 trn_all  0.2351 loss,  80.7% tp,  19.3% fn,      16.4% fp\n",
      "2023-12-11 17:38:35,961 INFO     pid:22692 __main__:191:main Epoch 130 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:38:35,977 WARNING  pid:22692 util:109:enumerateWithEstimate E130 Training ----/6754, starting\n",
      "2023-12-11 17:39:19,397 INFO     pid:22692 util:126:enumerateWithEstimate E130 Training   64/6754, done at 2023-12-11 17:46:21, 0:07:05\n",
      "2023-12-11 17:39:31,474 INFO     pid:22692 util:126:enumerateWithEstimate E130 Training  256/6754, done at 2023-12-11 17:46:20, 0:07:04\n",
      "2023-12-11 17:40:20,313 INFO     pid:22692 util:126:enumerateWithEstimate E130 Training 1024/6754, done at 2023-12-11 17:46:23, 0:07:07\n",
      "2023-12-11 17:43:36,404 INFO     pid:22692 util:126:enumerateWithEstimate E130 Training 4096/6754, done at 2023-12-11 17:46:25, 0:07:09\n",
      "2023-12-11 17:46:27,605 WARNING  pid:22692 util:139:enumerateWithEstimate E130 Training ----/6754, done at 2023-12-11 17:46:27\n",
      "2023-12-11 17:46:27,605 INFO     pid:22692 __main__:409:logMetrics E130 SegmentationTrainingApp\n",
      "2023-12-11 17:46:27,605 INFO     pid:22692 __main__:444:logMetrics E130 trn      0.2314 loss, 0.1344 fnloss, 0.9957 fploss, 0.8369 precision, 0.8050 recall, 0.8206 f1 score\n",
      "2023-12-11 17:46:27,605 INFO     pid:22692 __main__:458:logMetrics E130 trn_all  0.2314 loss,  80.5% tp,  19.5% fn,      15.7% fp\n",
      "2023-12-11 17:46:27,621 WARNING  pid:22692 util:109:enumerateWithEstimate E130 Validation  ----/862, starting\n",
      "2023-12-11 17:46:33,890 INFO     pid:22692 util:126:enumerateWithEstimate E130 Validation    64/862, done at 2023-12-11 17:46:48, 0:00:15\n",
      "2023-12-11 17:46:37,343 INFO     pid:22692 util:126:enumerateWithEstimate E130 Validation   256/862, done at 2023-12-11 17:46:48, 0:00:15\n",
      "2023-12-11 17:46:49,656 WARNING  pid:22692 util:139:enumerateWithEstimate E130 Validation  ----/862, done at 2023-12-11 17:46:49\n",
      "2023-12-11 17:46:49,656 INFO     pid:22692 __main__:409:logMetrics E130 SegmentationTrainingApp\n",
      "2023-12-11 17:46:49,671 INFO     pid:22692 __main__:444:logMetrics E130 val      0.3759 loss, 0.3088 fnloss, 0.9924 fploss, 0.8321 precision, 0.7566 recall, 0.7926 f1 score\n",
      "2023-12-11 17:46:49,671 INFO     pid:22692 __main__:458:logMetrics E130 val_all  0.3759 loss,  75.7% tp,  24.3% fn,      15.3% fp\n",
      "2023-12-11 17:46:49,796 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1756040.state\n",
      "2023-12-11 17:46:49,859 INFO     pid:22692 __main__:523:saveModel SHA1: 3f1086be9f93d804156b2f1c1f7efd0b5ca86855\n",
      "2023-12-11 17:47:36,187 INFO     pid:22692 __main__:191:main Epoch 131 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:47:36,187 WARNING  pid:22692 util:109:enumerateWithEstimate E131 Training ----/6754, starting\n",
      "2023-12-11 17:48:18,766 INFO     pid:22692 util:126:enumerateWithEstimate E131 Training   64/6754, done at 2023-12-11 17:55:16, 0:07:01\n",
      "2023-12-11 17:48:30,889 INFO     pid:22692 util:126:enumerateWithEstimate E131 Training  256/6754, done at 2023-12-11 17:55:20, 0:07:04\n",
      "2023-12-11 17:49:19,775 INFO     pid:22692 util:126:enumerateWithEstimate E131 Training 1024/6754, done at 2023-12-11 17:55:23, 0:07:07\n",
      "2023-12-11 17:52:35,928 INFO     pid:22692 util:126:enumerateWithEstimate E131 Training 4096/6754, done at 2023-12-11 17:55:25, 0:07:09\n",
      "2023-12-11 17:55:26,991 WARNING  pid:22692 util:139:enumerateWithEstimate E131 Training ----/6754, done at 2023-12-11 17:55:26\n",
      "2023-12-11 17:55:26,991 INFO     pid:22692 __main__:409:logMetrics E131 SegmentationTrainingApp\n",
      "2023-12-11 17:55:26,991 INFO     pid:22692 __main__:444:logMetrics E131 trn      0.2306 loss, 0.1411 fnloss, 0.9956 fploss, 0.8435 precision, 0.8024 recall, 0.8224 f1 score\n",
      "2023-12-11 17:55:26,991 INFO     pid:22692 __main__:458:logMetrics E131 trn_all  0.2306 loss,  80.2% tp,  19.8% fn,      14.9% fp\n",
      "2023-12-11 17:55:26,991 INFO     pid:22692 __main__:191:main Epoch 132 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 17:55:27,007 WARNING  pid:22692 util:109:enumerateWithEstimate E132 Training ----/6754, starting\n",
      "2023-12-11 17:56:10,050 INFO     pid:22692 util:126:enumerateWithEstimate E132 Training   64/6754, done at 2023-12-11 18:03:12, 0:07:05\n",
      "2023-12-11 17:56:22,236 INFO     pid:22692 util:126:enumerateWithEstimate E132 Training  256/6754, done at 2023-12-11 18:03:14, 0:07:07\n",
      "2023-12-11 17:57:11,123 INFO     pid:22692 util:126:enumerateWithEstimate E132 Training 1024/6754, done at 2023-12-11 18:03:15, 0:07:08\n",
      "2023-12-11 18:00:27,091 INFO     pid:22692 util:126:enumerateWithEstimate E132 Training 4096/6754, done at 2023-12-11 18:03:16, 0:07:09\n",
      "2023-12-11 18:03:16,575 WARNING  pid:22692 util:139:enumerateWithEstimate E132 Training ----/6754, done at 2023-12-11 18:03:16\n",
      "2023-12-11 18:03:16,591 INFO     pid:22692 __main__:409:logMetrics E132 SegmentationTrainingApp\n",
      "2023-12-11 18:03:16,591 INFO     pid:22692 __main__:444:logMetrics E132 trn      0.2308 loss, 0.1347 fnloss, 0.9957 fploss, 0.8351 precision, 0.8063 recall, 0.8204 f1 score\n",
      "2023-12-11 18:03:16,591 INFO     pid:22692 __main__:458:logMetrics E132 trn_all  0.2308 loss,  80.6% tp,  19.4% fn,      15.9% fp\n",
      "2023-12-11 18:03:16,591 INFO     pid:22692 __main__:191:main Epoch 133 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:03:16,591 WARNING  pid:22692 util:109:enumerateWithEstimate E133 Training ----/6754, starting\n",
      "2023-12-11 18:03:59,649 INFO     pid:22692 util:126:enumerateWithEstimate E133 Training   64/6754, done at 2023-12-11 18:11:04, 0:07:07\n",
      "2023-12-11 18:04:11,804 INFO     pid:22692 util:126:enumerateWithEstimate E133 Training  256/6754, done at 2023-12-11 18:11:03, 0:07:06\n",
      "2023-12-11 18:05:00,502 INFO     pid:22692 util:126:enumerateWithEstimate E133 Training 1024/6754, done at 2023-12-11 18:11:03, 0:07:07\n",
      "2023-12-11 18:08:13,970 INFO     pid:22692 util:126:enumerateWithEstimate E133 Training 4096/6754, done at 2023-12-11 18:11:01, 0:07:05\n",
      "2023-12-11 18:11:03,078 WARNING  pid:22692 util:139:enumerateWithEstimate E133 Training ----/6754, done at 2023-12-11 18:11:03\n",
      "2023-12-11 18:11:03,094 INFO     pid:22692 __main__:409:logMetrics E133 SegmentationTrainingApp\n",
      "2023-12-11 18:11:03,094 INFO     pid:22692 __main__:444:logMetrics E133 trn      0.2292 loss, 0.1345 fnloss, 0.9957 fploss, 0.8390 precision, 0.8028 recall, 0.8205 f1 score\n",
      "2023-12-11 18:11:03,094 INFO     pid:22692 __main__:458:logMetrics E133 trn_all  0.2292 loss,  80.3% tp,  19.7% fn,      15.4% fp\n",
      "2023-12-11 18:11:03,094 INFO     pid:22692 __main__:191:main Epoch 134 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:11:03,094 WARNING  pid:22692 util:109:enumerateWithEstimate E134 Training ----/6754, starting\n",
      "2023-12-11 18:11:46,246 INFO     pid:22692 util:126:enumerateWithEstimate E134 Training   64/6754, done at 2023-12-11 18:18:42, 0:06:58\n",
      "2023-12-11 18:11:58,416 INFO     pid:22692 util:126:enumerateWithEstimate E134 Training  256/6754, done at 2023-12-11 18:18:48, 0:07:05\n",
      "2023-12-11 18:12:46,865 INFO     pid:22692 util:126:enumerateWithEstimate E134 Training 1024/6754, done at 2023-12-11 18:18:48, 0:07:05\n",
      "2023-12-11 18:16:00,427 INFO     pid:22692 util:126:enumerateWithEstimate E134 Training 4096/6754, done at 2023-12-11 18:18:47, 0:07:04\n",
      "2023-12-11 18:18:49,367 WARNING  pid:22692 util:139:enumerateWithEstimate E134 Training ----/6754, done at 2023-12-11 18:18:49\n",
      "2023-12-11 18:18:49,367 INFO     pid:22692 __main__:409:logMetrics E134 SegmentationTrainingApp\n",
      "2023-12-11 18:18:49,367 INFO     pid:22692 __main__:444:logMetrics E134 trn      0.2316 loss, 0.1372 fnloss, 0.9957 fploss, 0.8393 precision, 0.8055 recall, 0.8220 f1 score\n",
      "2023-12-11 18:18:49,367 INFO     pid:22692 __main__:458:logMetrics E134 trn_all  0.2316 loss,  80.5% tp,  19.5% fn,      15.4% fp\n",
      "2023-12-11 18:18:49,367 INFO     pid:22692 __main__:191:main Epoch 135 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:18:49,382 WARNING  pid:22692 util:109:enumerateWithEstimate E135 Training ----/6754, starting\n",
      "2023-12-11 18:19:32,393 INFO     pid:22692 util:126:enumerateWithEstimate E135 Training   64/6754, done at 2023-12-11 18:26:36, 0:07:07\n",
      "2023-12-11 18:19:44,627 INFO     pid:22692 util:126:enumerateWithEstimate E135 Training  256/6754, done at 2023-12-11 18:26:38, 0:07:08\n",
      "2023-12-11 18:20:33,388 INFO     pid:22692 util:126:enumerateWithEstimate E135 Training 1024/6754, done at 2023-12-11 18:26:37, 0:07:08\n",
      "2023-12-11 18:23:47,109 INFO     pid:22692 util:126:enumerateWithEstimate E135 Training 4096/6754, done at 2023-12-11 18:26:34, 0:07:05\n",
      "2023-12-11 18:26:35,969 WARNING  pid:22692 util:139:enumerateWithEstimate E135 Training ----/6754, done at 2023-12-11 18:26:35\n",
      "2023-12-11 18:26:35,985 INFO     pid:22692 __main__:409:logMetrics E135 SegmentationTrainingApp\n",
      "2023-12-11 18:26:35,985 INFO     pid:22692 __main__:444:logMetrics E135 trn      0.2329 loss, 0.1361 fnloss, 0.9956 fploss, 0.8325 precision, 0.8072 recall, 0.8197 f1 score\n",
      "2023-12-11 18:26:35,985 INFO     pid:22692 __main__:458:logMetrics E135 trn_all  0.2329 loss,  80.7% tp,  19.3% fn,      16.2% fp\n",
      "2023-12-11 18:26:35,985 WARNING  pid:22692 util:109:enumerateWithEstimate E135 Validation  ----/862, starting\n",
      "2023-12-11 18:26:42,221 INFO     pid:22692 util:126:enumerateWithEstimate E135 Validation    64/862, done at 2023-12-11 18:26:56, 0:00:15\n",
      "2023-12-11 18:26:45,705 INFO     pid:22692 util:126:enumerateWithEstimate E135 Validation   256/862, done at 2023-12-11 18:26:56, 0:00:15\n",
      "2023-12-11 18:26:58,079 WARNING  pid:22692 util:139:enumerateWithEstimate E135 Validation  ----/862, done at 2023-12-11 18:26:58\n",
      "2023-12-11 18:26:58,079 INFO     pid:22692 __main__:409:logMetrics E135 SegmentationTrainingApp\n",
      "2023-12-11 18:26:58,079 INFO     pid:22692 __main__:444:logMetrics E135 val      0.3426 loss, 0.2559 fnloss, 0.9929 fploss, 0.8004 precision, 0.7788 recall, 0.7895 f1 score\n",
      "2023-12-11 18:26:58,079 INFO     pid:22692 __main__:458:logMetrics E135 val_all  0.3426 loss,  77.9% tp,  22.1% fn,      19.4% fp\n",
      "2023-12-11 18:26:58,204 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1823580.state\n",
      "2023-12-11 18:26:58,266 INFO     pid:22692 __main__:523:saveModel SHA1: 237c7267bd2c82d7d86542922e4bd6655fde9530\n",
      "2023-12-11 18:27:44,668 INFO     pid:22692 __main__:191:main Epoch 136 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:27:44,684 WARNING  pid:22692 util:109:enumerateWithEstimate E136 Training ----/6754, starting\n",
      "2023-12-11 18:28:27,585 INFO     pid:22692 util:126:enumerateWithEstimate E136 Training   64/6754, done at 2023-12-11 18:35:29, 0:07:05\n",
      "2023-12-11 18:28:39,693 INFO     pid:22692 util:126:enumerateWithEstimate E136 Training  256/6754, done at 2023-12-11 18:35:29, 0:07:05\n",
      "2023-12-11 18:29:28,704 INFO     pid:22692 util:126:enumerateWithEstimate E136 Training 1024/6754, done at 2023-12-11 18:35:33, 0:07:08\n",
      "2023-12-11 18:32:42,265 INFO     pid:22692 util:126:enumerateWithEstimate E136 Training 4096/6754, done at 2023-12-11 18:35:30, 0:07:05\n",
      "2023-12-11 18:35:31,378 WARNING  pid:22692 util:139:enumerateWithEstimate E136 Training ----/6754, done at 2023-12-11 18:35:31\n",
      "2023-12-11 18:35:31,378 INFO     pid:22692 __main__:409:logMetrics E136 SegmentationTrainingApp\n",
      "2023-12-11 18:35:31,394 INFO     pid:22692 __main__:444:logMetrics E136 trn      0.2273 loss, 0.1324 fnloss, 0.9957 fploss, 0.8356 precision, 0.8082 recall, 0.8217 f1 score\n",
      "2023-12-11 18:35:31,394 INFO     pid:22692 __main__:458:logMetrics E136 trn_all  0.2273 loss,  80.8% tp,  19.2% fn,      15.9% fp\n",
      "2023-12-11 18:35:31,394 INFO     pid:22692 __main__:191:main Epoch 137 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:35:31,394 WARNING  pid:22692 util:109:enumerateWithEstimate E137 Training ----/6754, starting\n",
      "2023-12-11 18:36:14,374 INFO     pid:22692 util:126:enumerateWithEstimate E137 Training   64/6754, done at 2023-12-11 18:43:16, 0:07:05\n",
      "2023-12-11 18:36:26,529 INFO     pid:22692 util:126:enumerateWithEstimate E137 Training  256/6754, done at 2023-12-11 18:43:17, 0:07:06\n",
      "2023-12-11 18:37:15,117 INFO     pid:22692 util:126:enumerateWithEstimate E137 Training 1024/6754, done at 2023-12-11 18:43:17, 0:07:06\n",
      "2023-12-11 18:40:28,678 INFO     pid:22692 util:126:enumerateWithEstimate E137 Training 4096/6754, done at 2023-12-11 18:43:16, 0:07:04\n",
      "2023-12-11 18:43:17,798 WARNING  pid:22692 util:139:enumerateWithEstimate E137 Training ----/6754, done at 2023-12-11 18:43:17\n",
      "2023-12-11 18:43:17,814 INFO     pid:22692 __main__:409:logMetrics E137 SegmentationTrainingApp\n",
      "2023-12-11 18:43:17,814 INFO     pid:22692 __main__:444:logMetrics E137 trn      0.2310 loss, 0.1382 fnloss, 0.9957 fploss, 0.8407 precision, 0.8056 recall, 0.8228 f1 score\n",
      "2023-12-11 18:43:17,814 INFO     pid:22692 __main__:458:logMetrics E137 trn_all  0.2310 loss,  80.6% tp,  19.4% fn,      15.3% fp\n",
      "2023-12-11 18:43:17,814 INFO     pid:22692 __main__:191:main Epoch 138 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:43:17,814 WARNING  pid:22692 util:109:enumerateWithEstimate E138 Training ----/6754, starting\n",
      "2023-12-11 18:44:00,887 INFO     pid:22692 util:126:enumerateWithEstimate E138 Training   64/6754, done at 2023-12-11 18:51:05, 0:07:07\n",
      "2023-12-11 18:44:13,121 INFO     pid:22692 util:126:enumerateWithEstimate E138 Training  256/6754, done at 2023-12-11 18:51:06, 0:07:08\n",
      "2023-12-11 18:45:01,978 INFO     pid:22692 util:126:enumerateWithEstimate E138 Training 1024/6754, done at 2023-12-11 18:51:06, 0:07:08\n",
      "2023-12-11 18:48:16,016 INFO     pid:22692 util:126:enumerateWithEstimate E138 Training 4096/6754, done at 2023-12-11 18:51:04, 0:07:06\n",
      "2023-12-11 18:51:05,008 WARNING  pid:22692 util:139:enumerateWithEstimate E138 Training ----/6754, done at 2023-12-11 18:51:05\n",
      "2023-12-11 18:51:05,008 INFO     pid:22692 __main__:409:logMetrics E138 SegmentationTrainingApp\n",
      "2023-12-11 18:51:05,024 INFO     pid:22692 __main__:444:logMetrics E138 trn      0.2302 loss, 0.1309 fnloss, 0.9958 fploss, 0.8325 precision, 0.8093 recall, 0.8208 f1 score\n",
      "2023-12-11 18:51:05,024 INFO     pid:22692 __main__:458:logMetrics E138 trn_all  0.2302 loss,  80.9% tp,  19.1% fn,      16.3% fp\n",
      "2023-12-11 18:51:05,024 INFO     pid:22692 __main__:191:main Epoch 139 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:51:05,024 WARNING  pid:22692 util:109:enumerateWithEstimate E139 Training ----/6754, starting\n",
      "2023-12-11 18:51:48,128 INFO     pid:22692 util:126:enumerateWithEstimate E139 Training   64/6754, done at 2023-12-11 18:58:54, 0:07:09\n",
      "2023-12-11 18:52:00,346 INFO     pid:22692 util:126:enumerateWithEstimate E139 Training  256/6754, done at 2023-12-11 18:58:53, 0:07:08\n",
      "2023-12-11 18:52:49,169 INFO     pid:22692 util:126:enumerateWithEstimate E139 Training 1024/6754, done at 2023-12-11 18:58:53, 0:07:08\n",
      "2023-12-11 18:56:02,942 INFO     pid:22692 util:126:enumerateWithEstimate E139 Training 4096/6754, done at 2023-12-11 18:58:50, 0:07:05\n",
      "2023-12-11 18:58:52,526 WARNING  pid:22692 util:139:enumerateWithEstimate E139 Training ----/6754, done at 2023-12-11 18:58:52\n",
      "2023-12-11 18:58:52,526 INFO     pid:22692 __main__:409:logMetrics E139 SegmentationTrainingApp\n",
      "2023-12-11 18:58:52,526 INFO     pid:22692 __main__:444:logMetrics E139 trn      0.2318 loss, 0.1410 fnloss, 0.9956 fploss, 0.8404 precision, 0.8012 recall, 0.8203 f1 score\n",
      "2023-12-11 18:58:52,526 INFO     pid:22692 __main__:458:logMetrics E139 trn_all  0.2318 loss,  80.1% tp,  19.9% fn,      15.2% fp\n",
      "2023-12-11 18:58:52,526 INFO     pid:22692 __main__:191:main Epoch 140 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 18:58:52,526 WARNING  pid:22692 util:109:enumerateWithEstimate E140 Training ----/6754, starting\n",
      "2023-12-11 18:59:35,569 INFO     pid:22692 util:126:enumerateWithEstimate E140 Training   64/6754, done at 2023-12-11 19:06:42, 0:07:09\n",
      "2023-12-11 18:59:47,802 INFO     pid:22692 util:126:enumerateWithEstimate E140 Training  256/6754, done at 2023-12-11 19:06:41, 0:07:09\n",
      "2023-12-11 19:00:36,562 INFO     pid:22692 util:126:enumerateWithEstimate E140 Training 1024/6754, done at 2023-12-11 19:06:40, 0:07:08\n",
      "2023-12-11 19:03:50,164 INFO     pid:22692 util:126:enumerateWithEstimate E140 Training 4096/6754, done at 2023-12-11 19:06:37, 0:07:05\n",
      "2023-12-11 19:06:39,031 WARNING  pid:22692 util:139:enumerateWithEstimate E140 Training ----/6754, done at 2023-12-11 19:06:39\n",
      "2023-12-11 19:06:39,031 INFO     pid:22692 __main__:409:logMetrics E140 SegmentationTrainingApp\n",
      "2023-12-11 19:06:39,031 INFO     pid:22692 __main__:444:logMetrics E140 trn      0.2286 loss, 0.1301 fnloss, 0.9958 fploss, 0.8361 precision, 0.8085 recall, 0.8221 f1 score\n",
      "2023-12-11 19:06:39,031 INFO     pid:22692 __main__:458:logMetrics E140 trn_all  0.2286 loss,  80.8% tp,  19.2% fn,      15.8% fp\n",
      "2023-12-11 19:06:39,031 WARNING  pid:22692 util:109:enumerateWithEstimate E140 Validation  ----/862, starting\n",
      "2023-12-11 19:06:45,207 INFO     pid:22692 util:126:enumerateWithEstimate E140 Validation    64/862, done at 2023-12-11 19:06:59, 0:00:15\n",
      "2023-12-11 19:06:48,660 INFO     pid:22692 util:126:enumerateWithEstimate E140 Validation   256/862, done at 2023-12-11 19:06:59, 0:00:15\n",
      "2023-12-11 19:07:00,956 WARNING  pid:22692 util:139:enumerateWithEstimate E140 Validation  ----/862, done at 2023-12-11 19:07:00\n",
      "2023-12-11 19:07:00,956 INFO     pid:22692 __main__:409:logMetrics E140 SegmentationTrainingApp\n",
      "2023-12-11 19:07:00,956 INFO     pid:22692 __main__:444:logMetrics E140 val      0.3670 loss, 0.3144 fnloss, 0.9917 fploss, 0.8498 precision, 0.7635 recall, 0.8043 f1 score\n",
      "2023-12-11 19:07:00,956 INFO     pid:22692 __main__:458:logMetrics E140 val_all  0.3670 loss,  76.3% tp,  23.7% fn,      13.5% fp\n",
      "2023-12-11 19:07:01,081 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1891120.state\n",
      "2023-12-11 19:07:01,144 INFO     pid:22692 __main__:523:saveModel SHA1: 8ed9c556e49c5be927160101b2e9528434769fb5\n",
      "2023-12-11 19:07:46,923 INFO     pid:22692 __main__:191:main Epoch 141 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:07:46,923 WARNING  pid:22692 util:109:enumerateWithEstimate E141 Training ----/6754, starting\n",
      "2023-12-11 19:08:29,341 INFO     pid:22692 util:126:enumerateWithEstimate E141 Training   64/6754, done at 2023-12-11 19:15:31, 0:07:05\n",
      "2023-12-11 19:08:41,558 INFO     pid:22692 util:126:enumerateWithEstimate E141 Training  256/6754, done at 2023-12-11 19:15:34, 0:07:08\n",
      "2023-12-11 19:09:30,603 INFO     pid:22692 util:126:enumerateWithEstimate E141 Training 1024/6754, done at 2023-12-11 19:15:36, 0:07:09\n",
      "2023-12-11 19:12:46,666 INFO     pid:22692 util:126:enumerateWithEstimate E141 Training 4096/6754, done at 2023-12-11 19:15:36, 0:07:09\n",
      "2023-12-11 19:15:37,975 WARNING  pid:22692 util:139:enumerateWithEstimate E141 Training ----/6754, done at 2023-12-11 19:15:37\n",
      "2023-12-11 19:15:37,975 INFO     pid:22692 __main__:409:logMetrics E141 SegmentationTrainingApp\n",
      "2023-12-11 19:15:37,975 INFO     pid:22692 __main__:444:logMetrics E141 trn      0.2318 loss, 0.1317 fnloss, 0.9958 fploss, 0.8299 precision, 0.8089 recall, 0.8192 f1 score\n",
      "2023-12-11 19:15:37,975 INFO     pid:22692 __main__:458:logMetrics E141 trn_all  0.2318 loss,  80.9% tp,  19.1% fn,      16.6% fp\n",
      "2023-12-11 19:15:37,975 INFO     pid:22692 __main__:191:main Epoch 142 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:15:37,990 WARNING  pid:22692 util:109:enumerateWithEstimate E142 Training ----/6754, starting\n",
      "2023-12-11 19:16:21,173 INFO     pid:22692 util:126:enumerateWithEstimate E142 Training   64/6754, done at 2023-12-11 19:23:25, 0:07:07\n",
      "2023-12-11 19:16:33,422 INFO     pid:22692 util:126:enumerateWithEstimate E142 Training  256/6754, done at 2023-12-11 19:23:27, 0:07:09\n",
      "2023-12-11 19:17:22,309 INFO     pid:22692 util:126:enumerateWithEstimate E142 Training 1024/6754, done at 2023-12-11 19:23:27, 0:07:09\n",
      "2023-12-11 19:20:35,688 INFO     pid:22692 util:126:enumerateWithEstimate E142 Training 4096/6754, done at 2023-12-11 19:23:23, 0:07:05\n",
      "2023-12-11 19:23:24,555 WARNING  pid:22692 util:139:enumerateWithEstimate E142 Training ----/6754, done at 2023-12-11 19:23:24\n",
      "2023-12-11 19:23:24,555 INFO     pid:22692 __main__:409:logMetrics E142 SegmentationTrainingApp\n",
      "2023-12-11 19:23:24,555 INFO     pid:22692 __main__:444:logMetrics E142 trn      0.2302 loss, 0.1309 fnloss, 0.9958 fploss, 0.8336 precision, 0.8093 recall, 0.8213 f1 score\n",
      "2023-12-11 19:23:24,555 INFO     pid:22692 __main__:458:logMetrics E142 trn_all  0.2302 loss,  80.9% tp,  19.1% fn,      16.1% fp\n",
      "2023-12-11 19:23:24,555 INFO     pid:22692 __main__:191:main Epoch 143 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:23:24,571 WARNING  pid:22692 util:109:enumerateWithEstimate E143 Training ----/6754, starting\n",
      "2023-12-11 19:24:07,613 INFO     pid:22692 util:126:enumerateWithEstimate E143 Training   64/6754, done at 2023-12-11 19:31:12, 0:07:07\n",
      "2023-12-11 19:24:19,785 INFO     pid:22692 util:126:enumerateWithEstimate E143 Training  256/6754, done at 2023-12-11 19:31:11, 0:07:07\n",
      "2023-12-11 19:25:08,267 INFO     pid:22692 util:126:enumerateWithEstimate E143 Training 1024/6754, done at 2023-12-11 19:31:10, 0:07:05\n",
      "2023-12-11 19:28:21,835 INFO     pid:22692 util:126:enumerateWithEstimate E143 Training 4096/6754, done at 2023-12-11 19:31:09, 0:07:04\n",
      "2023-12-11 19:31:10,934 WARNING  pid:22692 util:139:enumerateWithEstimate E143 Training ----/6754, done at 2023-12-11 19:31:10\n",
      "2023-12-11 19:31:10,934 INFO     pid:22692 __main__:409:logMetrics E143 SegmentationTrainingApp\n",
      "2023-12-11 19:31:10,934 INFO     pid:22692 __main__:444:logMetrics E143 trn      0.2308 loss, 0.1388 fnloss, 0.9957 fploss, 0.8383 precision, 0.8056 recall, 0.8216 f1 score\n",
      "2023-12-11 19:31:10,934 INFO     pid:22692 __main__:458:logMetrics E143 trn_all  0.2308 loss,  80.6% tp,  19.4% fn,      15.5% fp\n",
      "2023-12-11 19:31:10,934 INFO     pid:22692 __main__:191:main Epoch 144 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:31:10,934 WARNING  pid:22692 util:109:enumerateWithEstimate E144 Training ----/6754, starting\n",
      "2023-12-11 19:31:53,867 INFO     pid:22692 util:126:enumerateWithEstimate E144 Training   64/6754, done at 2023-12-11 19:38:54, 0:07:03\n",
      "2023-12-11 19:32:05,944 INFO     pid:22692 util:126:enumerateWithEstimate E144 Training  256/6754, done at 2023-12-11 19:38:54, 0:07:03\n",
      "2023-12-11 19:32:54,268 INFO     pid:22692 util:126:enumerateWithEstimate E144 Training 1024/6754, done at 2023-12-11 19:38:54, 0:07:03\n",
      "2023-12-11 19:36:07,323 INFO     pid:22692 util:126:enumerateWithEstimate E144 Training 4096/6754, done at 2023-12-11 19:38:54, 0:07:03\n",
      "2023-12-11 19:38:56,420 WARNING  pid:22692 util:139:enumerateWithEstimate E144 Training ----/6754, done at 2023-12-11 19:38:56\n",
      "2023-12-11 19:38:56,420 INFO     pid:22692 __main__:409:logMetrics E144 SegmentationTrainingApp\n",
      "2023-12-11 19:38:56,420 INFO     pid:22692 __main__:444:logMetrics E144 trn      0.2292 loss, 0.1354 fnloss, 0.9957 fploss, 0.8376 precision, 0.8057 recall, 0.8213 f1 score\n",
      "2023-12-11 19:38:56,420 INFO     pid:22692 __main__:458:logMetrics E144 trn_all  0.2292 loss,  80.6% tp,  19.4% fn,      15.6% fp\n",
      "2023-12-11 19:38:56,420 INFO     pid:22692 __main__:191:main Epoch 145 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:38:56,435 WARNING  pid:22692 util:109:enumerateWithEstimate E145 Training ----/6754, starting\n",
      "2023-12-11 19:39:39,766 INFO     pid:22692 util:126:enumerateWithEstimate E145 Training   64/6754, done at 2023-12-11 19:46:44, 0:07:07\n",
      "2023-12-11 19:39:51,952 INFO     pid:22692 util:126:enumerateWithEstimate E145 Training  256/6754, done at 2023-12-11 19:46:44, 0:07:07\n",
      "2023-12-11 19:40:40,839 INFO     pid:22692 util:126:enumerateWithEstimate E145 Training 1024/6754, done at 2023-12-11 19:46:45, 0:07:08\n",
      "2023-12-11 19:43:54,588 INFO     pid:22692 util:126:enumerateWithEstimate E145 Training 4096/6754, done at 2023-12-11 19:46:42, 0:07:05\n",
      "2023-12-11 19:46:44,174 WARNING  pid:22692 util:139:enumerateWithEstimate E145 Training ----/6754, done at 2023-12-11 19:46:44\n",
      "2023-12-11 19:46:44,174 INFO     pid:22692 __main__:409:logMetrics E145 SegmentationTrainingApp\n",
      "2023-12-11 19:46:44,174 INFO     pid:22692 __main__:444:logMetrics E145 trn      0.2284 loss, 0.1387 fnloss, 0.9956 fploss, 0.8474 precision, 0.8001 recall, 0.8231 f1 score\n",
      "2023-12-11 19:46:44,174 INFO     pid:22692 __main__:458:logMetrics E145 trn_all  0.2284 loss,  80.0% tp,  20.0% fn,      14.4% fp\n",
      "2023-12-11 19:46:44,174 WARNING  pid:22692 util:109:enumerateWithEstimate E145 Validation  ----/862, starting\n",
      "2023-12-11 19:46:50,422 INFO     pid:22692 util:126:enumerateWithEstimate E145 Validation    64/862, done at 2023-12-11 19:47:04, 0:00:15\n",
      "2023-12-11 19:46:53,859 INFO     pid:22692 util:126:enumerateWithEstimate E145 Validation   256/862, done at 2023-12-11 19:47:04, 0:00:15\n",
      "2023-12-11 19:47:06,155 WARNING  pid:22692 util:139:enumerateWithEstimate E145 Validation  ----/862, done at 2023-12-11 19:47:06\n",
      "2023-12-11 19:47:06,155 INFO     pid:22692 __main__:409:logMetrics E145 SegmentationTrainingApp\n",
      "2023-12-11 19:47:06,155 INFO     pid:22692 __main__:444:logMetrics E145 val      0.3770 loss, 0.3276 fnloss, 0.9919 fploss, 0.8862 precision, 0.7630 recall, 0.8200 f1 score\n",
      "2023-12-11 19:47:06,155 INFO     pid:22692 __main__:458:logMetrics E145 val_all  0.3770 loss,  76.3% tp,  23.7% fn,       9.8% fp\n",
      "2023-12-11 19:47:06,280 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.1958660.state\n",
      "2023-12-11 19:47:06,327 INFO     pid:22692 __main__:523:saveModel SHA1: 6b003fb8381bbd600d523c0b1b2efc8b3a173cab\n",
      "2023-12-11 19:47:52,542 INFO     pid:22692 __main__:191:main Epoch 146 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:47:52,542 WARNING  pid:22692 util:109:enumerateWithEstimate E146 Training ----/6754, starting\n",
      "2023-12-11 19:48:35,132 INFO     pid:22692 util:126:enumerateWithEstimate E146 Training   64/6754, done at 2023-12-11 19:55:37, 0:07:05\n",
      "2023-12-11 19:48:47,272 INFO     pid:22692 util:126:enumerateWithEstimate E146 Training  256/6754, done at 2023-12-11 19:55:37, 0:07:05\n",
      "2023-12-11 19:49:36,161 INFO     pid:22692 util:126:enumerateWithEstimate E146 Training 1024/6754, done at 2023-12-11 19:55:40, 0:07:08\n",
      "2023-12-11 19:52:49,397 INFO     pid:22692 util:126:enumerateWithEstimate E146 Training 4096/6754, done at 2023-12-11 19:55:36, 0:07:04\n",
      "2023-12-11 19:55:37,574 WARNING  pid:22692 util:139:enumerateWithEstimate E146 Training ----/6754, done at 2023-12-11 19:55:37\n",
      "2023-12-11 19:55:37,574 INFO     pid:22692 __main__:409:logMetrics E146 SegmentationTrainingApp\n",
      "2023-12-11 19:55:37,574 INFO     pid:22692 __main__:444:logMetrics E146 trn      0.2307 loss, 0.1443 fnloss, 0.9956 fploss, 0.8494 precision, 0.7931 recall, 0.8203 f1 score\n",
      "2023-12-11 19:55:37,574 INFO     pid:22692 __main__:458:logMetrics E146 trn_all  0.2307 loss,  79.3% tp,  20.7% fn,      14.1% fp\n",
      "2023-12-11 19:55:37,574 INFO     pid:22692 __main__:191:main Epoch 147 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 19:55:37,590 WARNING  pid:22692 util:109:enumerateWithEstimate E147 Training ----/6754, starting\n",
      "2023-12-11 19:56:21,023 INFO     pid:22692 util:126:enumerateWithEstimate E147 Training   64/6754, done at 2023-12-11 20:03:25, 0:07:07\n",
      "2023-12-11 19:56:33,303 INFO     pid:22692 util:126:enumerateWithEstimate E147 Training  256/6754, done at 2023-12-11 20:03:28, 0:07:10\n",
      "2023-12-11 19:57:22,157 INFO     pid:22692 util:126:enumerateWithEstimate E147 Training 1024/6754, done at 2023-12-11 20:03:26, 0:07:09\n",
      "2023-12-11 20:00:35,472 INFO     pid:22692 util:126:enumerateWithEstimate E147 Training 4096/6754, done at 2023-12-11 20:03:23, 0:07:05\n",
      "2023-12-11 20:03:24,038 WARNING  pid:22692 util:139:enumerateWithEstimate E147 Training ----/6754, done at 2023-12-11 20:03:24\n",
      "2023-12-11 20:03:24,038 INFO     pid:22692 __main__:409:logMetrics E147 SegmentationTrainingApp\n",
      "2023-12-11 20:03:24,038 INFO     pid:22692 __main__:444:logMetrics E147 trn      0.2293 loss, 0.1300 fnloss, 0.9958 fploss, 0.8353 precision, 0.8099 recall, 0.8224 f1 score\n",
      "2023-12-11 20:03:24,038 INFO     pid:22692 __main__:458:logMetrics E147 trn_all  0.2293 loss,  81.0% tp,  19.0% fn,      16.0% fp\n",
      "2023-12-11 20:03:24,038 INFO     pid:22692 __main__:191:main Epoch 148 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:03:24,054 WARNING  pid:22692 util:109:enumerateWithEstimate E148 Training ----/6754, starting\n",
      "2023-12-11 20:04:06,831 INFO     pid:22692 util:126:enumerateWithEstimate E148 Training   64/6754, done at 2023-12-11 20:11:06, 0:07:03\n",
      "2023-12-11 20:04:18,986 INFO     pid:22692 util:126:enumerateWithEstimate E148 Training  256/6754, done at 2023-12-11 20:11:09, 0:07:05\n",
      "2023-12-11 20:05:07,734 INFO     pid:22692 util:126:enumerateWithEstimate E148 Training 1024/6754, done at 2023-12-11 20:11:11, 0:07:07\n",
      "2023-12-11 20:08:21,014 INFO     pid:22692 util:126:enumerateWithEstimate E148 Training 4096/6754, done at 2023-12-11 20:11:08, 0:07:04\n",
      "2023-12-11 20:11:09,783 WARNING  pid:22692 util:139:enumerateWithEstimate E148 Training ----/6754, done at 2023-12-11 20:11:09\n",
      "2023-12-11 20:11:09,783 INFO     pid:22692 __main__:409:logMetrics E148 SegmentationTrainingApp\n",
      "2023-12-11 20:11:09,783 INFO     pid:22692 __main__:444:logMetrics E148 trn      0.2281 loss, 0.1402 fnloss, 0.9957 fploss, 0.8474 precision, 0.8009 recall, 0.8235 f1 score\n",
      "2023-12-11 20:11:09,783 INFO     pid:22692 __main__:458:logMetrics E148 trn_all  0.2281 loss,  80.1% tp,  19.9% fn,      14.4% fp\n",
      "2023-12-11 20:11:09,798 INFO     pid:22692 __main__:191:main Epoch 149 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:11:09,798 WARNING  pid:22692 util:109:enumerateWithEstimate E149 Training ----/6754, starting\n",
      "2023-12-11 20:11:52,888 INFO     pid:22692 util:126:enumerateWithEstimate E149 Training   64/6754, done at 2023-12-11 20:18:55, 0:07:05\n",
      "2023-12-11 20:12:05,074 INFO     pid:22692 util:126:enumerateWithEstimate E149 Training  256/6754, done at 2023-12-11 20:18:57, 0:07:07\n",
      "2023-12-11 20:12:53,852 INFO     pid:22692 util:126:enumerateWithEstimate E149 Training 1024/6754, done at 2023-12-11 20:18:57, 0:07:07\n",
      "2023-12-11 20:16:07,447 INFO     pid:22692 util:126:enumerateWithEstimate E149 Training 4096/6754, done at 2023-12-11 20:18:55, 0:07:05\n",
      "2023-12-11 20:18:56,496 WARNING  pid:22692 util:139:enumerateWithEstimate E149 Training ----/6754, done at 2023-12-11 20:18:56\n",
      "2023-12-11 20:18:56,496 INFO     pid:22692 __main__:409:logMetrics E149 SegmentationTrainingApp\n",
      "2023-12-11 20:18:56,496 INFO     pid:22692 __main__:444:logMetrics E149 trn      0.2304 loss, 0.1355 fnloss, 0.9957 fploss, 0.8391 precision, 0.8058 recall, 0.8221 f1 score\n",
      "2023-12-11 20:18:56,496 INFO     pid:22692 __main__:458:logMetrics E149 trn_all  0.2304 loss,  80.6% tp,  19.4% fn,      15.5% fp\n",
      "2023-12-11 20:18:56,496 INFO     pid:22692 __main__:191:main Epoch 150 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:18:56,511 WARNING  pid:22692 util:109:enumerateWithEstimate E150 Training ----/6754, starting\n",
      "2023-12-11 20:19:39,335 INFO     pid:22692 util:126:enumerateWithEstimate E150 Training   64/6754, done at 2023-12-11 20:26:45, 0:07:09\n",
      "2023-12-11 20:19:51,506 INFO     pid:22692 util:126:enumerateWithEstimate E150 Training  256/6754, done at 2023-12-11 20:26:43, 0:07:07\n",
      "2023-12-11 20:20:39,690 INFO     pid:22692 util:126:enumerateWithEstimate E150 Training 1024/6754, done at 2023-12-11 20:26:40, 0:07:03\n",
      "2023-12-11 20:23:53,156 INFO     pid:22692 util:126:enumerateWithEstimate E150 Training 4096/6754, done at 2023-12-11 20:26:40, 0:07:04\n",
      "2023-12-11 20:26:42,018 WARNING  pid:22692 util:139:enumerateWithEstimate E150 Training ----/6754, done at 2023-12-11 20:26:42\n",
      "2023-12-11 20:26:42,033 INFO     pid:22692 __main__:409:logMetrics E150 SegmentationTrainingApp\n",
      "2023-12-11 20:26:42,033 INFO     pid:22692 __main__:444:logMetrics E150 trn      0.2241 loss, 0.1313 fnloss, 0.9957 fploss, 0.8426 precision, 0.8112 recall, 0.8266 f1 score\n",
      "2023-12-11 20:26:42,033 INFO     pid:22692 __main__:458:logMetrics E150 trn_all  0.2241 loss,  81.1% tp,  18.9% fn,      15.2% fp\n",
      "2023-12-11 20:26:42,033 WARNING  pid:22692 util:109:enumerateWithEstimate E150 Validation  ----/862, starting\n",
      "2023-12-11 20:26:48,585 INFO     pid:22692 util:126:enumerateWithEstimate E150 Validation    64/862, done at 2023-12-11 20:27:02, 0:00:15\n",
      "2023-12-11 20:26:52,053 INFO     pid:22692 util:126:enumerateWithEstimate E150 Validation   256/862, done at 2023-12-11 20:27:02, 0:00:15\n",
      "2023-12-11 20:27:04,427 WARNING  pid:22692 util:139:enumerateWithEstimate E150 Validation  ----/862, done at 2023-12-11 20:27:04\n",
      "2023-12-11 20:27:04,427 INFO     pid:22692 __main__:409:logMetrics E150 SegmentationTrainingApp\n",
      "2023-12-11 20:27:04,427 INFO     pid:22692 __main__:444:logMetrics E150 val      0.3618 loss, 0.3010 fnloss, 0.9920 fploss, 0.8462 precision, 0.7542 recall, 0.7976 f1 score\n",
      "2023-12-11 20:27:04,427 INFO     pid:22692 __main__:458:logMetrics E150 val_all  0.3618 loss,  75.4% tp,  24.6% fn,      13.7% fp\n",
      "2023-12-11 20:27:04,552 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.2026200.state\n",
      "2023-12-11 20:27:04,615 INFO     pid:22692 __main__:523:saveModel SHA1: d58747ad64938f15cf27f43f7859eb575a705a3b\n",
      "2023-12-11 20:27:52,454 INFO     pid:22692 __main__:191:main Epoch 151 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:27:52,454 WARNING  pid:22692 util:109:enumerateWithEstimate E151 Training ----/6754, starting\n",
      "2023-12-11 20:28:35,466 INFO     pid:22692 util:126:enumerateWithEstimate E151 Training   64/6754, done at 2023-12-11 20:35:35, 0:07:03\n",
      "2023-12-11 20:28:47,527 INFO     pid:22692 util:126:enumerateWithEstimate E151 Training  256/6754, done at 2023-12-11 20:35:35, 0:07:03\n",
      "2023-12-11 20:29:36,462 INFO     pid:22692 util:126:enumerateWithEstimate E151 Training 1024/6754, done at 2023-12-11 20:35:40, 0:07:07\n",
      "2023-12-11 20:32:49,866 INFO     pid:22692 util:126:enumerateWithEstimate E151 Training 4096/6754, done at 2023-12-11 20:35:37, 0:07:05\n",
      "2023-12-11 20:35:38,714 WARNING  pid:22692 util:139:enumerateWithEstimate E151 Training ----/6754, done at 2023-12-11 20:35:38\n",
      "2023-12-11 20:35:38,714 INFO     pid:22692 __main__:409:logMetrics E151 SegmentationTrainingApp\n",
      "2023-12-11 20:35:38,714 INFO     pid:22692 __main__:444:logMetrics E151 trn      0.2236 loss, 0.1300 fnloss, 0.9958 fploss, 0.8472 precision, 0.8093 recall, 0.8278 f1 score\n",
      "2023-12-11 20:35:38,714 INFO     pid:22692 __main__:458:logMetrics E151 trn_all  0.2236 loss,  80.9% tp,  19.1% fn,      14.6% fp\n",
      "2023-12-11 20:35:38,729 INFO     pid:22692 __main__:191:main Epoch 152 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:35:38,729 WARNING  pid:22692 util:109:enumerateWithEstimate E152 Training ----/6754, starting\n",
      "2023-12-11 20:36:21,584 INFO     pid:22692 util:126:enumerateWithEstimate E152 Training   64/6754, done at 2023-12-11 20:43:23, 0:07:05\n",
      "2023-12-11 20:36:33,817 INFO     pid:22692 util:126:enumerateWithEstimate E152 Training  256/6754, done at 2023-12-11 20:43:27, 0:07:08\n",
      "2023-12-11 20:37:22,251 INFO     pid:22692 util:126:enumerateWithEstimate E152 Training 1024/6754, done at 2023-12-11 20:43:24, 0:07:05\n",
      "2023-12-11 20:40:35,867 INFO     pid:22692 util:126:enumerateWithEstimate E152 Training 4096/6754, done at 2023-12-11 20:43:23, 0:07:04\n",
      "2023-12-11 20:43:24,898 WARNING  pid:22692 util:139:enumerateWithEstimate E152 Training ----/6754, done at 2023-12-11 20:43:24\n",
      "2023-12-11 20:43:24,898 INFO     pid:22692 __main__:409:logMetrics E152 SegmentationTrainingApp\n",
      "2023-12-11 20:43:24,898 INFO     pid:22692 __main__:444:logMetrics E152 trn      0.2283 loss, 0.1366 fnloss, 0.9956 fploss, 0.8429 precision, 0.8044 recall, 0.8232 f1 score\n",
      "2023-12-11 20:43:24,898 INFO     pid:22692 __main__:458:logMetrics E152 trn_all  0.2283 loss,  80.4% tp,  19.6% fn,      15.0% fp\n",
      "2023-12-11 20:43:24,898 INFO     pid:22692 __main__:191:main Epoch 153 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:43:24,914 WARNING  pid:22692 util:109:enumerateWithEstimate E153 Training ----/6754, starting\n",
      "2023-12-11 20:44:07,784 INFO     pid:22692 util:126:enumerateWithEstimate E153 Training   64/6754, done at 2023-12-11 20:51:10, 0:07:05\n",
      "2023-12-11 20:44:19,862 INFO     pid:22692 util:126:enumerateWithEstimate E153 Training  256/6754, done at 2023-12-11 20:51:08, 0:07:04\n",
      "2023-12-11 20:45:08,607 INFO     pid:22692 util:126:enumerateWithEstimate E153 Training 1024/6754, done at 2023-12-11 20:51:11, 0:07:06\n",
      "2023-12-11 20:48:21,782 INFO     pid:22692 util:126:enumerateWithEstimate E153 Training 4096/6754, done at 2023-12-11 20:51:09, 0:07:04\n",
      "2023-12-11 20:51:10,687 WARNING  pid:22692 util:139:enumerateWithEstimate E153 Training ----/6754, done at 2023-12-11 20:51:10\n",
      "2023-12-11 20:51:10,687 INFO     pid:22692 __main__:409:logMetrics E153 SegmentationTrainingApp\n",
      "2023-12-11 20:51:10,687 INFO     pid:22692 __main__:444:logMetrics E153 trn      0.2265 loss, 0.1302 fnloss, 0.9957 fploss, 0.8392 precision, 0.8113 recall, 0.8250 f1 score\n",
      "2023-12-11 20:51:10,687 INFO     pid:22692 __main__:458:logMetrics E153 trn_all  0.2265 loss,  81.1% tp,  18.9% fn,      15.5% fp\n",
      "2023-12-11 20:51:10,703 INFO     pid:22692 __main__:191:main Epoch 154 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:51:10,703 WARNING  pid:22692 util:109:enumerateWithEstimate E154 Training ----/6754, starting\n",
      "2023-12-11 20:51:53,606 INFO     pid:22692 util:126:enumerateWithEstimate E154 Training   64/6754, done at 2023-12-11 20:58:55, 0:07:05\n",
      "2023-12-11 20:52:05,792 INFO     pid:22692 util:126:enumerateWithEstimate E154 Training  256/6754, done at 2023-12-11 20:58:57, 0:07:07\n",
      "2023-12-11 20:52:54,537 INFO     pid:22692 util:126:enumerateWithEstimate E154 Training 1024/6754, done at 2023-12-11 20:58:58, 0:07:07\n",
      "2023-12-11 20:56:07,772 INFO     pid:22692 util:126:enumerateWithEstimate E154 Training 4096/6754, done at 2023-12-11 20:58:55, 0:07:04\n",
      "2023-12-11 20:58:57,054 WARNING  pid:22692 util:139:enumerateWithEstimate E154 Training ----/6754, done at 2023-12-11 20:58:57\n",
      "2023-12-11 20:58:57,054 INFO     pid:22692 __main__:409:logMetrics E154 SegmentationTrainingApp\n",
      "2023-12-11 20:58:57,054 INFO     pid:22692 __main__:444:logMetrics E154 trn      0.2271 loss, 0.1333 fnloss, 0.9957 fploss, 0.8400 precision, 0.8111 recall, 0.8253 f1 score\n",
      "2023-12-11 20:58:57,054 INFO     pid:22692 __main__:458:logMetrics E154 trn_all  0.2271 loss,  81.1% tp,  18.9% fn,      15.5% fp\n",
      "2023-12-11 20:58:57,054 INFO     pid:22692 __main__:191:main Epoch 155 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 20:58:57,070 WARNING  pid:22692 util:109:enumerateWithEstimate E155 Training ----/6754, starting\n",
      "2023-12-11 20:59:39,909 INFO     pid:22692 util:126:enumerateWithEstimate E155 Training   64/6754, done at 2023-12-11 21:06:44, 0:07:07\n",
      "2023-12-11 20:59:52,113 INFO     pid:22692 util:126:enumerateWithEstimate E155 Training  256/6754, done at 2023-12-11 21:06:44, 0:07:08\n",
      "2023-12-11 21:00:40,796 INFO     pid:22692 util:126:enumerateWithEstimate E155 Training 1024/6754, done at 2023-12-11 21:06:44, 0:07:07\n",
      "2023-12-11 21:03:54,265 INFO     pid:22692 util:126:enumerateWithEstimate E155 Training 4096/6754, done at 2023-12-11 21:06:41, 0:07:05\n",
      "2023-12-11 21:06:43,092 WARNING  pid:22692 util:139:enumerateWithEstimate E155 Training ----/6754, done at 2023-12-11 21:06:43\n",
      "2023-12-11 21:06:43,092 INFO     pid:22692 __main__:409:logMetrics E155 SegmentationTrainingApp\n",
      "2023-12-11 21:06:43,092 INFO     pid:22692 __main__:444:logMetrics E155 trn      0.2271 loss, 0.1361 fnloss, 0.9956 fploss, 0.8422 precision, 0.8062 recall, 0.8238 f1 score\n",
      "2023-12-11 21:06:43,092 INFO     pid:22692 __main__:458:logMetrics E155 trn_all  0.2271 loss,  80.6% tp,  19.4% fn,      15.1% fp\n",
      "2023-12-11 21:06:43,092 WARNING  pid:22692 util:109:enumerateWithEstimate E155 Validation  ----/862, starting\n",
      "2023-12-11 21:06:49,252 INFO     pid:22692 util:126:enumerateWithEstimate E155 Validation    64/862, done at 2023-12-11 21:07:03, 0:00:15\n",
      "2023-12-11 21:06:52,705 INFO     pid:22692 util:126:enumerateWithEstimate E155 Validation   256/862, done at 2023-12-11 21:07:03, 0:00:15\n",
      "2023-12-11 21:07:05,110 WARNING  pid:22692 util:139:enumerateWithEstimate E155 Validation  ----/862, done at 2023-12-11 21:07:05\n",
      "2023-12-11 21:07:05,126 INFO     pid:22692 __main__:409:logMetrics E155 SegmentationTrainingApp\n",
      "2023-12-11 21:07:05,126 INFO     pid:22692 __main__:444:logMetrics E155 val      0.3879 loss, 0.3477 fnloss, 0.9916 fploss, 0.8871 precision, 0.7142 recall, 0.7913 f1 score\n",
      "2023-12-11 21:07:05,126 INFO     pid:22692 __main__:458:logMetrics E155 val_all  0.3879 loss,  71.4% tp,  28.6% fn,       9.1% fp\n",
      "2023-12-11 21:07:05,235 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.2093740.state\n",
      "2023-12-11 21:07:05,298 INFO     pid:22692 __main__:523:saveModel SHA1: 887bc119208ccee0e32549de827e5d6124ff3e23\n",
      "2023-12-11 21:07:51,528 INFO     pid:22692 __main__:191:main Epoch 156 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:07:51,528 WARNING  pid:22692 util:109:enumerateWithEstimate E156 Training ----/6754, starting\n",
      "2023-12-11 21:08:34,101 INFO     pid:22692 util:126:enumerateWithEstimate E156 Training   64/6754, done at 2023-12-11 21:15:34, 0:07:03\n",
      "2023-12-11 21:08:46,272 INFO     pid:22692 util:126:enumerateWithEstimate E156 Training  256/6754, done at 2023-12-11 21:15:37, 0:07:06\n",
      "2023-12-11 21:09:35,282 INFO     pid:22692 util:126:enumerateWithEstimate E156 Training 1024/6754, done at 2023-12-11 21:15:40, 0:07:09\n",
      "2023-12-11 21:12:51,168 INFO     pid:22692 util:126:enumerateWithEstimate E156 Training 4096/6754, done at 2023-12-11 21:15:40, 0:07:09\n",
      "2023-12-11 21:15:42,135 WARNING  pid:22692 util:139:enumerateWithEstimate E156 Training ----/6754, done at 2023-12-11 21:15:42\n",
      "2023-12-11 21:15:42,135 INFO     pid:22692 __main__:409:logMetrics E156 SegmentationTrainingApp\n",
      "2023-12-11 21:15:42,135 INFO     pid:22692 __main__:444:logMetrics E156 trn      0.2257 loss, 0.1362 fnloss, 0.9956 fploss, 0.8431 precision, 0.8035 recall, 0.8228 f1 score\n",
      "2023-12-11 21:15:42,135 INFO     pid:22692 __main__:458:logMetrics E156 trn_all  0.2257 loss,  80.3% tp,  19.7% fn,      14.9% fp\n",
      "2023-12-11 21:15:42,150 INFO     pid:22692 __main__:191:main Epoch 157 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:15:42,150 WARNING  pid:22692 util:109:enumerateWithEstimate E157 Training ----/6754, starting\n",
      "2023-12-11 21:16:25,318 INFO     pid:22692 util:126:enumerateWithEstimate E157 Training   64/6754, done at 2023-12-11 21:23:27, 0:07:05\n",
      "2023-12-11 21:16:37,488 INFO     pid:22692 util:126:enumerateWithEstimate E157 Training  256/6754, done at 2023-12-11 21:23:28, 0:07:06\n",
      "2023-12-11 21:17:26,218 INFO     pid:22692 util:126:enumerateWithEstimate E157 Training 1024/6754, done at 2023-12-11 21:23:29, 0:07:07\n",
      "2023-12-11 21:20:42,120 INFO     pid:22692 util:126:enumerateWithEstimate E157 Training 4096/6754, done at 2023-12-11 21:23:31, 0:07:09\n",
      "2023-12-11 21:23:33,353 WARNING  pid:22692 util:139:enumerateWithEstimate E157 Training ----/6754, done at 2023-12-11 21:23:33\n",
      "2023-12-11 21:23:33,353 INFO     pid:22692 __main__:409:logMetrics E157 SegmentationTrainingApp\n",
      "2023-12-11 21:23:33,353 INFO     pid:22692 __main__:444:logMetrics E157 trn      0.2256 loss, 0.1257 fnloss, 0.9958 fploss, 0.8358 precision, 0.8150 recall, 0.8253 f1 score\n",
      "2023-12-11 21:23:33,368 INFO     pid:22692 __main__:458:logMetrics E157 trn_all  0.2256 loss,  81.5% tp,  18.5% fn,      16.0% fp\n",
      "2023-12-11 21:23:33,368 INFO     pid:22692 __main__:191:main Epoch 158 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:23:33,368 WARNING  pid:22692 util:109:enumerateWithEstimate E158 Training ----/6754, starting\n",
      "2023-12-11 21:24:16,569 INFO     pid:22692 util:126:enumerateWithEstimate E158 Training   64/6754, done at 2023-12-11 21:31:14, 0:07:01\n",
      "2023-12-11 21:24:28,755 INFO     pid:22692 util:126:enumerateWithEstimate E158 Training  256/6754, done at 2023-12-11 21:31:19, 0:07:06\n",
      "2023-12-11 21:25:17,391 INFO     pid:22692 util:126:enumerateWithEstimate E158 Training 1024/6754, done at 2023-12-11 21:31:20, 0:07:06\n",
      "2023-12-11 21:28:30,918 INFO     pid:22692 util:126:enumerateWithEstimate E158 Training 4096/6754, done at 2023-12-11 21:31:18, 0:07:05\n",
      "2023-12-11 21:31:19,838 WARNING  pid:22692 util:139:enumerateWithEstimate E158 Training ----/6754, done at 2023-12-11 21:31:19\n",
      "2023-12-11 21:31:19,838 INFO     pid:22692 __main__:409:logMetrics E158 SegmentationTrainingApp\n",
      "2023-12-11 21:31:19,838 INFO     pid:22692 __main__:444:logMetrics E158 trn      0.2261 loss, 0.1339 fnloss, 0.9956 fploss, 0.8408 precision, 0.8113 recall, 0.8258 f1 score\n",
      "2023-12-11 21:31:19,838 INFO     pid:22692 __main__:458:logMetrics E158 trn_all  0.2261 loss,  81.1% tp,  18.9% fn,      15.4% fp\n",
      "2023-12-11 21:31:19,854 INFO     pid:22692 __main__:191:main Epoch 159 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:31:19,854 WARNING  pid:22692 util:109:enumerateWithEstimate E159 Training ----/6754, starting\n",
      "2023-12-11 21:32:02,787 INFO     pid:22692 util:126:enumerateWithEstimate E159 Training   64/6754, done at 2023-12-11 21:39:02, 0:07:03\n",
      "2023-12-11 21:32:14,957 INFO     pid:22692 util:126:enumerateWithEstimate E159 Training  256/6754, done at 2023-12-11 21:39:06, 0:07:06\n",
      "2023-12-11 21:33:03,718 INFO     pid:22692 util:126:enumerateWithEstimate E159 Training 1024/6754, done at 2023-12-11 21:39:07, 0:07:07\n",
      "2023-12-11 21:36:17,043 INFO     pid:22692 util:126:enumerateWithEstimate E159 Training 4096/6754, done at 2023-12-11 21:39:04, 0:07:04\n",
      "2023-12-11 21:39:05,855 WARNING  pid:22692 util:139:enumerateWithEstimate E159 Training ----/6754, done at 2023-12-11 21:39:05\n",
      "2023-12-11 21:39:05,855 INFO     pid:22692 __main__:409:logMetrics E159 SegmentationTrainingApp\n",
      "2023-12-11 21:39:05,855 INFO     pid:22692 __main__:444:logMetrics E159 trn      0.2259 loss, 0.1308 fnloss, 0.9956 fploss, 0.8368 precision, 0.8155 recall, 0.8260 f1 score\n",
      "2023-12-11 21:39:05,855 INFO     pid:22692 __main__:458:logMetrics E159 trn_all  0.2259 loss,  81.6% tp,  18.4% fn,      15.9% fp\n",
      "2023-12-11 21:39:05,871 INFO     pid:22692 __main__:191:main Epoch 160 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:39:05,871 WARNING  pid:22692 util:109:enumerateWithEstimate E160 Training ----/6754, starting\n",
      "2023-12-11 21:39:49,121 INFO     pid:22692 util:126:enumerateWithEstimate E160 Training   64/6754, done at 2023-12-11 21:46:55, 0:07:09\n",
      "2023-12-11 21:40:01,338 INFO     pid:22692 util:126:enumerateWithEstimate E160 Training  256/6754, done at 2023-12-11 21:46:54, 0:07:08\n",
      "2023-12-11 21:40:50,146 INFO     pid:22692 util:126:enumerateWithEstimate E160 Training 1024/6754, done at 2023-12-11 21:46:54, 0:07:08\n",
      "2023-12-11 21:44:03,237 INFO     pid:22692 util:126:enumerateWithEstimate E160 Training 4096/6754, done at 2023-12-11 21:46:50, 0:07:04\n",
      "2023-12-11 21:46:52,284 WARNING  pid:22692 util:139:enumerateWithEstimate E160 Training ----/6754, done at 2023-12-11 21:46:52\n",
      "2023-12-11 21:46:52,299 INFO     pid:22692 __main__:409:logMetrics E160 SegmentationTrainingApp\n",
      "2023-12-11 21:46:52,299 INFO     pid:22692 __main__:444:logMetrics E160 trn      0.2205 loss, 0.1243 fnloss, 0.9957 fploss, 0.8385 precision, 0.8200 recall, 0.8292 f1 score\n",
      "2023-12-11 21:46:52,299 INFO     pid:22692 __main__:458:logMetrics E160 trn_all  0.2205 loss,  82.0% tp,  18.0% fn,      15.8% fp\n",
      "2023-12-11 21:46:52,299 WARNING  pid:22692 util:109:enumerateWithEstimate E160 Validation  ----/862, starting\n",
      "2023-12-11 21:46:58,570 INFO     pid:22692 util:126:enumerateWithEstimate E160 Validation    64/862, done at 2023-12-11 21:47:12, 0:00:15\n",
      "2023-12-11 21:47:02,039 INFO     pid:22692 util:126:enumerateWithEstimate E160 Validation   256/862, done at 2023-12-11 21:47:12, 0:00:15\n",
      "2023-12-11 21:47:14,319 WARNING  pid:22692 util:139:enumerateWithEstimate E160 Validation  ----/862, done at 2023-12-11 21:47:14\n",
      "2023-12-11 21:47:14,319 INFO     pid:22692 __main__:409:logMetrics E160 SegmentationTrainingApp\n",
      "2023-12-11 21:47:14,319 INFO     pid:22692 __main__:444:logMetrics E160 val      0.3614 loss, 0.3114 fnloss, 0.9920 fploss, 0.8780 precision, 0.7805 recall, 0.8264 f1 score\n",
      "2023-12-11 21:47:14,319 INFO     pid:22692 __main__:458:logMetrics E160 val_all  0.3614 loss,  78.0% tp,  22.0% fn,      10.8% fp\n",
      "2023-12-11 21:47:14,444 INFO     pid:22692 __main__:511:saveModel Saved model params to models\\udet\\seg_2023-12-11_00.29.58_final-cls.2161280.state\n",
      "2023-12-11 21:47:14,507 INFO     pid:22692 __main__:523:saveModel SHA1: 912a2de06f3ff20952a89f1154c7d901ee798601\n",
      "2023-12-11 21:48:00,910 INFO     pid:22692 __main__:191:main Epoch 161 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:48:00,910 WARNING  pid:22692 util:109:enumerateWithEstimate E161 Training ----/6754, starting\n",
      "2023-12-11 21:48:43,283 INFO     pid:22692 util:126:enumerateWithEstimate E161 Training   64/6754, done at 2023-12-11 21:55:43, 0:07:03\n",
      "2023-12-11 21:48:55,484 INFO     pid:22692 util:126:enumerateWithEstimate E161 Training  256/6754, done at 2023-12-11 21:55:47, 0:07:07\n",
      "2023-12-11 21:49:44,011 INFO     pid:22692 util:126:enumerateWithEstimate E161 Training 1024/6754, done at 2023-12-11 21:55:46, 0:07:06\n",
      "2023-12-11 21:52:57,334 INFO     pid:22692 util:126:enumerateWithEstimate E161 Training 4096/6754, done at 2023-12-11 21:55:44, 0:07:04\n",
      "2023-12-11 21:55:46,164 WARNING  pid:22692 util:139:enumerateWithEstimate E161 Training ----/6754, done at 2023-12-11 21:55:46\n",
      "2023-12-11 21:55:46,164 INFO     pid:22692 __main__:409:logMetrics E161 SegmentationTrainingApp\n",
      "2023-12-11 21:55:46,164 INFO     pid:22692 __main__:444:logMetrics E161 trn      0.2264 loss, 0.1358 fnloss, 0.9956 fploss, 0.8444 precision, 0.8055 recall, 0.8245 f1 score\n",
      "2023-12-11 21:55:46,164 INFO     pid:22692 __main__:458:logMetrics E161 trn_all  0.2264 loss,  80.6% tp,  19.4% fn,      14.8% fp\n",
      "2023-12-11 21:55:46,164 INFO     pid:22692 __main__:191:main Epoch 162 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 21:55:46,180 WARNING  pid:22692 util:109:enumerateWithEstimate E162 Training ----/6754, starting\n",
      "2023-12-11 21:56:29,149 INFO     pid:22692 util:126:enumerateWithEstimate E162 Training   64/6754, done at 2023-12-11 22:03:33, 0:07:07\n",
      "2023-12-11 21:56:41,366 INFO     pid:22692 util:126:enumerateWithEstimate E162 Training  256/6754, done at 2023-12-11 22:03:34, 0:07:08\n",
      "2023-12-11 21:57:29,799 INFO     pid:22692 util:126:enumerateWithEstimate E162 Training 1024/6754, done at 2023-12-11 22:03:31, 0:07:05\n",
      "2023-12-11 22:00:42,846 INFO     pid:22692 util:126:enumerateWithEstimate E162 Training 4096/6754, done at 2023-12-11 22:03:30, 0:07:04\n",
      "2023-12-11 22:03:31,486 WARNING  pid:22692 util:139:enumerateWithEstimate E162 Training ----/6754, done at 2023-12-11 22:03:31\n",
      "2023-12-11 22:03:31,502 INFO     pid:22692 __main__:409:logMetrics E162 SegmentationTrainingApp\n",
      "2023-12-11 22:03:31,502 INFO     pid:22692 __main__:444:logMetrics E162 trn      0.2259 loss, 0.1353 fnloss, 0.9957 fploss, 0.8441 precision, 0.8082 recall, 0.8257 f1 score\n",
      "2023-12-11 22:03:31,502 INFO     pid:22692 __main__:458:logMetrics E162 trn_all  0.2259 loss,  80.8% tp,  19.2% fn,      14.9% fp\n",
      "2023-12-11 22:03:31,502 INFO     pid:22692 __main__:191:main Epoch 163 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 22:03:31,502 WARNING  pid:22692 util:109:enumerateWithEstimate E163 Training ----/6754, starting\n",
      "2023-12-11 22:04:14,357 INFO     pid:22692 util:126:enumerateWithEstimate E163 Training   64/6754, done at 2023-12-11 22:11:12, 0:07:01\n",
      "2023-12-11 22:04:26,465 INFO     pid:22692 util:126:enumerateWithEstimate E163 Training  256/6754, done at 2023-12-11 22:11:15, 0:07:04\n",
      "2023-12-11 22:05:15,054 INFO     pid:22692 util:126:enumerateWithEstimate E163 Training 1024/6754, done at 2023-12-11 22:11:17, 0:07:05\n",
      "2023-12-11 22:08:28,131 INFO     pid:22692 util:126:enumerateWithEstimate E163 Training 4096/6754, done at 2023-12-11 22:11:15, 0:07:04\n",
      "2023-12-11 22:11:17,053 WARNING  pid:22692 util:139:enumerateWithEstimate E163 Training ----/6754, done at 2023-12-11 22:11:17\n",
      "2023-12-11 22:11:17,053 INFO     pid:22692 __main__:409:logMetrics E163 SegmentationTrainingApp\n",
      "2023-12-11 22:11:17,068 INFO     pid:22692 __main__:444:logMetrics E163 trn      0.2204 loss, 0.1249 fnloss, 0.9957 fploss, 0.8379 precision, 0.8200 recall, 0.8288 f1 score\n",
      "2023-12-11 22:11:17,068 INFO     pid:22692 __main__:458:logMetrics E163 trn_all  0.2204 loss,  82.0% tp,  18.0% fn,      15.9% fp\n",
      "2023-12-11 22:11:17,068 INFO     pid:22692 __main__:191:main Epoch 164 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 22:11:17,068 WARNING  pid:22692 util:109:enumerateWithEstimate E164 Training ----/6754, starting\n",
      "2023-12-11 22:12:00,204 INFO     pid:22692 util:126:enumerateWithEstimate E164 Training   64/6754, done at 2023-12-11 22:19:04, 0:07:07\n",
      "2023-12-11 22:12:12,391 INFO     pid:22692 util:126:enumerateWithEstimate E164 Training  256/6754, done at 2023-12-11 22:19:04, 0:07:07\n",
      "2023-12-11 22:13:01,137 INFO     pid:22692 util:126:enumerateWithEstimate E164 Training 1024/6754, done at 2023-12-11 22:19:04, 0:07:07\n",
      "2023-12-11 22:16:14,322 INFO     pid:22692 util:126:enumerateWithEstimate E164 Training 4096/6754, done at 2023-12-11 22:19:01, 0:07:04\n",
      "2023-12-11 22:19:03,400 WARNING  pid:22692 util:139:enumerateWithEstimate E164 Training ----/6754, done at 2023-12-11 22:19:03\n",
      "2023-12-11 22:19:03,402 INFO     pid:22692 __main__:409:logMetrics E164 SegmentationTrainingApp\n",
      "2023-12-11 22:19:03,404 INFO     pid:22692 __main__:444:logMetrics E164 trn      0.2271 loss, 0.1375 fnloss, 0.9956 fploss, 0.8453 precision, 0.8094 recall, 0.8270 f1 score\n",
      "2023-12-11 22:19:03,405 INFO     pid:22692 __main__:458:logMetrics E164 trn_all  0.2271 loss,  80.9% tp,  19.1% fn,      14.8% fp\n",
      "2023-12-11 22:19:03,410 INFO     pid:22692 __main__:191:main Epoch 165 of 200, 6754/862 batches of size 2*1\n",
      "2023-12-11 22:19:03,414 WARNING  pid:22692 util:109:enumerateWithEstimate E165 Training ----/6754, starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m SegmentationTrainingApp(sys_argv\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--epochs=200\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--augmented\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal-cls\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--num-workers=16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch-size=2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmain()\n",
      "Cell \u001b[1;32mIn[7], line 200\u001b[0m, in \u001b[0;36mSegmentationTrainingApp.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_ndx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcli_args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    191\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m batches of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    192\u001b[0m         epoch_ndx,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcli_args\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m         (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    198\u001b[0m     ))\n\u001b[1;32m--> 200\u001b[0m     trnMetrics_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoTraining(epoch_ndx, train_dl)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogMetrics(epoch_ndx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrn\u001b[39m\u001b[38;5;124m'\u001b[39m, trnMetrics_t)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_ndx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch_ndx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_cadence \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# if epoch_ndx % self.validation_cadence == 0:\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;66;03m# if validation is wanted\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 228\u001b[0m, in \u001b[0;36mSegmentationTrainingApp.doTraining\u001b[1;34m(self, epoch_ndx, train_dl)\u001b[0m\n\u001b[0;32m    221\u001b[0m train_dl\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mshuffleSamples()\n\u001b[0;32m    223\u001b[0m batch_iter \u001b[38;5;241m=\u001b[39m enumerateWithEstimate(\n\u001b[0;32m    224\u001b[0m     train_dl,\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_ndx),\n\u001b[0;32m    226\u001b[0m     start_ndx\u001b[38;5;241m=\u001b[39mtrain_dl\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m    227\u001b[0m )\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_ndx, batch_tup \u001b[38;5;129;01min\u001b[39;00m batch_iter:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    231\u001b[0m     loss_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputeBatchLoss(batch_ndx, batch_tup, train_dl\u001b[38;5;241m.\u001b[39mbatch_size, trnMetrics_g)\n",
      "File \u001b[1;32mC:\\LUNA\\udet\\util.py:114\u001b[0m, in \u001b[0;36menumerateWithEstimate\u001b[1;34m(iter1, desc_str, start_ndx, print_ndx, backoff, iter_len)\u001b[0m\n\u001b[0;32m    109\u001b[0m log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ----/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, starting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    110\u001b[0m     desc_str,\n\u001b[0;32m    111\u001b[0m     iter_len,\n\u001b[0;32m    112\u001b[0m ))\n\u001b[0;32m    113\u001b[0m start_ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (current_ndx, item) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iter1):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (current_ndx, item)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_ndx \u001b[38;5;241m==\u001b[39m print_ndx:\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;66;03m# ... <1>\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SegmentationTrainingApp(sys_argv=['--epochs=200','--augmented', 'final-cls',\"--num-workers=16\", \"--batch-size=2\"]).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir E:\\LUNA\\nodule_detection\\runs\\p2ch13\\2023-08-17_12.51.58_val_seg_final-cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMbfUFaTZt05"
   },
   "outputs": [],
   "source": [
    "# run('p2ch13.training.SegmentationTrainingApp', f'--epochs={final_epochs}', '--augmented', 'final-cls')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
